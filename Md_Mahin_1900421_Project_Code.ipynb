{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdvancedML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize Functions to Generate Outputs"
      ],
      "metadata": {
        "id": "dWvZxt7Q18PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "    \n",
        "def plot_reconstruction(model, n=24):\n",
        "    x,_ = next(iter(data_loader))\n",
        "    x = x[:n,:,:,:].to(device)\n",
        "    try:\n",
        "        out, _, _, log_p = model(x.view(-1, image_size)) \n",
        "    except:\n",
        "        out, _, _ = model(x.view(-1, image_size)) \n",
        "    x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
        "    out_grid = torchvision.utils.make_grid(x_concat).cpu().data\n",
        "    show(out_grid)\n",
        "\n",
        "def plot_reconstructionG(model, l_onehot, n=24):\n",
        "    x,_ = next(iter(data_loader))\n",
        "    x = x[:n,:,:,:].to(device)\n",
        "    try:\n",
        "        out, _, _, log_p = model(x.view(-1, image_size), l_onehot) \n",
        "    except:\n",
        "        out, _, _ = model(x.view(-1, image_size)) \n",
        "    x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
        "    out_grid = torchvision.utils.make_grid(x_concat).cpu().data\n",
        "    show(out_grid)\n",
        "def plot_generation(model, n=24):\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(n, z_dim).to(device)\n",
        "        out = model.decode(z).view(-1, 1, 28, 28)\n",
        "\n",
        "    out_grid = torchvision.utils.make_grid(out).cpu()\n",
        "    show(out_grid)\n",
        "\n",
        "def plot_conditional_generation(model, n=8, fix_number=None):\n",
        "    with torch.no_grad():\n",
        "        matrix = np.zeros((n,n_classes))\n",
        "        matrix[:,0] = 1\n",
        "\n",
        "        if fix_number is None:\n",
        "            final = matrix[:]\n",
        "            for i in range(1,n_classes):\n",
        "                final = np.vstack((final,np.roll(matrix,i)))\n",
        "            #z = torch.randn(8*n_classes, z_dim).to(device)\n",
        "            z = torch.randn(8, z_dim)\n",
        "            z = z.repeat(n_classes,1).to(device)\n",
        "            y_onehot = torch.tensor(final).type(torch.FloatTensor).to(device)\n",
        "            out = model.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
        "        else:\n",
        "            z = torch.randn(n, z_dim).to(device)\n",
        "            y_onehot = torch.tensor(np.roll(matrix, fix_number)).type(torch.FloatTensor).to(device)\n",
        "            out = model.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
        "\n",
        "    out_grid = torchvision.utils.make_grid(out).cpu()\n",
        "    show(out_grid)"
      ],
      "metadata": {
        "id": "SXG4ZzjSNVj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Device and Read Data"
      ],
      "metadata": {
        "id": "EjQQ9vxs2Vfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Devices\n"
      ],
      "metadata": {
        "id": "sVUqH56T2NMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create a directory if not exists\n",
        "sample_dir = 'samples'\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)"
      ],
      "metadata": {
        "id": "pnUOo-TiNYUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data"
      ],
      "metadata": {
        "id": "ByKqRkiz2aw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "data_dir = 'data'\n",
        "# MNIST dataset\n",
        "dataset = torchvision.datasets.MNIST(root=data_dir,\n",
        "                                     train=True,\n",
        "                                     transform=transforms.ToTensor(),\n",
        "                                     download=True)\n",
        "\n",
        "# Data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=transforms.ToTensor()),\n",
        "    batch_size=10, shuffle=False)"
      ],
      "metadata": {
        "id": "-XQuS0XJNeR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Model Architecture"
      ],
      "metadata": {
        "id": "6SX_aNLq2ety"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZlzKxB1tjdZ"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "h_dim = 400\n",
        "z_dim = 20\n",
        "num_epochs = 25\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(image_size, h_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
        "        self.fc5 = nn.Linear(h_dim, image_size)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc2(h), self.fc3(h)\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        return torch.sigmoid(self.fc5(h))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconst = self.decode(z)\n",
        "        return x_reconst, mu, log_var\n",
        "\n",
        "model1 = VAE().to(device)\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
        "\n",
        "model2 = VAE().to(device)\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
        "\n",
        "model3 = VAE().to(device)\n",
        "optimizer3 = torch.optim.Adam(model3.parameters(), lr=learning_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beta Optimizer VAE Training"
      ],
      "metadata": {
        "id": "FimoCa-b2kqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpmath import *\n",
        "import random\n",
        "beta_values = []\n",
        "run = []\n",
        "# Number of arm or value ranges\n",
        "d = 10\n",
        "\n",
        "# Parameters for n arm bandit \n",
        "arm_selected = []\n",
        "numbers_of_selections = [0] * d \n",
        "sums_of_reward = [0] * d\n",
        "arm = 0\n",
        "# Simulated anneling parameters\n",
        "# for each arm we will save those values\n",
        "arm_best = [0] * d\n",
        "arm_current = [0] * d\n",
        "arm_best_reward = [0] * d\n",
        "arm_current_reward = [0] * d\n",
        "candidate = 0 \n",
        "candidate_reward = 0\n",
        "step_size_list = [0] * d\n",
        "temp = 10\n",
        "pre_loss = 0\n",
        "pre_loss_per_arm = [0] * d\n",
        "#Define search space for each arms\n",
        "beg_list = []\n",
        "end_list = []\n",
        "search_domain = 10\n",
        "search_range = search_domain/d\n",
        "start = 1\n",
        "end = 0\n",
        "for i in range(0,d-1):\n",
        "  beg_list.append(start)\n",
        "  end = start + search_range\n",
        "  end_list.append(end)\n",
        "  start += search_range\n",
        "beg_list.append(start)\n",
        "end_list.append(search_domain*2)\n",
        "print(step_size_list)\n",
        "# Define step sizes for first d-1 arm and last arm. last arm will have different \n",
        "# range than the first arm\n",
        "for i in range(0,d-1):\n",
        "  step_size_list[i] = (end_list[i] - beg_list[i]) * 0.25\n",
        "step_size_list[d-1] = (end_list[d-1] - beg_list[d-1]) * 0.25\n",
        "\n",
        "# Initialize arm best based on their limit, their middle point\n",
        "for i in range(0,d):\n",
        "  arm_best[i] = (beg_list[i] + end_list[i])/2\n",
        "arm_generation = 1\n",
        "n = 0\n",
        "c = 0\n",
        "epoch_count = 0\n",
        "epoch_counts = []\n",
        "epoch_betas = []\n",
        "epoch_loss = []\n",
        "epoch_loss_main = []\n",
        "iteration_loss = []\n",
        "#*****\n",
        "iteration_betas = []\n",
        "total_couns = []\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_beta = []\n",
        "    for i, (x, _) in enumerate(data_loader):\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "          # Forward pass\n",
        "          x = x.to(device).view(-1, image_size)\n",
        "          x_reconst, mu, log_var = model1(x)\n",
        "          \n",
        "          # Compute reconstruction loss and kl divergence\n",
        "          # For KL divergence between Gaussians, see Appendix B in VAE paper or (Doersch, 2016):\n",
        "          # https://arxiv.org/abs/1606.05908\n",
        "          reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "          kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "          if (arm_generation == 1):\n",
        "            # Select the arm, n arm bandit using UCB\n",
        "            max_upper_bound = 0\n",
        "            for i in range(0, d):\n",
        "                if (numbers_of_selections[i] > 0):\n",
        "                    average_reward = sums_of_reward[i] / numbers_of_selections[i]\n",
        "                    delta_i = math.sqrt(2 * math.log(n+1) / numbers_of_selections[i])\n",
        "                    upper_bound = average_reward + delta_i\n",
        "                else:\n",
        "                    upper_bound = 1e400\n",
        "                if upper_bound > max_upper_bound:\n",
        "                    max_upper_bound = upper_bound\n",
        "                    arm = i\n",
        "            arm_selected.append(arm)\n",
        "            numbers_of_selections[arm] += 1\n",
        "            print(arm)\n",
        "          print(i)\n",
        "          # search a beta value in arm's search space using simulated anneling\n",
        "          if(numbers_of_selections[arm] == 1):\n",
        "            # we will use the loss function here =\n",
        "            reward = reconst_loss + arm_best[arm]*kl_div\n",
        "            arm_best_reward[arm] = 1/ reward\n",
        "            arm_current[arm] = arm_best[arm]\n",
        "            arm_current_reward[arm] = arm_best_reward[arm]\n",
        "            sums_of_reward[arm] += arm_best_reward[arm]\n",
        "            pre_loss = reward\n",
        "            pre_loss_per_arm[arm] = reward\n",
        "            iteration_loss.append(pre_loss)\n",
        "            optimizer1.zero_grad()\n",
        "            reward.backward(retain_graph=False)\n",
        "            optimizer1.step()\n",
        "\n",
        "          else:\n",
        "            \n",
        "            \n",
        "            if arm_generation == 1:\n",
        "              step_size = step_size_list[arm]\n",
        "              step_size_list[arm] = step_size - step_size * 0.05\n",
        "              candidate = arm_current[arm] + np.random.normal() * step_size\n",
        "              candidate_reward_loss = reconst_loss + candidate*kl_div\n",
        "              optimizer1.zero_grad()\n",
        "              iteration_loss.append(candidate_reward_loss)\n",
        "              candidate_reward_loss.backward(retain_graph=False)\n",
        "              optimizer1.step()\n",
        "              arm_generation = 0\n",
        "              \n",
        "            \n",
        "            else:\n",
        "              new_loss = 1/(reconst_loss + candidate*kl_div)\n",
        "              iteration_loss.append(new_loss)\n",
        "              candidate_reward = (1/candidate_reward_loss) - (1/pre_loss)\n",
        "              # check for new best solution\n",
        "              if new_loss > arm_best_reward[arm]:\n",
        "                # store new best point\n",
        "                arm_best[arm], arm_best_reward[arm] = candidate, new_loss\n",
        "\n",
        "              # difference between candidate and current point evaluation\n",
        "              diff = candidate_reward - arm_current_reward[arm]\n",
        "              # calculate temperature for current epoch\n",
        "              t = temp / float(n + 1)\n",
        "            \n",
        "              # calculate metropolis acceptance criterion\n",
        "              #try:\n",
        "              #  metropolis = exp(-diff / t)\n",
        "              #except OverflowError:\n",
        "              #  metropolis = float('inf')\n",
        "              # check if we should keep the new point\n",
        "              if diff < 0 or np.log(np.random.rand())*t < diff:\n",
        "                # store the new current point\n",
        "                arm_current[arm], arm_current_reward[arm] = candidate, new_loss\n",
        "              # calculate the reward, we will consider the best reward from each arm, not \n",
        "              # the exploration arm, this way our loss function within the arm will remain\n",
        "            \n",
        "            \n",
        "              #consistant\n",
        "              pre_loss = new_loss\n",
        "              pre_loss_per_arm[arm] = new_loss\n",
        "              reward = 1/pre_loss - 1/arm_current_reward[arm]\n",
        "              sums_of_reward[arm] += candidate_reward\n",
        "              #loss = arm_best_reward[arm]\n",
        "              arm_generation = 1\n",
        "            \n",
        "            #beta = random.randint(beg_list[arm], end_list[arm])\n",
        "          beta_values.append(arm_current[arm])\n",
        "          epoch_beta.append(arm_current[arm])\n",
        "          iteration_betas.append(arm_current[arm])\n",
        "          # Backprop and optimize\n",
        "          #loss = reconst_loss + candidate * kl_div\n",
        "          #pre_loss = loss\n",
        "          #print(loss)\n",
        "          #sums_of_reward[arm] += 1/loss\n",
        "          \n",
        "          #optimizer.zero_grad()\n",
        "          #loss.backward(retain_graph=False)\n",
        "          #optimizer.step()\n",
        "          total_couns.append(c)\n",
        "          c += 1\n",
        "\n",
        "        n += 1    \n",
        "        run.append(n)\n",
        "        if (i+1) % 10 == 0:\n",
        "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
        "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size, kl_div.item()/batch_size))\n",
        "    epoch_count += 1\n",
        "    epoch_counts.append(epoch_count)\n",
        "    epoch_betas.append(sum(iteration_betas)/len(iteration_betas))\n",
        "    torch.cuda.empty_cache()\n",
        "    epoch_loss_main.append(sum(iteration_loss)/len(iteration_loss))\n",
        "    \n"
      ],
      "metadata": {
        "id": "u11p3VIzNRzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca0f405-83ac-4fe3-82e8-2d355f4d8ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 142.1418, KL Div: 3.9955\n",
            "384\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 147.3286, KL Div: 4.1720\n",
            "386\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 140.7021, KL Div: 4.0002\n",
            "388\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 148.1332, KL Div: 4.1464\n",
            "390\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 148.3616, KL Div: 4.1218\n",
            "392\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 138.3914, KL Div: 4.1875\n",
            "394\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 142.6860, KL Div: 4.1699\n",
            "396\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 143.7518, KL Div: 4.0756\n",
            "398\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 140.2491, KL Div: 4.1491\n",
            "400\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 142.7719, KL Div: 4.0480\n",
            "402\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 138.0460, KL Div: 4.2127\n",
            "404\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 145.8797, KL Div: 4.3305\n",
            "406\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 143.3190, KL Div: 4.0088\n",
            "408\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 142.7081, KL Div: 4.0576\n",
            "410\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 147.6464, KL Div: 4.1086\n",
            "412\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 148.1258, KL Div: 3.8920\n",
            "414\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 147.5119, KL Div: 4.0978\n",
            "416\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 148.9788, KL Div: 3.9058\n",
            "418\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 145.0236, KL Div: 3.8410\n",
            "420\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 148.7327, KL Div: 3.9726\n",
            "422\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 143.5298, KL Div: 4.0677\n",
            "424\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 144.7170, KL Div: 3.9202\n",
            "426\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 142.3714, KL Div: 3.9677\n",
            "428\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 143.9357, KL Div: 4.0888\n",
            "430\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 152.0046, KL Div: 4.0540\n",
            "432\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 133.7445, KL Div: 4.1398\n",
            "434\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 148.4090, KL Div: 4.1419\n",
            "436\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 147.8444, KL Div: 4.2048\n",
            "438\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 145.3795, KL Div: 4.3825\n",
            "440\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 143.2307, KL Div: 4.2057\n",
            "442\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 144.4557, KL Div: 4.4687\n",
            "444\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 134.7692, KL Div: 4.2629\n",
            "446\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 141.9146, KL Div: 4.3041\n",
            "448\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 138.8556, KL Div: 4.4734\n",
            "450\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 139.4986, KL Div: 4.2134\n",
            "452\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 147.7040, KL Div: 4.2577\n",
            "454\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 148.8010, KL Div: 3.9858\n",
            "456\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 146.0476, KL Div: 4.0962\n",
            "458\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 143.2827, KL Div: 3.7273\n",
            "460\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 141.3002, KL Div: 3.7787\n",
            "462\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 147.4000, KL Div: 3.6650\n",
            "464\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 141.3539, KL Div: 4.0081\n",
            "466\n",
            "8\n",
            "9\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 146.4609, KL Div: 4.1079\n",
            "468\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.4679, KL Div: 3.9499\n",
            "1\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.1047, KL Div: 4.2322\n",
            "3\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 149.5048, KL Div: 4.1266\n",
            "5\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 152.2901, KL Div: 4.0567\n",
            "7\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.3698, KL Div: 4.0337\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.8333, KL Div: 4.1547\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.0614, KL Div: 4.2697\n",
            "11\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.8789, KL Div: 4.2882\n",
            "13\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.0396, KL Div: 4.2228\n",
            "15\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.9795, KL Div: 4.3784\n",
            "17\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 134.4726, KL Div: 4.0757\n",
            "19\n",
            "Epoch[21/25], Step [20/469], Reconst Loss: 148.0514, KL Div: 4.1699\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.4438, KL Div: 4.0146\n",
            "21\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.2322, KL Div: 4.0902\n",
            "23\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.2506, KL Div: 4.0425\n",
            "25\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.1176, KL Div: 3.7983\n",
            "27\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.6525, KL Div: 4.0709\n",
            "29\n",
            "Epoch[21/25], Step [30/469], Reconst Loss: 145.7910, KL Div: 3.8432\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.4590, KL Div: 3.9492\n",
            "31\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.1940, KL Div: 3.8165\n",
            "33\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 150.5656, KL Div: 3.8391\n",
            "35\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.8822, KL Div: 4.0034\n",
            "37\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.4454, KL Div: 4.0829\n",
            "39\n",
            "Epoch[21/25], Step [40/469], Reconst Loss: 143.6130, KL Div: 4.2306\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.9171, KL Div: 4.0178\n",
            "41\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.7449, KL Div: 4.2304\n",
            "43\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.5006, KL Div: 4.0365\n",
            "45\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.8564, KL Div: 3.7898\n",
            "47\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 150.0377, KL Div: 4.0899\n",
            "49\n",
            "Epoch[21/25], Step [50/469], Reconst Loss: 140.8442, KL Div: 3.9786\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.6267, KL Div: 4.1511\n",
            "51\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.4994, KL Div: 4.2629\n",
            "53\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.5918, KL Div: 4.2665\n",
            "55\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 137.2929, KL Div: 4.1019\n",
            "57\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.1151, KL Div: 4.0742\n",
            "59\n",
            "Epoch[21/25], Step [60/469], Reconst Loss: 137.3307, KL Div: 4.1522\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.8474, KL Div: 4.1384\n",
            "61\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 136.0953, KL Div: 4.1641\n",
            "63\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 138.1429, KL Div: 4.0100\n",
            "65\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.7077, KL Div: 4.2018\n",
            "67\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.5627, KL Div: 4.0111\n",
            "69\n",
            "Epoch[21/25], Step [70/469], Reconst Loss: 142.3793, KL Div: 4.0544\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.7355, KL Div: 3.9507\n",
            "71\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.0439, KL Div: 4.0654\n",
            "73\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.6665, KL Div: 4.0714\n",
            "75\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.5313, KL Div: 4.1992\n",
            "77\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.7544, KL Div: 3.9806\n",
            "79\n",
            "Epoch[21/25], Step [80/469], Reconst Loss: 150.0454, KL Div: 4.2135\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.3569, KL Div: 4.0238\n",
            "81\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.1521, KL Div: 4.3227\n",
            "83\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.5877, KL Div: 4.2191\n",
            "85\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.5038, KL Div: 4.2626\n",
            "87\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.0060, KL Div: 4.3922\n",
            "89\n",
            "Epoch[21/25], Step [90/469], Reconst Loss: 144.4582, KL Div: 4.1818\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.0498, KL Div: 4.2242\n",
            "91\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 138.9499, KL Div: 4.2173\n",
            "93\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.4171, KL Div: 4.1516\n",
            "95\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.0681, KL Div: 4.2395\n",
            "97\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.4212, KL Div: 4.2365\n",
            "99\n",
            "Epoch[21/25], Step [100/469], Reconst Loss: 144.7154, KL Div: 4.0545\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.2055, KL Div: 4.0836\n",
            "101\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.6588, KL Div: 3.9908\n",
            "103\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 152.1374, KL Div: 4.2384\n",
            "105\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.2305, KL Div: 4.0192\n",
            "107\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.2601, KL Div: 4.1697\n",
            "109\n",
            "Epoch[21/25], Step [110/469], Reconst Loss: 149.3137, KL Div: 3.9318\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.7739, KL Div: 3.9134\n",
            "111\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.9787, KL Div: 3.9521\n",
            "113\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.6821, KL Div: 4.1725\n",
            "115\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.1311, KL Div: 4.0445\n",
            "117\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.7536, KL Div: 4.1031\n",
            "119\n",
            "Epoch[21/25], Step [120/469], Reconst Loss: 144.1993, KL Div: 4.0656\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.6499, KL Div: 4.1007\n",
            "121\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.7670, KL Div: 4.1116\n",
            "123\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.7568, KL Div: 4.0945\n",
            "125\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.4688, KL Div: 4.1951\n",
            "127\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.8698, KL Div: 4.1489\n",
            "129\n",
            "Epoch[21/25], Step [130/469], Reconst Loss: 144.0066, KL Div: 4.2358\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.0307, KL Div: 4.1459\n",
            "131\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.7463, KL Div: 4.1149\n",
            "133\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.9342, KL Div: 4.2481\n",
            "135\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.4951, KL Div: 4.1206\n",
            "137\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.6612, KL Div: 4.1366\n",
            "139\n",
            "Epoch[21/25], Step [140/469], Reconst Loss: 141.3273, KL Div: 4.2665\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.2109, KL Div: 4.3210\n",
            "141\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.8982, KL Div: 4.2341\n",
            "143\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.9843, KL Div: 4.2083\n",
            "145\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 153.7123, KL Div: 4.1845\n",
            "147\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.7490, KL Div: 3.9752\n",
            "149\n",
            "Epoch[21/25], Step [150/469], Reconst Loss: 147.5519, KL Div: 4.2054\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.0840, KL Div: 4.3698\n",
            "151\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.8434, KL Div: 4.1330\n",
            "153\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 150.1989, KL Div: 4.2713\n",
            "155\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.6568, KL Div: 4.0114\n",
            "157\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.4145, KL Div: 4.1891\n",
            "159\n",
            "Epoch[21/25], Step [160/469], Reconst Loss: 145.6389, KL Div: 4.0996\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.5812, KL Div: 4.2098\n",
            "161\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.4926, KL Div: 4.1838\n",
            "163\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.9876, KL Div: 4.0804\n",
            "165\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 136.6348, KL Div: 4.0660\n",
            "167\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.1771, KL Div: 4.1478\n",
            "169\n",
            "Epoch[21/25], Step [170/469], Reconst Loss: 145.8812, KL Div: 4.3274\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.5019, KL Div: 4.2958\n",
            "171\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 138.9500, KL Div: 4.2096\n",
            "173\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.1078, KL Div: 4.2697\n",
            "175\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.9610, KL Div: 4.1051\n",
            "177\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.7641, KL Div: 4.1713\n",
            "179\n",
            "Epoch[21/25], Step [180/469], Reconst Loss: 145.1392, KL Div: 4.3987\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.4562, KL Div: 4.1788\n",
            "181\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.5614, KL Div: 4.3195\n",
            "183\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.4841, KL Div: 4.2465\n",
            "185\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.8749, KL Div: 4.3863\n",
            "187\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.2979, KL Div: 4.0912\n",
            "189\n",
            "Epoch[21/25], Step [190/469], Reconst Loss: 144.7496, KL Div: 4.1897\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.0991, KL Div: 4.1467\n",
            "191\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 138.2348, KL Div: 4.1465\n",
            "193\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.6781, KL Div: 4.0885\n",
            "195\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.7142, KL Div: 4.2239\n",
            "197\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.7480, KL Div: 4.2750\n",
            "199\n",
            "Epoch[21/25], Step [200/469], Reconst Loss: 141.3775, KL Div: 4.2470\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 153.1778, KL Div: 4.1757\n",
            "201\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.4347, KL Div: 4.2273\n",
            "203\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.8603, KL Div: 4.1848\n",
            "205\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.5513, KL Div: 4.2410\n",
            "207\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.5328, KL Div: 4.0879\n",
            "209\n",
            "Epoch[21/25], Step [210/469], Reconst Loss: 142.3945, KL Div: 4.0986\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.0586, KL Div: 3.9493\n",
            "211\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.4421, KL Div: 4.0035\n",
            "213\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.4616, KL Div: 4.1048\n",
            "215\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.5831, KL Div: 4.0228\n",
            "217\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.9248, KL Div: 3.8749\n",
            "219\n",
            "Epoch[21/25], Step [220/469], Reconst Loss: 146.0710, KL Div: 4.0310\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 136.3576, KL Div: 4.0857\n",
            "221\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.7166, KL Div: 4.1599\n",
            "223\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.1688, KL Div: 3.9101\n",
            "225\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.9348, KL Div: 3.9635\n",
            "227\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.7542, KL Div: 3.9989\n",
            "229\n",
            "Epoch[21/25], Step [230/469], Reconst Loss: 145.5321, KL Div: 4.0032\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 149.7856, KL Div: 4.1066\n",
            "231\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.1744, KL Div: 4.0651\n",
            "233\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.2129, KL Div: 4.0264\n",
            "235\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 149.0671, KL Div: 4.0681\n",
            "237\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.7112, KL Div: 4.3362\n",
            "239\n",
            "Epoch[21/25], Step [240/469], Reconst Loss: 145.5164, KL Div: 4.2663\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.5694, KL Div: 4.2732\n",
            "241\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.2447, KL Div: 4.2057\n",
            "243\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.1321, KL Div: 4.2809\n",
            "245\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.7522, KL Div: 4.2825\n",
            "247\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.6738, KL Div: 4.3364\n",
            "249\n",
            "Epoch[21/25], Step [250/469], Reconst Loss: 140.6091, KL Div: 4.3593\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.7036, KL Div: 4.3865\n",
            "251\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.7809, KL Div: 4.3475\n",
            "253\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 138.9858, KL Div: 4.1341\n",
            "255\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 138.3713, KL Div: 4.2599\n",
            "257\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.0309, KL Div: 4.1530\n",
            "259\n",
            "Epoch[21/25], Step [260/469], Reconst Loss: 142.8829, KL Div: 4.0453\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.6584, KL Div: 4.1569\n",
            "261\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 153.0278, KL Div: 4.0443\n",
            "263\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.1194, KL Div: 3.9997\n",
            "265\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 152.9995, KL Div: 3.9527\n",
            "267\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.8088, KL Div: 4.1310\n",
            "269\n",
            "Epoch[21/25], Step [270/469], Reconst Loss: 146.9711, KL Div: 4.3157\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.1571, KL Div: 4.2829\n",
            "271\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.0120, KL Div: 4.1670\n",
            "273\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.7410, KL Div: 4.1781\n",
            "275\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.7925, KL Div: 4.0109\n",
            "277\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.0640, KL Div: 4.1195\n",
            "279\n",
            "Epoch[21/25], Step [280/469], Reconst Loss: 139.3929, KL Div: 3.9687\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.6761, KL Div: 4.2877\n",
            "281\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.6904, KL Div: 4.1604\n",
            "283\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.4417, KL Div: 3.9955\n",
            "285\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.4117, KL Div: 4.2345\n",
            "287\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.4983, KL Div: 4.0179\n",
            "289\n",
            "Epoch[21/25], Step [290/469], Reconst Loss: 141.2095, KL Div: 4.2088\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 137.5430, KL Div: 4.0591\n",
            "291\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.3612, KL Div: 4.2169\n",
            "293\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.9946, KL Div: 4.2696\n",
            "295\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.5873, KL Div: 4.1419\n",
            "297\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.2119, KL Div: 4.2445\n",
            "299\n",
            "Epoch[21/25], Step [300/469], Reconst Loss: 141.0695, KL Div: 3.9553\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.5675, KL Div: 4.1487\n",
            "301\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.8618, KL Div: 4.0267\n",
            "303\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.2989, KL Div: 3.9846\n",
            "305\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.0270, KL Div: 4.2401\n",
            "307\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.4466, KL Div: 4.1154\n",
            "309\n",
            "Epoch[21/25], Step [310/469], Reconst Loss: 150.0572, KL Div: 4.1041\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.0742, KL Div: 4.0965\n",
            "311\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.5111, KL Div: 4.1545\n",
            "313\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 136.0382, KL Div: 4.1147\n",
            "315\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.1952, KL Div: 4.0733\n",
            "317\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.0237, KL Div: 4.2708\n",
            "319\n",
            "Epoch[21/25], Step [320/469], Reconst Loss: 140.3114, KL Div: 4.2023\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.1964, KL Div: 4.3510\n",
            "321\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.4057, KL Div: 4.2901\n",
            "323\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 137.8091, KL Div: 4.4685\n",
            "325\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.6812, KL Div: 4.3243\n",
            "327\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 136.1715, KL Div: 3.9657\n",
            "329\n",
            "Epoch[21/25], Step [330/469], Reconst Loss: 147.5329, KL Div: 4.1760\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.7300, KL Div: 4.0693\n",
            "331\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.7240, KL Div: 3.9742\n",
            "333\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.0585, KL Div: 4.1044\n",
            "335\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.1423, KL Div: 3.9368\n",
            "337\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.7896, KL Div: 3.9595\n",
            "339\n",
            "Epoch[21/25], Step [340/469], Reconst Loss: 138.4262, KL Div: 3.9109\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.0521, KL Div: 4.1655\n",
            "341\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.7918, KL Div: 3.9927\n",
            "343\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.2074, KL Div: 4.0919\n",
            "345\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.9936, KL Div: 4.1260\n",
            "347\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.0739, KL Div: 4.0645\n",
            "349\n",
            "Epoch[21/25], Step [350/469], Reconst Loss: 148.9416, KL Div: 4.2691\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.3663, KL Div: 4.1210\n",
            "351\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.4340, KL Div: 4.3001\n",
            "353\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.5995, KL Div: 4.2954\n",
            "355\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.8099, KL Div: 4.1716\n",
            "357\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.2514, KL Div: 4.0989\n",
            "359\n",
            "Epoch[21/25], Step [360/469], Reconst Loss: 142.3118, KL Div: 4.2450\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.7014, KL Div: 4.0252\n",
            "361\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.2142, KL Div: 4.2027\n",
            "363\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.0162, KL Div: 4.2929\n",
            "365\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 136.0839, KL Div: 4.1420\n",
            "367\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.2261, KL Div: 4.2442\n",
            "369\n",
            "Epoch[21/25], Step [370/469], Reconst Loss: 140.7072, KL Div: 4.3627\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.8783, KL Div: 4.2802\n",
            "371\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 150.0342, KL Div: 4.0467\n",
            "373\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 143.3174, KL Div: 4.1990\n",
            "375\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.9337, KL Div: 4.3270\n",
            "377\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.6753, KL Div: 4.2068\n",
            "379\n",
            "Epoch[21/25], Step [380/469], Reconst Loss: 147.8005, KL Div: 4.1215\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.3065, KL Div: 4.3418\n",
            "381\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.0724, KL Div: 4.1699\n",
            "383\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.7515, KL Div: 4.1654\n",
            "385\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.5952, KL Div: 4.3054\n",
            "387\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 138.1099, KL Div: 4.2819\n",
            "389\n",
            "Epoch[21/25], Step [390/469], Reconst Loss: 142.6551, KL Div: 4.3224\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.8222, KL Div: 4.1959\n",
            "391\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.1833, KL Div: 4.2368\n",
            "393\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.8214, KL Div: 4.1505\n",
            "395\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.1834, KL Div: 4.1723\n",
            "397\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.6030, KL Div: 4.1193\n",
            "399\n",
            "Epoch[21/25], Step [400/469], Reconst Loss: 148.8647, KL Div: 4.0960\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.6524, KL Div: 3.9857\n",
            "401\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.6693, KL Div: 4.0228\n",
            "403\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.2650, KL Div: 3.9826\n",
            "405\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 149.6556, KL Div: 4.0943\n",
            "407\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.6540, KL Div: 3.9043\n",
            "409\n",
            "Epoch[21/25], Step [410/469], Reconst Loss: 142.8573, KL Div: 3.8985\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 151.4723, KL Div: 4.0761\n",
            "411\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 149.0351, KL Div: 4.0572\n",
            "413\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 137.7802, KL Div: 4.1183\n",
            "415\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.6976, KL Div: 4.1672\n",
            "417\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 146.7306, KL Div: 4.2242\n",
            "419\n",
            "Epoch[21/25], Step [420/469], Reconst Loss: 140.0184, KL Div: 4.1649\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.8028, KL Div: 4.2419\n",
            "421\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 151.2407, KL Div: 4.3744\n",
            "423\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.4672, KL Div: 4.1946\n",
            "425\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.9668, KL Div: 4.4275\n",
            "427\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 135.1314, KL Div: 4.1270\n",
            "429\n",
            "Epoch[21/25], Step [430/469], Reconst Loss: 139.7534, KL Div: 4.1537\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 138.8394, KL Div: 4.2950\n",
            "431\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.7667, KL Div: 4.2793\n",
            "433\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 149.1245, KL Div: 4.1799\n",
            "435\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.2384, KL Div: 4.2571\n",
            "437\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.7729, KL Div: 4.2179\n",
            "439\n",
            "Epoch[21/25], Step [440/469], Reconst Loss: 141.0968, KL Div: 4.0469\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 148.1064, KL Div: 4.4323\n",
            "441\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 140.4833, KL Div: 4.2783\n",
            "443\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 141.7154, KL Div: 4.0115\n",
            "445\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 139.5767, KL Div: 4.0541\n",
            "447\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.1347, KL Div: 4.1252\n",
            "449\n",
            "Epoch[21/25], Step [450/469], Reconst Loss: 144.5835, KL Div: 3.8166\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.3219, KL Div: 3.9588\n",
            "451\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 145.4111, KL Div: 3.9746\n",
            "453\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.4676, KL Div: 3.9730\n",
            "455\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 147.0579, KL Div: 3.9060\n",
            "457\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 156.0879, KL Div: 3.8063\n",
            "459\n",
            "Epoch[21/25], Step [460/469], Reconst Loss: 148.8877, KL Div: 4.1881\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 144.0200, KL Div: 4.1376\n",
            "461\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.2100, KL Div: 3.9832\n",
            "463\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 137.7529, KL Div: 4.2433\n",
            "465\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 142.7965, KL Div: 4.1731\n",
            "467\n",
            "8\n",
            "9\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 105.1349, KL Div: 3.2133\n",
            "0\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.1322, KL Div: 4.1313\n",
            "2\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.4002, KL Div: 4.1918\n",
            "4\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 138.7826, KL Div: 4.3061\n",
            "6\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.8218, KL Div: 4.4434\n",
            "8\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.7095, KL Div: 4.2056\n",
            "10\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.7352, KL Div: 4.4122\n",
            "12\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.3221, KL Div: 4.1987\n",
            "14\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 149.3027, KL Div: 4.2369\n",
            "16\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.3891, KL Div: 4.3525\n",
            "18\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.3997, KL Div: 4.1815\n",
            "20\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.9724, KL Div: 4.3038\n",
            "22\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.4577, KL Div: 4.1642\n",
            "24\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.3540, KL Div: 3.9727\n",
            "26\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.1939, KL Div: 4.0354\n",
            "28\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.4550, KL Div: 3.9786\n",
            "30\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.3480, KL Div: 4.1836\n",
            "32\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.1663, KL Div: 4.0009\n",
            "34\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.7426, KL Div: 3.9739\n",
            "36\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.8163, KL Div: 4.1131\n",
            "38\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 148.0731, KL Div: 4.0053\n",
            "40\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 137.6300, KL Div: 3.9680\n",
            "42\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.6893, KL Div: 4.0107\n",
            "44\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.8796, KL Div: 4.1454\n",
            "46\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 151.5108, KL Div: 4.0419\n",
            "48\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.7444, KL Div: 4.0236\n",
            "50\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.4491, KL Div: 4.0115\n",
            "52\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.8854, KL Div: 4.3846\n",
            "54\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.8330, KL Div: 4.0076\n",
            "56\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.9537, KL Div: 4.1405\n",
            "58\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 134.7975, KL Div: 4.1282\n",
            "60\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.5120, KL Div: 3.9275\n",
            "62\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 149.8503, KL Div: 4.1070\n",
            "64\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.5112, KL Div: 4.2275\n",
            "66\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.8766, KL Div: 4.2488\n",
            "68\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.9896, KL Div: 4.3532\n",
            "70\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.0115, KL Div: 4.2891\n",
            "72\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 149.5547, KL Div: 4.4684\n",
            "74\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.2666, KL Div: 4.3114\n",
            "76\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.6329, KL Div: 4.3660\n",
            "78\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.9480, KL Div: 4.1619\n",
            "80\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 137.0511, KL Div: 4.3666\n",
            "82\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 138.9540, KL Div: 4.2305\n",
            "84\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.2507, KL Div: 4.2998\n",
            "86\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.8502, KL Div: 4.0207\n",
            "88\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.2495, KL Div: 4.3210\n",
            "90\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.0058, KL Div: 4.0325\n",
            "92\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.1686, KL Div: 4.1413\n",
            "94\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 137.1438, KL Div: 4.1150\n",
            "96\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.5143, KL Div: 3.8906\n",
            "98\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 149.8209, KL Div: 4.1501\n",
            "100\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.1940, KL Div: 4.3618\n",
            "102\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.9697, KL Div: 4.2399\n",
            "104\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.2204, KL Div: 4.2624\n",
            "106\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.0106, KL Div: 4.0353\n",
            "108\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.5222, KL Div: 4.1643\n",
            "110\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.5486, KL Div: 4.2004\n",
            "112\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.7193, KL Div: 4.1352\n",
            "114\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.4344, KL Div: 4.2669\n",
            "116\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 137.4258, KL Div: 4.3469\n",
            "118\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.2778, KL Div: 4.2563\n",
            "120\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.8286, KL Div: 4.2304\n",
            "122\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.8137, KL Div: 4.0964\n",
            "124\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.0663, KL Div: 4.1970\n",
            "126\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.8922, KL Div: 4.1563\n",
            "128\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.3646, KL Div: 4.2704\n",
            "130\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.7829, KL Div: 4.0952\n",
            "132\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.3823, KL Div: 3.7786\n",
            "134\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.0906, KL Div: 4.0495\n",
            "136\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.6831, KL Div: 4.1183\n",
            "138\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 150.2585, KL Div: 4.0817\n",
            "140\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.4016, KL Div: 3.8457\n",
            "142\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.4084, KL Div: 4.0061\n",
            "144\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.4234, KL Div: 4.0945\n",
            "146\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.5877, KL Div: 3.9951\n",
            "148\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.5616, KL Div: 4.1494\n",
            "150\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.4912, KL Div: 4.1884\n",
            "152\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.7416, KL Div: 4.1104\n",
            "154\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.8994, KL Div: 4.1362\n",
            "156\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.6665, KL Div: 4.0965\n",
            "158\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.8368, KL Div: 4.3061\n",
            "160\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.9448, KL Div: 4.1389\n",
            "162\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.2917, KL Div: 4.1664\n",
            "164\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 136.9044, KL Div: 3.9921\n",
            "166\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.0845, KL Div: 4.3429\n",
            "168\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.6929, KL Div: 4.1214\n",
            "170\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.1319, KL Div: 4.2218\n",
            "172\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.5627, KL Div: 3.9683\n",
            "174\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.0040, KL Div: 3.9211\n",
            "176\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.4423, KL Div: 4.1714\n",
            "178\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.1804, KL Div: 4.1538\n",
            "180\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 148.4578, KL Div: 4.1143\n",
            "182\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.3866, KL Div: 4.0360\n",
            "184\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.2732, KL Div: 4.2087\n",
            "186\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.3418, KL Div: 4.1316\n",
            "188\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.5301, KL Div: 4.1737\n",
            "190\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.8601, KL Div: 4.3017\n",
            "192\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.9921, KL Div: 4.1071\n",
            "194\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.2399, KL Div: 4.2969\n",
            "196\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.8920, KL Div: 4.1301\n",
            "198\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 138.7480, KL Div: 4.1852\n",
            "200\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 148.2238, KL Div: 4.1724\n",
            "202\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.8949, KL Div: 4.0535\n",
            "204\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.7759, KL Div: 4.2916\n",
            "206\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.2133, KL Div: 4.3102\n",
            "208\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.3344, KL Div: 4.4024\n",
            "210\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.5016, KL Div: 4.4070\n",
            "212\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 149.4545, KL Div: 4.1054\n",
            "214\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 136.5745, KL Div: 4.2893\n",
            "216\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.0897, KL Div: 4.2475\n",
            "218\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.9078, KL Div: 4.3329\n",
            "220\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.2512, KL Div: 4.5037\n",
            "222\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.6612, KL Div: 4.4086\n",
            "224\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 135.4605, KL Div: 4.5027\n",
            "226\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.2709, KL Div: 4.1267\n",
            "228\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.1525, KL Div: 4.2123\n",
            "230\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.0818, KL Div: 3.9706\n",
            "232\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.6631, KL Div: 3.9468\n",
            "234\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.9034, KL Div: 3.9304\n",
            "236\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.5278, KL Div: 4.0559\n",
            "238\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.4217, KL Div: 4.0412\n",
            "240\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.9459, KL Div: 3.9838\n",
            "242\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.5036, KL Div: 4.0340\n",
            "244\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.0660, KL Div: 4.0201\n",
            "246\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.6640, KL Div: 4.2664\n",
            "248\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.7064, KL Div: 4.2343\n",
            "250\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.0030, KL Div: 4.0642\n",
            "252\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 148.5558, KL Div: 4.4865\n",
            "254\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 135.6758, KL Div: 4.1204\n",
            "256\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.4090, KL Div: 4.3568\n",
            "258\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 136.3871, KL Div: 4.3419\n",
            "260\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.7100, KL Div: 4.3724\n",
            "262\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.7653, KL Div: 4.4327\n",
            "264\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.5241, KL Div: 4.2912\n",
            "266\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.5258, KL Div: 4.3522\n",
            "268\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.9101, KL Div: 4.1633\n",
            "270\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.8114, KL Div: 4.1842\n",
            "272\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.1425, KL Div: 4.0174\n",
            "274\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.5672, KL Div: 4.0070\n",
            "276\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.3268, KL Div: 4.0521\n",
            "278\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.8934, KL Div: 3.9035\n",
            "280\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.3963, KL Div: 4.0907\n",
            "282\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 151.7525, KL Div: 4.0215\n",
            "284\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 153.2168, KL Div: 4.0372\n",
            "286\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.6522, KL Div: 4.0489\n",
            "288\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.0901, KL Div: 4.1271\n",
            "290\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.6085, KL Div: 4.2165\n",
            "292\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.6828, KL Div: 4.1798\n",
            "294\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 135.9072, KL Div: 4.1534\n",
            "296\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.2249, KL Div: 4.1845\n",
            "298\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 134.3804, KL Div: 4.1304\n",
            "300\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.1395, KL Div: 4.0673\n",
            "302\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.5655, KL Div: 4.4694\n",
            "304\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.0915, KL Div: 4.3549\n",
            "306\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.5652, KL Div: 4.0388\n",
            "308\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.7789, KL Div: 4.0515\n",
            "310\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.5939, KL Div: 3.9272\n",
            "312\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.8946, KL Div: 4.1455\n",
            "314\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 148.6438, KL Div: 4.1138\n",
            "316\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.2912, KL Div: 4.1843\n",
            "318\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.2837, KL Div: 3.9916\n",
            "320\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.6400, KL Div: 4.0551\n",
            "322\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 136.9764, KL Div: 4.2413\n",
            "324\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.9237, KL Div: 4.3162\n",
            "326\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.8873, KL Div: 4.0840\n",
            "328\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 136.2408, KL Div: 4.2009\n",
            "330\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.3603, KL Div: 4.3315\n",
            "332\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.3933, KL Div: 4.2695\n",
            "334\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.0748, KL Div: 4.2528\n",
            "336\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 138.4090, KL Div: 4.0339\n",
            "338\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.1073, KL Div: 4.0989\n",
            "340\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.3869, KL Div: 4.1829\n",
            "342\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.3806, KL Div: 4.1293\n",
            "344\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.8080, KL Div: 4.1155\n",
            "346\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.4481, KL Div: 4.0990\n",
            "348\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.8625, KL Div: 4.0880\n",
            "350\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.4722, KL Div: 4.3353\n",
            "352\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.5171, KL Div: 4.2283\n",
            "354\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.4538, KL Div: 4.2075\n",
            "356\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.2326, KL Div: 4.3597\n",
            "358\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 136.0094, KL Div: 4.0067\n",
            "360\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.8962, KL Div: 4.1107\n",
            "362\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.0653, KL Div: 4.0874\n",
            "364\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.9580, KL Div: 3.9978\n",
            "366\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 152.2769, KL Div: 4.0786\n",
            "368\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 153.0942, KL Div: 4.0994\n",
            "370\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.5490, KL Div: 4.1478\n",
            "372\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.2496, KL Div: 4.4225\n",
            "374\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.2815, KL Div: 4.1991\n",
            "376\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.7584, KL Div: 4.1423\n",
            "378\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.9292, KL Div: 4.1257\n",
            "380\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 148.7140, KL Div: 4.3642\n",
            "382\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 148.4861, KL Div: 4.4331\n",
            "384\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.2223, KL Div: 4.1494\n",
            "386\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 148.0212, KL Div: 4.2309\n",
            "388\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.5189, KL Div: 4.1628\n",
            "390\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 149.6163, KL Div: 4.1119\n",
            "392\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.8003, KL Div: 4.0744\n",
            "394\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.0455, KL Div: 4.0215\n",
            "396\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.6976, KL Div: 4.2940\n",
            "398\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.8210, KL Div: 4.0265\n",
            "400\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.0802, KL Div: 4.1767\n",
            "402\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.5551, KL Div: 4.2590\n",
            "404\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 148.7545, KL Div: 3.9714\n",
            "406\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.2183, KL Div: 4.0500\n",
            "408\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.4647, KL Div: 4.2461\n",
            "410\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.7505, KL Div: 3.9692\n",
            "412\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.5129, KL Div: 4.0605\n",
            "414\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.6403, KL Div: 4.0033\n",
            "416\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 137.7709, KL Div: 4.0685\n",
            "418\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.7702, KL Div: 4.1925\n",
            "420\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 139.3519, KL Div: 3.9203\n",
            "422\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.8899, KL Div: 4.0947\n",
            "424\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.5858, KL Div: 4.2284\n",
            "426\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.7862, KL Div: 4.2585\n",
            "428\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 138.7743, KL Div: 4.2779\n",
            "430\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.1946, KL Div: 4.3673\n",
            "432\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 140.1513, KL Div: 4.2067\n",
            "434\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.1513, KL Div: 4.0999\n",
            "436\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 150.9836, KL Div: 4.4660\n",
            "438\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 137.1718, KL Div: 4.2274\n",
            "440\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.4743, KL Div: 4.1794\n",
            "442\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.7563, KL Div: 4.3046\n",
            "444\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.6257, KL Div: 4.0954\n",
            "446\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.9095, KL Div: 4.0645\n",
            "448\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 144.1080, KL Div: 4.1468\n",
            "450\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.0859, KL Div: 4.1788\n",
            "452\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 146.4130, KL Div: 3.8173\n",
            "454\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 148.0598, KL Div: 4.0902\n",
            "456\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 147.5078, KL Div: 4.0086\n",
            "458\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.2241, KL Div: 3.9671\n",
            "460\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 142.4823, KL Div: 4.1995\n",
            "462\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 141.8092, KL Div: 4.1456\n",
            "464\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 145.6782, KL Div: 4.1838\n",
            "466\n",
            "8\n",
            "9\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 143.8030, KL Div: 4.2378\n",
            "468\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 135.1432, KL Div: 4.1182\n",
            "1\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.6647, KL Div: 4.4076\n",
            "3\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 150.9149, KL Div: 4.3568\n",
            "5\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.5116, KL Div: 4.1371\n",
            "7\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.2331, KL Div: 4.3119\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 136.0073, KL Div: 4.2390\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 138.1441, KL Div: 4.2293\n",
            "11\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.2691, KL Div: 4.1071\n",
            "13\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.4722, KL Div: 4.1354\n",
            "15\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.9293, KL Div: 4.1845\n",
            "17\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 138.7844, KL Div: 4.2443\n",
            "19\n",
            "Epoch[23/25], Step [20/469], Reconst Loss: 139.7300, KL Div: 4.1398\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.9065, KL Div: 4.1213\n",
            "21\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 138.6914, KL Div: 4.1463\n",
            "23\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 138.2095, KL Div: 4.1894\n",
            "25\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.1985, KL Div: 4.1344\n",
            "27\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.8555, KL Div: 4.1449\n",
            "29\n",
            "Epoch[23/25], Step [30/469], Reconst Loss: 148.5785, KL Div: 4.2941\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.5764, KL Div: 4.0781\n",
            "31\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.7336, KL Div: 4.1382\n",
            "33\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.2073, KL Div: 4.0917\n",
            "35\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.5559, KL Div: 4.2551\n",
            "37\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.1118, KL Div: 4.1330\n",
            "39\n",
            "Epoch[23/25], Step [40/469], Reconst Loss: 157.5196, KL Div: 4.1262\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.5850, KL Div: 4.0065\n",
            "41\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.4868, KL Div: 4.0408\n",
            "43\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.2805, KL Div: 4.0522\n",
            "45\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.9436, KL Div: 4.0719\n",
            "47\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.9510, KL Div: 4.2161\n",
            "49\n",
            "Epoch[23/25], Step [50/469], Reconst Loss: 144.4951, KL Div: 4.1470\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 149.8123, KL Div: 4.1897\n",
            "51\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.0601, KL Div: 4.3349\n",
            "53\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.8359, KL Div: 4.3962\n",
            "55\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.1146, KL Div: 4.4833\n",
            "57\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.0783, KL Div: 4.4377\n",
            "59\n",
            "Epoch[23/25], Step [60/469], Reconst Loss: 142.9598, KL Div: 4.2458\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.4601, KL Div: 4.4485\n",
            "61\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.9696, KL Div: 4.3980\n",
            "63\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.2148, KL Div: 4.3130\n",
            "65\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 138.1799, KL Div: 4.2567\n",
            "67\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.8330, KL Div: 4.3055\n",
            "69\n",
            "Epoch[23/25], Step [70/469], Reconst Loss: 148.5832, KL Div: 4.1628\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.0518, KL Div: 4.0702\n",
            "71\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 151.0186, KL Div: 3.9910\n",
            "73\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.4234, KL Div: 4.1731\n",
            "75\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.1121, KL Div: 4.0603\n",
            "77\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.8195, KL Div: 4.0601\n",
            "79\n",
            "Epoch[23/25], Step [80/469], Reconst Loss: 142.8877, KL Div: 3.8397\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.6015, KL Div: 3.8460\n",
            "81\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.6725, KL Div: 3.7505\n",
            "83\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 151.0441, KL Div: 3.8330\n",
            "85\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 153.9921, KL Div: 3.9914\n",
            "87\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.0244, KL Div: 4.1910\n",
            "89\n",
            "Epoch[23/25], Step [90/469], Reconst Loss: 136.3378, KL Div: 4.0070\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 149.0250, KL Div: 4.0541\n",
            "91\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.6988, KL Div: 4.2325\n",
            "93\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.7052, KL Div: 4.1446\n",
            "95\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.0515, KL Div: 4.1326\n",
            "97\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 137.7366, KL Div: 4.4705\n",
            "99\n",
            "Epoch[23/25], Step [100/469], Reconst Loss: 142.0294, KL Div: 4.3575\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.7920, KL Div: 4.3935\n",
            "101\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.3399, KL Div: 4.3412\n",
            "103\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 135.4468, KL Div: 4.3730\n",
            "105\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.5064, KL Div: 4.2734\n",
            "107\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.6730, KL Div: 4.2829\n",
            "109\n",
            "Epoch[23/25], Step [110/469], Reconst Loss: 142.3738, KL Div: 4.1983\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.0758, KL Div: 4.1995\n",
            "111\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.4829, KL Div: 4.1247\n",
            "113\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.5288, KL Div: 4.2216\n",
            "115\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.3613, KL Div: 4.1535\n",
            "117\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.0600, KL Div: 4.1241\n",
            "119\n",
            "Epoch[23/25], Step [120/469], Reconst Loss: 150.2671, KL Div: 3.8002\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.6224, KL Div: 4.1785\n",
            "121\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.2299, KL Div: 3.9465\n",
            "123\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.5875, KL Div: 4.0271\n",
            "125\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.4224, KL Div: 4.0779\n",
            "127\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.2199, KL Div: 4.1622\n",
            "129\n",
            "Epoch[23/25], Step [130/469], Reconst Loss: 146.3737, KL Div: 4.2256\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.4124, KL Div: 4.2160\n",
            "131\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.7785, KL Div: 4.3355\n",
            "133\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 138.4369, KL Div: 4.2884\n",
            "135\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.3174, KL Div: 4.2794\n",
            "137\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.4781, KL Div: 4.2360\n",
            "139\n",
            "Epoch[23/25], Step [140/469], Reconst Loss: 139.9423, KL Div: 4.3402\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.0951, KL Div: 4.2897\n",
            "141\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.4990, KL Div: 4.3974\n",
            "143\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.8945, KL Div: 4.3887\n",
            "145\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.8531, KL Div: 4.1722\n",
            "147\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.4771, KL Div: 4.2387\n",
            "149\n",
            "Epoch[23/25], Step [150/469], Reconst Loss: 141.9136, KL Div: 4.1803\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.5333, KL Div: 4.2140\n",
            "151\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.2997, KL Div: 4.0711\n",
            "153\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 151.2153, KL Div: 4.1524\n",
            "155\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.6527, KL Div: 4.0387\n",
            "157\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.4339, KL Div: 4.0707\n",
            "159\n",
            "Epoch[23/25], Step [160/469], Reconst Loss: 147.6983, KL Div: 4.0590\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.1138, KL Div: 4.1448\n",
            "161\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.9586, KL Div: 4.0651\n",
            "163\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 136.3110, KL Div: 4.2045\n",
            "165\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.8535, KL Div: 4.1060\n",
            "167\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.9930, KL Div: 4.2627\n",
            "169\n",
            "Epoch[23/25], Step [170/469], Reconst Loss: 149.9347, KL Div: 4.1634\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.3972, KL Div: 4.0524\n",
            "171\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.2954, KL Div: 4.2669\n",
            "173\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.0286, KL Div: 4.2377\n",
            "175\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 135.5220, KL Div: 4.0899\n",
            "177\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.2138, KL Div: 4.1618\n",
            "179\n",
            "Epoch[23/25], Step [180/469], Reconst Loss: 137.4703, KL Div: 4.3156\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.6153, KL Div: 4.3631\n",
            "181\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.7058, KL Div: 4.3690\n",
            "183\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.0138, KL Div: 4.3699\n",
            "185\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 137.7665, KL Div: 4.0334\n",
            "187\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.6695, KL Div: 4.4944\n",
            "189\n",
            "Epoch[23/25], Step [190/469], Reconst Loss: 140.1574, KL Div: 4.3021\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.4361, KL Div: 4.1102\n",
            "191\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 150.2885, KL Div: 4.3045\n",
            "193\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 152.0827, KL Div: 4.3041\n",
            "195\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.4442, KL Div: 4.3849\n",
            "197\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 138.5162, KL Div: 4.1281\n",
            "199\n",
            "Epoch[23/25], Step [200/469], Reconst Loss: 143.0165, KL Div: 4.4982\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.1841, KL Div: 4.0198\n",
            "201\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.5486, KL Div: 4.2290\n",
            "203\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.3044, KL Div: 4.3907\n",
            "205\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.2105, KL Div: 4.2267\n",
            "207\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.2287, KL Div: 4.1113\n",
            "209\n",
            "Epoch[23/25], Step [210/469], Reconst Loss: 142.4863, KL Div: 4.1660\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.9946, KL Div: 4.1536\n",
            "211\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 149.0784, KL Div: 4.1157\n",
            "213\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.9582, KL Div: 4.1019\n",
            "215\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.3232, KL Div: 4.0786\n",
            "217\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 150.5717, KL Div: 4.0562\n",
            "219\n",
            "Epoch[23/25], Step [220/469], Reconst Loss: 142.4755, KL Div: 4.1222\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.8590, KL Div: 4.0204\n",
            "221\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.1349, KL Div: 4.2382\n",
            "223\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.2242, KL Div: 4.2501\n",
            "225\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.9288, KL Div: 4.1342\n",
            "227\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.6180, KL Div: 4.1063\n",
            "229\n",
            "Epoch[23/25], Step [230/469], Reconst Loss: 139.7554, KL Div: 4.1859\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.9811, KL Div: 4.1447\n",
            "231\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 137.9952, KL Div: 4.2022\n",
            "233\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.4371, KL Div: 4.2141\n",
            "235\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.3943, KL Div: 4.1958\n",
            "237\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.7701, KL Div: 4.3419\n",
            "239\n",
            "Epoch[23/25], Step [240/469], Reconst Loss: 143.5213, KL Div: 4.2616\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.5096, KL Div: 4.1502\n",
            "241\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.4694, KL Div: 4.5504\n",
            "243\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.1178, KL Div: 4.2462\n",
            "245\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.0681, KL Div: 4.2797\n",
            "247\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.3631, KL Div: 4.1322\n",
            "249\n",
            "Epoch[23/25], Step [250/469], Reconst Loss: 142.5126, KL Div: 4.3111\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.0228, KL Div: 4.1071\n",
            "251\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.1863, KL Div: 4.1868\n",
            "253\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 138.1577, KL Div: 4.1720\n",
            "255\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.1996, KL Div: 4.2712\n",
            "257\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.9940, KL Div: 4.2629\n",
            "259\n",
            "Epoch[23/25], Step [260/469], Reconst Loss: 147.1719, KL Div: 4.2922\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.5481, KL Div: 4.2149\n",
            "261\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.8896, KL Div: 4.1187\n",
            "263\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.9759, KL Div: 4.3377\n",
            "265\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.3524, KL Div: 4.2856\n",
            "267\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.5496, KL Div: 4.2281\n",
            "269\n",
            "Epoch[23/25], Step [270/469], Reconst Loss: 137.1542, KL Div: 4.2206\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.1360, KL Div: 4.0732\n",
            "271\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 138.0019, KL Div: 4.2132\n",
            "273\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.0926, KL Div: 4.3167\n",
            "275\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.5694, KL Div: 4.1309\n",
            "277\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.4996, KL Div: 4.3446\n",
            "279\n",
            "Epoch[23/25], Step [280/469], Reconst Loss: 143.8624, KL Div: 4.2074\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.2166, KL Div: 4.1464\n",
            "281\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.9176, KL Div: 3.9423\n",
            "283\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 149.0489, KL Div: 4.3192\n",
            "285\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.7336, KL Div: 4.1189\n",
            "287\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.1767, KL Div: 4.2044\n",
            "289\n",
            "Epoch[23/25], Step [290/469], Reconst Loss: 140.1399, KL Div: 4.1218\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.3614, KL Div: 4.1223\n",
            "291\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 149.1350, KL Div: 4.1202\n",
            "293\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.2968, KL Div: 4.3342\n",
            "295\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.4665, KL Div: 4.3773\n",
            "297\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.5528, KL Div: 4.2278\n",
            "299\n",
            "Epoch[23/25], Step [300/469], Reconst Loss: 143.2340, KL Div: 4.1692\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.2414, KL Div: 4.3189\n",
            "301\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.6427, KL Div: 4.3113\n",
            "303\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.7975, KL Div: 4.1384\n",
            "305\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.0069, KL Div: 4.1512\n",
            "307\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.7170, KL Div: 4.2043\n",
            "309\n",
            "Epoch[23/25], Step [310/469], Reconst Loss: 143.4657, KL Div: 4.2139\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.8321, KL Div: 4.1790\n",
            "311\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.2479, KL Div: 4.2746\n",
            "313\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.9060, KL Div: 4.2059\n",
            "315\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.2991, KL Div: 4.2038\n",
            "317\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.5336, KL Div: 4.5223\n",
            "319\n",
            "Epoch[23/25], Step [320/469], Reconst Loss: 141.2780, KL Div: 4.0382\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.7348, KL Div: 4.2428\n",
            "321\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.4804, KL Div: 4.4775\n",
            "323\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.5021, KL Div: 4.3228\n",
            "325\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.3105, KL Div: 4.1659\n",
            "327\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.6171, KL Div: 4.3278\n",
            "329\n",
            "Epoch[23/25], Step [330/469], Reconst Loss: 144.0041, KL Div: 4.2573\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 134.0531, KL Div: 4.0528\n",
            "331\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.6415, KL Div: 4.1683\n",
            "333\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.7138, KL Div: 4.0534\n",
            "335\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.2021, KL Div: 4.2672\n",
            "337\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.9903, KL Div: 4.0201\n",
            "339\n",
            "Epoch[23/25], Step [340/469], Reconst Loss: 138.6793, KL Div: 4.2170\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.1243, KL Div: 4.0885\n",
            "341\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.2183, KL Div: 4.1232\n",
            "343\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.2727, KL Div: 4.2728\n",
            "345\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.7726, KL Div: 4.2837\n",
            "347\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.3052, KL Div: 4.3120\n",
            "349\n",
            "Epoch[23/25], Step [350/469], Reconst Loss: 147.2033, KL Div: 4.1902\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 135.8546, KL Div: 4.2294\n",
            "351\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.1350, KL Div: 4.2266\n",
            "353\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.8306, KL Div: 4.0238\n",
            "355\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.8695, KL Div: 4.0418\n",
            "357\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.8642, KL Div: 4.0626\n",
            "359\n",
            "Epoch[23/25], Step [360/469], Reconst Loss: 141.2612, KL Div: 4.1120\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.6861, KL Div: 4.1948\n",
            "361\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.2461, KL Div: 4.2322\n",
            "363\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.5798, KL Div: 4.2049\n",
            "365\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.1742, KL Div: 4.1995\n",
            "367\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 135.8211, KL Div: 4.2876\n",
            "369\n",
            "Epoch[23/25], Step [370/469], Reconst Loss: 142.5302, KL Div: 3.9288\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.0017, KL Div: 4.0504\n",
            "371\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.6452, KL Div: 4.1188\n",
            "373\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.1308, KL Div: 4.0953\n",
            "375\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.6658, KL Div: 4.1149\n",
            "377\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.7743, KL Div: 4.0598\n",
            "379\n",
            "Epoch[23/25], Step [380/469], Reconst Loss: 143.2355, KL Div: 4.0566\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.3274, KL Div: 4.1797\n",
            "381\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.5516, KL Div: 4.2524\n",
            "383\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 150.1770, KL Div: 4.2513\n",
            "385\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.4217, KL Div: 4.1682\n",
            "387\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.9613, KL Div: 4.3749\n",
            "389\n",
            "Epoch[23/25], Step [390/469], Reconst Loss: 141.6441, KL Div: 4.1770\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.3914, KL Div: 4.3761\n",
            "391\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.5078, KL Div: 4.2977\n",
            "393\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.6270, KL Div: 4.3569\n",
            "395\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.7829, KL Div: 4.3869\n",
            "397\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 137.4306, KL Div: 4.2695\n",
            "399\n",
            "Epoch[23/25], Step [400/469], Reconst Loss: 144.2066, KL Div: 4.2667\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.6593, KL Div: 4.2222\n",
            "401\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.1038, KL Div: 4.1924\n",
            "403\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.7281, KL Div: 4.2667\n",
            "405\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.5196, KL Div: 4.1009\n",
            "407\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 143.2979, KL Div: 4.1409\n",
            "409\n",
            "Epoch[23/25], Step [410/469], Reconst Loss: 152.6888, KL Div: 4.1898\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.6505, KL Div: 4.2472\n",
            "411\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 152.9734, KL Div: 4.1837\n",
            "413\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.5552, KL Div: 4.1624\n",
            "415\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 137.9411, KL Div: 4.2947\n",
            "417\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.5704, KL Div: 4.2527\n",
            "419\n",
            "Epoch[23/25], Step [420/469], Reconst Loss: 141.6559, KL Div: 4.3218\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.1369, KL Div: 4.0998\n",
            "421\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 136.5385, KL Div: 4.3229\n",
            "423\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.8367, KL Div: 4.0373\n",
            "425\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 138.7967, KL Div: 4.1819\n",
            "427\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.9469, KL Div: 4.0131\n",
            "429\n",
            "Epoch[23/25], Step [430/469], Reconst Loss: 144.7769, KL Div: 4.2009\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.9003, KL Div: 4.2145\n",
            "431\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.4245, KL Div: 4.2423\n",
            "433\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.0246, KL Div: 4.0150\n",
            "435\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.1022, KL Div: 4.2313\n",
            "437\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.1493, KL Div: 4.1022\n",
            "439\n",
            "Epoch[23/25], Step [440/469], Reconst Loss: 139.8634, KL Div: 4.1046\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 147.5614, KL Div: 4.1788\n",
            "441\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.1497, KL Div: 4.3715\n",
            "443\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.6594, KL Div: 4.3015\n",
            "445\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 139.3889, KL Div: 4.1807\n",
            "447\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 136.4679, KL Div: 3.9692\n",
            "449\n",
            "Epoch[23/25], Step [450/469], Reconst Loss: 144.7101, KL Div: 4.0055\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 142.6899, KL Div: 4.2434\n",
            "451\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 140.3273, KL Div: 4.0376\n",
            "453\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.3747, KL Div: 4.3885\n",
            "455\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 144.9117, KL Div: 4.3619\n",
            "457\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 148.5157, KL Div: 4.2352\n",
            "459\n",
            "Epoch[23/25], Step [460/469], Reconst Loss: 147.9889, KL Div: 4.1075\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 137.8760, KL Div: 4.0632\n",
            "461\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 141.5811, KL Div: 4.2399\n",
            "463\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 146.5090, KL Div: 4.0557\n",
            "465\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 145.8281, KL Div: 4.1372\n",
            "467\n",
            "8\n",
            "9\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 105.1410, KL Div: 3.0252\n",
            "0\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 148.0291, KL Div: 4.2023\n",
            "2\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 149.1380, KL Div: 4.1849\n",
            "4\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.3368, KL Div: 4.1820\n",
            "6\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 137.2240, KL Div: 3.9938\n",
            "8\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.0814, KL Div: 4.1781\n",
            "10\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.2261, KL Div: 4.3319\n",
            "12\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.5031, KL Div: 4.2316\n",
            "14\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 136.6884, KL Div: 4.3532\n",
            "16\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.7333, KL Div: 4.4763\n",
            "18\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.2658, KL Div: 4.3073\n",
            "20\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 148.7132, KL Div: 4.4072\n",
            "22\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.3846, KL Div: 4.2587\n",
            "24\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 136.6615, KL Div: 4.1818\n",
            "26\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.9803, KL Div: 4.3728\n",
            "28\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 150.5085, KL Div: 4.2830\n",
            "30\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.1072, KL Div: 4.1704\n",
            "32\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.5969, KL Div: 4.0148\n",
            "34\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 137.5114, KL Div: 4.1188\n",
            "36\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.4636, KL Div: 4.0743\n",
            "38\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.6335, KL Div: 4.0564\n",
            "40\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 138.9117, KL Div: 4.0882\n",
            "42\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.2296, KL Div: 4.0414\n",
            "44\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.3706, KL Div: 4.3412\n",
            "46\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 149.2682, KL Div: 4.0348\n",
            "48\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.9067, KL Div: 4.2661\n",
            "50\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.0689, KL Div: 4.2595\n",
            "52\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.4936, KL Div: 4.1905\n",
            "54\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.3658, KL Div: 4.1246\n",
            "56\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.4079, KL Div: 4.2575\n",
            "58\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.6347, KL Div: 4.2220\n",
            "60\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.9389, KL Div: 4.1234\n",
            "62\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 148.8731, KL Div: 4.1420\n",
            "64\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.4188, KL Div: 4.1310\n",
            "66\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.3576, KL Div: 4.1171\n",
            "68\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.0222, KL Div: 4.2676\n",
            "70\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 148.7528, KL Div: 4.1516\n",
            "72\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.6733, KL Div: 4.1244\n",
            "74\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 137.7570, KL Div: 4.2815\n",
            "76\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.1137, KL Div: 4.3547\n",
            "78\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.4537, KL Div: 4.3744\n",
            "80\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.6984, KL Div: 4.2483\n",
            "82\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.2208, KL Div: 4.2798\n",
            "84\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.1262, KL Div: 4.3591\n",
            "86\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.1854, KL Div: 4.1262\n",
            "88\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.9826, KL Div: 4.1836\n",
            "90\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.9765, KL Div: 4.2786\n",
            "92\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 150.5041, KL Div: 4.0963\n",
            "94\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.7893, KL Div: 4.1353\n",
            "96\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 134.7379, KL Div: 4.0744\n",
            "98\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.8918, KL Div: 4.1379\n",
            "100\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.0485, KL Div: 4.0349\n",
            "102\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.1809, KL Div: 3.9632\n",
            "104\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.1134, KL Div: 4.1322\n",
            "106\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.6857, KL Div: 3.9100\n",
            "108\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 148.6895, KL Div: 4.1874\n",
            "110\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.1451, KL Div: 4.0030\n",
            "112\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.6673, KL Div: 4.2103\n",
            "114\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 135.2229, KL Div: 4.2349\n",
            "116\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.8981, KL Div: 4.2667\n",
            "118\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.9598, KL Div: 3.9870\n",
            "120\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.1356, KL Div: 4.1830\n",
            "122\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 153.6036, KL Div: 4.1398\n",
            "124\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.6175, KL Div: 4.2719\n",
            "126\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.9686, KL Div: 4.3583\n",
            "128\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.5148, KL Div: 4.1024\n",
            "130\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 138.5281, KL Div: 4.0009\n",
            "132\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.7533, KL Div: 4.2460\n",
            "134\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.8005, KL Div: 4.0345\n",
            "136\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.6115, KL Div: 4.1862\n",
            "138\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 136.9663, KL Div: 4.2892\n",
            "140\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 149.3713, KL Div: 4.2516\n",
            "142\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 153.7465, KL Div: 4.1532\n",
            "144\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.2967, KL Div: 4.2015\n",
            "146\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.4922, KL Div: 4.3011\n",
            "148\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.2403, KL Div: 4.3171\n",
            "150\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.2692, KL Div: 4.2030\n",
            "152\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.2853, KL Div: 4.1807\n",
            "154\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.7132, KL Div: 4.1665\n",
            "156\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.6119, KL Div: 4.2644\n",
            "158\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.8397, KL Div: 4.1359\n",
            "160\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.0099, KL Div: 4.3181\n",
            "162\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.9290, KL Div: 4.3325\n",
            "164\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 135.5854, KL Div: 4.0707\n",
            "166\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.7809, KL Div: 4.1938\n",
            "168\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.7785, KL Div: 4.3248\n",
            "170\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.7061, KL Div: 4.1132\n",
            "172\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.8901, KL Div: 4.1352\n",
            "174\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 136.8600, KL Div: 4.2243\n",
            "176\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.6581, KL Div: 4.3626\n",
            "178\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.0477, KL Div: 4.2042\n",
            "180\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.5324, KL Div: 4.1365\n",
            "182\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.2108, KL Div: 4.1810\n",
            "184\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.1678, KL Div: 4.2478\n",
            "186\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 138.9590, KL Div: 4.1032\n",
            "188\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.8947, KL Div: 4.1397\n",
            "190\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.0411, KL Div: 4.3004\n",
            "192\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.9852, KL Div: 4.2702\n",
            "194\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.2089, KL Div: 4.1756\n",
            "196\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.0627, KL Div: 4.1871\n",
            "198\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.5093, KL Div: 4.1306\n",
            "200\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.8971, KL Div: 4.0389\n",
            "202\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.6611, KL Div: 4.1272\n",
            "204\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.1416, KL Div: 4.3366\n",
            "206\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.1872, KL Div: 4.1565\n",
            "208\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.2583, KL Div: 4.3503\n",
            "210\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.4932, KL Div: 4.3182\n",
            "212\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.2012, KL Div: 4.0678\n",
            "214\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.2455, KL Div: 4.1612\n",
            "216\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 136.5138, KL Div: 4.1449\n",
            "218\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 137.0518, KL Div: 4.0515\n",
            "220\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 148.2008, KL Div: 4.2162\n",
            "222\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.0005, KL Div: 4.1660\n",
            "224\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.3140, KL Div: 4.2487\n",
            "226\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.7301, KL Div: 4.0617\n",
            "228\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.8273, KL Div: 4.1143\n",
            "230\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.9898, KL Div: 4.2986\n",
            "232\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.2377, KL Div: 4.2842\n",
            "234\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.5881, KL Div: 4.2892\n",
            "236\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.4244, KL Div: 4.2788\n",
            "238\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 138.4359, KL Div: 4.2635\n",
            "240\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.6084, KL Div: 4.0509\n",
            "242\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.3978, KL Div: 4.3373\n",
            "244\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.8142, KL Div: 4.2907\n",
            "246\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.8646, KL Div: 4.2048\n",
            "248\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 136.8040, KL Div: 4.0369\n",
            "250\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.1561, KL Div: 4.2393\n",
            "252\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.6613, KL Div: 4.1295\n",
            "254\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 154.8159, KL Div: 4.2544\n",
            "256\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 149.6585, KL Div: 4.1749\n",
            "258\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.2112, KL Div: 4.4251\n",
            "260\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.5587, KL Div: 4.2007\n",
            "262\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 137.8858, KL Div: 4.2399\n",
            "264\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.8230, KL Div: 4.1089\n",
            "266\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.2547, KL Div: 4.2171\n",
            "268\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.5121, KL Div: 4.1633\n",
            "270\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.1977, KL Div: 4.3318\n",
            "272\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.3253, KL Div: 4.3002\n",
            "274\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.2501, KL Div: 4.3912\n",
            "276\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.4517, KL Div: 4.2154\n",
            "278\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.8785, KL Div: 4.2832\n",
            "280\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.1095, KL Div: 4.1251\n",
            "282\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.5053, KL Div: 4.0955\n",
            "284\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.4207, KL Div: 4.3031\n",
            "286\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 148.9050, KL Div: 4.2245\n",
            "288\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 138.7031, KL Div: 4.1791\n",
            "290\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 134.2746, KL Div: 4.3982\n",
            "292\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.0688, KL Div: 4.2397\n",
            "294\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 137.6409, KL Div: 4.2426\n",
            "296\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.6506, KL Div: 4.3518\n",
            "298\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.1765, KL Div: 4.4236\n",
            "300\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.3310, KL Div: 4.3014\n",
            "302\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 138.0941, KL Div: 4.1599\n",
            "304\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.1951, KL Div: 4.1342\n",
            "306\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.3837, KL Div: 4.1996\n",
            "308\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 138.8248, KL Div: 3.9034\n",
            "310\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.5218, KL Div: 3.9667\n",
            "312\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 152.5125, KL Div: 3.9643\n",
            "314\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 136.3562, KL Div: 4.1407\n",
            "316\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.1259, KL Div: 4.0244\n",
            "318\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.2503, KL Div: 4.3139\n",
            "320\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.9077, KL Div: 4.2182\n",
            "322\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 150.7924, KL Div: 4.1371\n",
            "324\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.5583, KL Div: 4.2155\n",
            "326\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.6298, KL Div: 4.3345\n",
            "328\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 150.9249, KL Div: 4.2441\n",
            "330\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.7162, KL Div: 4.1696\n",
            "332\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.0776, KL Div: 4.1721\n",
            "334\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.1681, KL Div: 4.1724\n",
            "336\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.0965, KL Div: 4.3595\n",
            "338\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.5136, KL Div: 4.2684\n",
            "340\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 135.1786, KL Div: 3.9045\n",
            "342\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 149.5443, KL Div: 4.0197\n",
            "344\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.8938, KL Div: 4.0587\n",
            "346\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.2675, KL Div: 4.2027\n",
            "348\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.9427, KL Div: 4.1869\n",
            "350\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 149.2753, KL Div: 4.1659\n",
            "352\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.3934, KL Div: 4.1402\n",
            "354\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.0704, KL Div: 4.2434\n",
            "356\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.8026, KL Div: 4.1615\n",
            "358\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.4703, KL Div: 4.1134\n",
            "360\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.7839, KL Div: 4.3278\n",
            "362\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.6084, KL Div: 4.1871\n",
            "364\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 136.8034, KL Div: 4.2301\n",
            "366\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.3652, KL Div: 4.3099\n",
            "368\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.7994, KL Div: 4.2301\n",
            "370\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.5524, KL Div: 4.1502\n",
            "372\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.6547, KL Div: 4.1646\n",
            "374\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.0815, KL Div: 4.1710\n",
            "376\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.6100, KL Div: 4.1713\n",
            "378\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.7623, KL Div: 4.2604\n",
            "380\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.9711, KL Div: 4.1623\n",
            "382\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.2896, KL Div: 4.2350\n",
            "384\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 141.5072, KL Div: 4.1299\n",
            "386\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.9314, KL Div: 4.0944\n",
            "388\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.8153, KL Div: 4.0562\n",
            "390\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 148.8318, KL Div: 4.1819\n",
            "392\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 138.6988, KL Div: 4.0988\n",
            "394\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.9370, KL Div: 4.1420\n",
            "396\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.5803, KL Div: 4.3280\n",
            "398\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.2711, KL Div: 4.3433\n",
            "400\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.8227, KL Div: 4.3389\n",
            "402\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.2695, KL Div: 4.4670\n",
            "404\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.7215, KL Div: 4.5369\n",
            "406\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.5174, KL Div: 4.4156\n",
            "408\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 136.9566, KL Div: 4.5055\n",
            "410\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.0644, KL Div: 4.3809\n",
            "412\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.7071, KL Div: 4.2589\n",
            "414\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 135.7063, KL Div: 4.4816\n",
            "416\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.7440, KL Div: 4.1994\n",
            "418\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.6399, KL Div: 4.2387\n",
            "420\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.5440, KL Div: 4.0102\n",
            "422\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.9731, KL Div: 3.9837\n",
            "424\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.9445, KL Div: 3.8637\n",
            "426\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 149.8013, KL Div: 3.8737\n",
            "428\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.7079, KL Div: 4.0099\n",
            "430\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 142.0533, KL Div: 4.1173\n",
            "432\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.3732, KL Div: 4.0061\n",
            "434\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.6951, KL Div: 4.2288\n",
            "436\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.9625, KL Div: 4.3575\n",
            "438\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.3967, KL Div: 4.3972\n",
            "440\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 146.1870, KL Div: 4.2382\n",
            "442\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.1516, KL Div: 4.2369\n",
            "444\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 147.2414, KL Div: 4.2207\n",
            "446\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 136.6865, KL Div: 4.3248\n",
            "448\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 145.2881, KL Div: 4.3930\n",
            "450\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 135.3028, KL Div: 4.3135\n",
            "452\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 137.7012, KL Div: 4.3509\n",
            "454\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 140.1803, KL Div: 4.2068\n",
            "456\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 144.3987, KL Div: 4.3024\n",
            "458\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.4879, KL Div: 4.3553\n",
            "460\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 143.5358, KL Div: 3.9839\n",
            "462\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 150.1920, KL Div: 4.3196\n",
            "464\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 148.3452, KL Div: 4.1255\n",
            "466\n",
            "8\n",
            "9\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 139.2804, KL Div: 4.1129\n",
            "468\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.9673, KL Div: 4.2682\n",
            "1\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.7769, KL Div: 4.1798\n",
            "3\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.4247, KL Div: 4.3669\n",
            "5\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.9473, KL Div: 4.1796\n",
            "7\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.3952, KL Div: 4.2467\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.4303, KL Div: 4.2783\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.9451, KL Div: 4.2177\n",
            "11\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.5838, KL Div: 4.2624\n",
            "13\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.8300, KL Div: 4.2594\n",
            "15\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.0513, KL Div: 4.3229\n",
            "17\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.0547, KL Div: 4.1860\n",
            "19\n",
            "Epoch[25/25], Step [20/469], Reconst Loss: 142.1510, KL Div: 4.0547\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.0368, KL Div: 4.2419\n",
            "21\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.7332, KL Div: 4.3927\n",
            "23\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.1756, KL Div: 4.1230\n",
            "25\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.3625, KL Div: 4.2201\n",
            "27\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 135.0408, KL Div: 4.3197\n",
            "29\n",
            "Epoch[25/25], Step [30/469], Reconst Loss: 138.1150, KL Div: 4.3427\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.6297, KL Div: 4.2415\n",
            "31\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 137.3936, KL Div: 4.1079\n",
            "33\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.5355, KL Div: 4.3985\n",
            "35\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.6939, KL Div: 4.3022\n",
            "37\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.9053, KL Div: 4.0581\n",
            "39\n",
            "Epoch[25/25], Step [40/469], Reconst Loss: 136.7886, KL Div: 4.1707\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.7142, KL Div: 4.2215\n",
            "41\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.9346, KL Div: 4.1124\n",
            "43\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.2804, KL Div: 4.1721\n",
            "45\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.1006, KL Div: 4.1354\n",
            "47\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.0730, KL Div: 4.0819\n",
            "49\n",
            "Epoch[25/25], Step [50/469], Reconst Loss: 139.5611, KL Div: 4.2365\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.6927, KL Div: 4.1410\n",
            "51\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 136.0286, KL Div: 4.1779\n",
            "53\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 136.9724, KL Div: 4.1893\n",
            "55\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 137.3717, KL Div: 4.4355\n",
            "57\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.5090, KL Div: 4.2269\n",
            "59\n",
            "Epoch[25/25], Step [60/469], Reconst Loss: 147.0680, KL Div: 4.3765\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.6152, KL Div: 4.2552\n",
            "61\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.2047, KL Div: 4.1236\n",
            "63\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.9026, KL Div: 4.2935\n",
            "65\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.6533, KL Div: 4.3107\n",
            "67\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.5351, KL Div: 4.1082\n",
            "69\n",
            "Epoch[25/25], Step [70/469], Reconst Loss: 152.1871, KL Div: 4.0698\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.4082, KL Div: 4.1744\n",
            "71\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 148.0528, KL Div: 4.1736\n",
            "73\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 139.2932, KL Div: 4.0985\n",
            "75\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.1222, KL Div: 4.1982\n",
            "77\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.8410, KL Div: 4.1484\n",
            "79\n",
            "Epoch[25/25], Step [80/469], Reconst Loss: 147.1197, KL Div: 4.1820\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.3126, KL Div: 4.1549\n",
            "81\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.9218, KL Div: 4.1168\n",
            "83\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.0460, KL Div: 4.2815\n",
            "85\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.2026, KL Div: 4.2557\n",
            "87\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 137.9442, KL Div: 4.2119\n",
            "89\n",
            "Epoch[25/25], Step [90/469], Reconst Loss: 142.3518, KL Div: 4.3904\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.0649, KL Div: 4.3125\n",
            "91\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.8751, KL Div: 4.3099\n",
            "93\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.6822, KL Div: 4.1313\n",
            "95\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.6225, KL Div: 4.1967\n",
            "97\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.2501, KL Div: 4.2774\n",
            "99\n",
            "Epoch[25/25], Step [100/469], Reconst Loss: 142.9662, KL Div: 4.1355\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 139.3528, KL Div: 4.0784\n",
            "101\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.3581, KL Div: 4.1171\n",
            "103\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.1539, KL Div: 4.2959\n",
            "105\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.2947, KL Div: 4.1783\n",
            "107\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.0978, KL Div: 4.2983\n",
            "109\n",
            "Epoch[25/25], Step [110/469], Reconst Loss: 149.1297, KL Div: 4.2253\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.1694, KL Div: 4.2578\n",
            "111\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.3557, KL Div: 4.4601\n",
            "113\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.9673, KL Div: 4.1530\n",
            "115\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 139.2666, KL Div: 4.3936\n",
            "117\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.2618, KL Div: 4.3534\n",
            "119\n",
            "Epoch[25/25], Step [120/469], Reconst Loss: 143.0624, KL Div: 4.3627\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.9286, KL Div: 4.5019\n",
            "121\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.3533, KL Div: 4.2580\n",
            "123\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.4562, KL Div: 4.1037\n",
            "125\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.8854, KL Div: 4.2782\n",
            "127\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.2749, KL Div: 4.2260\n",
            "129\n",
            "Epoch[25/25], Step [130/469], Reconst Loss: 142.6169, KL Div: 4.0685\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.7103, KL Div: 4.1190\n",
            "131\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 135.2062, KL Div: 4.0998\n",
            "133\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.3656, KL Div: 4.1315\n",
            "135\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.3983, KL Div: 4.0697\n",
            "137\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.1091, KL Div: 4.2008\n",
            "139\n",
            "Epoch[25/25], Step [140/469], Reconst Loss: 144.5898, KL Div: 4.1890\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.5881, KL Div: 4.1699\n",
            "141\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 139.9223, KL Div: 4.1294\n",
            "143\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.6537, KL Div: 4.0903\n",
            "145\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.3354, KL Div: 4.2006\n",
            "147\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.0281, KL Div: 4.0684\n",
            "149\n",
            "Epoch[25/25], Step [150/469], Reconst Loss: 148.8517, KL Div: 4.1026\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.0433, KL Div: 4.1365\n",
            "151\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.8168, KL Div: 4.0875\n",
            "153\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 148.4076, KL Div: 4.2109\n",
            "155\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.0297, KL Div: 4.1157\n",
            "157\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.7000, KL Div: 4.2154\n",
            "159\n",
            "Epoch[25/25], Step [160/469], Reconst Loss: 148.5302, KL Div: 4.2408\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.8583, KL Div: 4.2019\n",
            "161\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.8322, KL Div: 4.3179\n",
            "163\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.5466, KL Div: 4.3334\n",
            "165\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.5417, KL Div: 4.3707\n",
            "167\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 137.7943, KL Div: 4.4443\n",
            "169\n",
            "Epoch[25/25], Step [170/469], Reconst Loss: 138.0259, KL Div: 4.4291\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.1632, KL Div: 4.4513\n",
            "171\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.9620, KL Div: 4.4739\n",
            "173\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.8351, KL Div: 4.3710\n",
            "175\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.2420, KL Div: 4.3105\n",
            "177\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.0889, KL Div: 4.2921\n",
            "179\n",
            "Epoch[25/25], Step [180/469], Reconst Loss: 139.6413, KL Div: 4.1321\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.5557, KL Div: 4.1652\n",
            "181\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.3762, KL Div: 4.1516\n",
            "183\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.6641, KL Div: 4.1011\n",
            "185\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 139.9814, KL Div: 4.0962\n",
            "187\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.0911, KL Div: 4.1691\n",
            "189\n",
            "Epoch[25/25], Step [190/469], Reconst Loss: 145.5490, KL Div: 4.0194\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.7543, KL Div: 4.2024\n",
            "191\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.1704, KL Div: 4.2240\n",
            "193\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 154.9294, KL Div: 4.2429\n",
            "195\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.5191, KL Div: 4.1755\n",
            "197\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 135.2188, KL Div: 4.1267\n",
            "199\n",
            "Epoch[25/25], Step [200/469], Reconst Loss: 144.0536, KL Div: 4.3494\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 137.9224, KL Div: 4.0331\n",
            "201\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.0920, KL Div: 4.3220\n",
            "203\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 132.6577, KL Div: 4.3350\n",
            "205\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.8000, KL Div: 4.3456\n",
            "207\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 135.1841, KL Div: 4.3179\n",
            "209\n",
            "Epoch[25/25], Step [210/469], Reconst Loss: 148.0771, KL Div: 4.1900\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.4958, KL Div: 4.2821\n",
            "211\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.0110, KL Div: 4.0326\n",
            "213\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 150.2456, KL Div: 4.2242\n",
            "215\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 149.3611, KL Div: 3.9699\n",
            "217\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.3443, KL Div: 4.0463\n",
            "219\n",
            "Epoch[25/25], Step [220/469], Reconst Loss: 142.1294, KL Div: 4.0377\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.3667, KL Div: 4.0975\n",
            "221\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.3279, KL Div: 4.0667\n",
            "223\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.9996, KL Div: 4.1304\n",
            "225\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.8975, KL Div: 4.2150\n",
            "227\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.0699, KL Div: 4.0253\n",
            "229\n",
            "Epoch[25/25], Step [230/469], Reconst Loss: 146.5551, KL Div: 4.2513\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.5750, KL Div: 4.1198\n",
            "231\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.9092, KL Div: 4.2262\n",
            "233\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.0323, KL Div: 4.0140\n",
            "235\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.9518, KL Div: 4.2010\n",
            "237\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.7991, KL Div: 4.2429\n",
            "239\n",
            "Epoch[25/25], Step [240/469], Reconst Loss: 143.3846, KL Div: 4.2441\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.2204, KL Div: 4.2478\n",
            "241\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.4129, KL Div: 4.1842\n",
            "243\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 139.6772, KL Div: 4.2318\n",
            "245\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.8797, KL Div: 4.2923\n",
            "247\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.8436, KL Div: 4.2256\n",
            "249\n",
            "Epoch[25/25], Step [250/469], Reconst Loss: 145.3207, KL Div: 4.1532\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.8980, KL Div: 4.0899\n",
            "251\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.0010, KL Div: 4.3097\n",
            "253\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 149.7169, KL Div: 4.1772\n",
            "255\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.3816, KL Div: 4.1621\n",
            "257\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 137.3732, KL Div: 4.3326\n",
            "259\n",
            "Epoch[25/25], Step [260/469], Reconst Loss: 135.3703, KL Div: 4.1237\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.1651, KL Div: 4.2680\n",
            "261\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.3741, KL Div: 4.3039\n",
            "263\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.9062, KL Div: 4.0796\n",
            "265\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.3258, KL Div: 4.3259\n",
            "267\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.9650, KL Div: 4.2124\n",
            "269\n",
            "Epoch[25/25], Step [270/469], Reconst Loss: 146.0681, KL Div: 4.1153\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 135.1103, KL Div: 4.0250\n",
            "271\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.9733, KL Div: 4.0471\n",
            "273\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 152.9905, KL Div: 4.1947\n",
            "275\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 149.7054, KL Div: 4.1891\n",
            "277\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.2353, KL Div: 4.0579\n",
            "279\n",
            "Epoch[25/25], Step [280/469], Reconst Loss: 146.4824, KL Div: 4.3056\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.4004, KL Div: 4.2029\n",
            "281\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.5260, KL Div: 4.4494\n",
            "283\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.2580, KL Div: 4.1813\n",
            "285\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.5972, KL Div: 4.2735\n",
            "287\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.3242, KL Div: 4.2067\n",
            "289\n",
            "Epoch[25/25], Step [290/469], Reconst Loss: 142.9624, KL Div: 4.2235\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 134.4423, KL Div: 4.1664\n",
            "291\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.8340, KL Div: 4.3444\n",
            "293\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.5291, KL Div: 4.2669\n",
            "295\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.0674, KL Div: 4.2759\n",
            "297\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.9814, KL Div: 4.1922\n",
            "299\n",
            "Epoch[25/25], Step [300/469], Reconst Loss: 140.6414, KL Div: 4.0614\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.7859, KL Div: 4.1248\n",
            "301\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.8611, KL Div: 4.1195\n",
            "303\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.3371, KL Div: 4.0838\n",
            "305\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.8516, KL Div: 3.9976\n",
            "307\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.8066, KL Div: 4.1646\n",
            "309\n",
            "Epoch[25/25], Step [310/469], Reconst Loss: 147.0573, KL Div: 4.0875\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 137.4941, KL Div: 3.9008\n",
            "311\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.5394, KL Div: 4.2858\n",
            "313\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.7958, KL Div: 4.2433\n",
            "315\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.3869, KL Div: 4.0736\n",
            "317\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.9010, KL Div: 4.3607\n",
            "319\n",
            "Epoch[25/25], Step [320/469], Reconst Loss: 141.9288, KL Div: 4.5161\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.7044, KL Div: 4.3733\n",
            "321\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.9217, KL Div: 4.3455\n",
            "323\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.6942, KL Div: 4.3571\n",
            "325\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.7766, KL Div: 4.3225\n",
            "327\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.2410, KL Div: 4.3105\n",
            "329\n",
            "Epoch[25/25], Step [330/469], Reconst Loss: 139.5544, KL Div: 4.3152\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.7892, KL Div: 4.1537\n",
            "331\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 137.4158, KL Div: 4.0913\n",
            "333\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.1034, KL Div: 4.3440\n",
            "335\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.0772, KL Div: 4.2228\n",
            "337\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 136.9969, KL Div: 4.1392\n",
            "339\n",
            "Epoch[25/25], Step [340/469], Reconst Loss: 143.2671, KL Div: 4.3929\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 150.7632, KL Div: 4.3888\n",
            "341\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 134.2776, KL Div: 4.0485\n",
            "343\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.6664, KL Div: 4.3898\n",
            "345\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.5385, KL Div: 4.1187\n",
            "347\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 149.2684, KL Div: 4.1154\n",
            "349\n",
            "Epoch[25/25], Step [350/469], Reconst Loss: 136.7587, KL Div: 4.0279\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 134.1588, KL Div: 4.0989\n",
            "351\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.5276, KL Div: 4.1546\n",
            "353\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.3296, KL Div: 4.2962\n",
            "355\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.6262, KL Div: 4.2768\n",
            "357\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.4093, KL Div: 4.2572\n",
            "359\n",
            "Epoch[25/25], Step [360/469], Reconst Loss: 139.0202, KL Div: 4.1039\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.6806, KL Div: 4.2762\n",
            "361\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.1592, KL Div: 4.2991\n",
            "363\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.9768, KL Div: 4.1650\n",
            "365\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 151.7892, KL Div: 4.3825\n",
            "367\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.4264, KL Div: 4.2272\n",
            "369\n",
            "Epoch[25/25], Step [370/469], Reconst Loss: 139.4158, KL Div: 4.0903\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.7174, KL Div: 4.2264\n",
            "371\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.9769, KL Div: 4.2697\n",
            "373\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.6621, KL Div: 4.3526\n",
            "375\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.5047, KL Div: 4.2354\n",
            "377\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.7525, KL Div: 4.1306\n",
            "379\n",
            "Epoch[25/25], Step [380/469], Reconst Loss: 138.2911, KL Div: 4.2826\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.9092, KL Div: 4.0953\n",
            "381\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 137.9524, KL Div: 4.5129\n",
            "383\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.7867, KL Div: 4.2130\n",
            "385\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.3622, KL Div: 4.1699\n",
            "387\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.7311, KL Div: 4.1022\n",
            "389\n",
            "Epoch[25/25], Step [390/469], Reconst Loss: 141.4047, KL Div: 3.9980\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.3320, KL Div: 4.1608\n",
            "391\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.3210, KL Div: 4.2274\n",
            "393\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.3460, KL Div: 4.0692\n",
            "395\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.8343, KL Div: 4.1291\n",
            "397\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.5536, KL Div: 4.0343\n",
            "399\n",
            "Epoch[25/25], Step [400/469], Reconst Loss: 152.8121, KL Div: 4.2606\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.2807, KL Div: 4.2257\n",
            "401\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.6458, KL Div: 3.9915\n",
            "403\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.6967, KL Div: 4.3205\n",
            "405\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.1062, KL Div: 4.1320\n",
            "407\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.6862, KL Div: 4.1076\n",
            "409\n",
            "Epoch[25/25], Step [410/469], Reconst Loss: 148.9219, KL Div: 4.1192\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.5341, KL Div: 4.2011\n",
            "411\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.2271, KL Div: 4.1081\n",
            "413\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.5096, KL Div: 4.5176\n",
            "415\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.5037, KL Div: 4.3406\n",
            "417\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 138.2932, KL Div: 4.2209\n",
            "419\n",
            "Epoch[25/25], Step [420/469], Reconst Loss: 145.4382, KL Div: 4.3262\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 135.8728, KL Div: 4.4388\n",
            "421\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.7742, KL Div: 4.1082\n",
            "423\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.7882, KL Div: 4.3237\n",
            "425\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 139.1688, KL Div: 4.3546\n",
            "427\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.2898, KL Div: 4.2144\n",
            "429\n",
            "Epoch[25/25], Step [430/469], Reconst Loss: 135.8634, KL Div: 4.1088\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 139.9885, KL Div: 4.1362\n",
            "431\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 146.1284, KL Div: 4.1897\n",
            "433\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 136.6308, KL Div: 4.3085\n",
            "435\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.5876, KL Div: 4.0239\n",
            "437\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.5357, KL Div: 4.0321\n",
            "439\n",
            "Epoch[25/25], Step [440/469], Reconst Loss: 141.0084, KL Div: 3.9209\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 144.0990, KL Div: 3.9902\n",
            "441\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 139.4576, KL Div: 3.9413\n",
            "443\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.3542, KL Div: 3.9597\n",
            "445\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.7838, KL Div: 4.1602\n",
            "447\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.3505, KL Div: 4.1440\n",
            "449\n",
            "Epoch[25/25], Step [450/469], Reconst Loss: 140.9854, KL Div: 4.0353\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.1493, KL Div: 4.2539\n",
            "451\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.7952, KL Div: 4.2220\n",
            "453\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 143.5974, KL Div: 4.3326\n",
            "455\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 142.8676, KL Div: 4.2603\n",
            "457\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 147.3224, KL Div: 4.3016\n",
            "459\n",
            "Epoch[25/25], Step [460/469], Reconst Loss: 141.6890, KL Div: 4.0737\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 132.2081, KL Div: 4.4335\n",
            "461\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 141.3111, KL Div: 4.3407\n",
            "463\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 140.0478, KL Div: 4.2711\n",
            "465\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 145.9643, KL Div: 4.2052\n",
            "467\n",
            "8\n",
            "9\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 105.5813, KL Div: 3.3348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beta Optimizer VAE Outputs\n"
      ],
      "metadata": {
        "id": "39ZTDAc02rX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beta Value Update for Epochs"
      ],
      "metadata": {
        "id": "CX3eiMBM2vG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(epoch_counts, epoch_betas, 'o--', color='grey', alpha=0.3)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('beta')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "AOjSoVytuHRF",
        "outputId": "8b23758f-c10d-4b81-971a-4640f9ac3c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beta Value Update for Iterations"
      ],
      "metadata": {
        "id": "L7k0TN462zTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(total_couns, iteration_betas, 'o--', color='grey', alpha=0.3)\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('beta')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "L0OzzyaAFnWe",
        "outputId": "8471cc15-cb18-4145-9fb2-b711470d4e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconstruction of Images using Beta Optimizer VAE"
      ],
      "metadata": {
        "id": "z_eIbLhq26qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_reconstruction(model1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "NZ5bWHtcToeb",
        "outputId": "a9cf8259-44f4-4ee6-dedb-d3732b4f3319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation of Images using Beta Optimizer VAE\n"
      ],
      "metadata": {
        "id": "FXPVYdB73IBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_generation(model1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Ujnu6wwhTpc2",
        "outputId": "8976b507-ac35-4c40-b525-038abd650fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normal VAE Training"
      ],
      "metadata": {
        "id": "HlwtYzpz3mJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##EDITED BY Abhijit\n",
        "## Code for the baseline\n",
        "\n",
        "\n",
        "epoch_loss_vae = []\n",
        "iteration_loss_vae = []\n",
        "for epoch in range(25):\n",
        "    epoch_loss = []\n",
        "    for i, (x, _) in enumerate(data_loader):\n",
        "        # Forward pass\n",
        "        x = x.to(device).view(-1, image_size)\n",
        "        x_reconst, mu, log_var = model2(x)\n",
        "        \n",
        "        # Compute reconstruction loss and kl divergence\n",
        "        # For KL divergence between Gaussians, see Appendix B in VAE paper or (Doersch, 2016):\n",
        "        # https://arxiv.org/abs/1606.05908\n",
        "        reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        loss = reconst_loss + kl_div\n",
        "        print(loss)\n",
        "        optimizer2.zero_grad()\n",
        "        loss.backward(retain_graph=False)\n",
        "        optimizer2.step()\n",
        "        \n",
        "        if (i+1) % 10 == 0:\n",
        "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
        "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size, kl_div.item()/batch_size))\n",
        "            \n",
        "        iteration_loss_vae.append(loss)\n",
        "    torch.cuda.empty_cache()\n",
        "    epoch_loss_vae.append(sum(iteration_loss_vae)/len(iteration_loss_vae))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ_t8S9esLS8",
        "outputId": "121ae58b-534d-47c5-a537-08e3738ff524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "tensor(13457.4668, grad_fn=<AddBackward0>)\n",
            "tensor(13206.5449, grad_fn=<AddBackward0>)\n",
            "tensor(13144.9980, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [140/469], Reconst Loss: 77.5214, KL Div: 25.1739\n",
            "tensor(13468.9316, grad_fn=<AddBackward0>)\n",
            "tensor(13552.2598, grad_fn=<AddBackward0>)\n",
            "tensor(13062.9297, grad_fn=<AddBackward0>)\n",
            "tensor(13831.3428, grad_fn=<AddBackward0>)\n",
            "tensor(13210.3154, grad_fn=<AddBackward0>)\n",
            "tensor(13387.6699, grad_fn=<AddBackward0>)\n",
            "tensor(13280.8027, grad_fn=<AddBackward0>)\n",
            "tensor(13702.1602, grad_fn=<AddBackward0>)\n",
            "tensor(13141.0156, grad_fn=<AddBackward0>)\n",
            "tensor(12977.0322, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [150/469], Reconst Loss: 76.3512, KL Div: 25.0318\n",
            "tensor(13462.9648, grad_fn=<AddBackward0>)\n",
            "tensor(13229.2988, grad_fn=<AddBackward0>)\n",
            "tensor(13339.3350, grad_fn=<AddBackward0>)\n",
            "tensor(13870.9717, grad_fn=<AddBackward0>)\n",
            "tensor(13210.3232, grad_fn=<AddBackward0>)\n",
            "tensor(13460.9902, grad_fn=<AddBackward0>)\n",
            "tensor(13114.2598, grad_fn=<AddBackward0>)\n",
            "tensor(13040.0996, grad_fn=<AddBackward0>)\n",
            "tensor(13785.3945, grad_fn=<AddBackward0>)\n",
            "tensor(13548.6006, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [160/469], Reconst Loss: 80.0577, KL Div: 25.7907\n",
            "tensor(12974.5107, grad_fn=<AddBackward0>)\n",
            "tensor(13486.7812, grad_fn=<AddBackward0>)\n",
            "tensor(13327.3232, grad_fn=<AddBackward0>)\n",
            "tensor(13205.1201, grad_fn=<AddBackward0>)\n",
            "tensor(12707.3496, grad_fn=<AddBackward0>)\n",
            "tensor(13533.6123, grad_fn=<AddBackward0>)\n",
            "tensor(13048.8594, grad_fn=<AddBackward0>)\n",
            "tensor(13542.8184, grad_fn=<AddBackward0>)\n",
            "tensor(13470.6738, grad_fn=<AddBackward0>)\n",
            "tensor(13145.5107, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [170/469], Reconst Loss: 77.5556, KL Div: 25.1437\n",
            "tensor(13160.8809, grad_fn=<AddBackward0>)\n",
            "tensor(13594.3447, grad_fn=<AddBackward0>)\n",
            "tensor(13139.7197, grad_fn=<AddBackward0>)\n",
            "tensor(12803.4766, grad_fn=<AddBackward0>)\n",
            "tensor(12961.7236, grad_fn=<AddBackward0>)\n",
            "tensor(13052.3936, grad_fn=<AddBackward0>)\n",
            "tensor(13283.4883, grad_fn=<AddBackward0>)\n",
            "tensor(13238.1650, grad_fn=<AddBackward0>)\n",
            "tensor(12758.1611, grad_fn=<AddBackward0>)\n",
            "tensor(13218.9189, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [180/469], Reconst Loss: 78.0648, KL Div: 25.2080\n",
            "tensor(13550.1260, grad_fn=<AddBackward0>)\n",
            "tensor(13960.0508, grad_fn=<AddBackward0>)\n",
            "tensor(13104.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13266.7734, grad_fn=<AddBackward0>)\n",
            "tensor(12726.1865, grad_fn=<AddBackward0>)\n",
            "tensor(13171.2012, grad_fn=<AddBackward0>)\n",
            "tensor(13364.3008, grad_fn=<AddBackward0>)\n",
            "tensor(13416.7480, grad_fn=<AddBackward0>)\n",
            "tensor(13405.7490, grad_fn=<AddBackward0>)\n",
            "tensor(13111.9277, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [190/469], Reconst Loss: 77.6784, KL Div: 24.7586\n",
            "tensor(13666.9531, grad_fn=<AddBackward0>)\n",
            "tensor(13017.9189, grad_fn=<AddBackward0>)\n",
            "tensor(12910.4434, grad_fn=<AddBackward0>)\n",
            "tensor(13483.9189, grad_fn=<AddBackward0>)\n",
            "tensor(13803.2178, grad_fn=<AddBackward0>)\n",
            "tensor(13056.6621, grad_fn=<AddBackward0>)\n",
            "tensor(13680.4590, grad_fn=<AddBackward0>)\n",
            "tensor(13369.2920, grad_fn=<AddBackward0>)\n",
            "tensor(13273.6123, grad_fn=<AddBackward0>)\n",
            "tensor(13410.9883, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [200/469], Reconst Loss: 79.3249, KL Div: 25.4485\n",
            "tensor(13508.0010, grad_fn=<AddBackward0>)\n",
            "tensor(13340.6338, grad_fn=<AddBackward0>)\n",
            "tensor(13667.9062, grad_fn=<AddBackward0>)\n",
            "tensor(13293.2168, grad_fn=<AddBackward0>)\n",
            "tensor(13140.6621, grad_fn=<AddBackward0>)\n",
            "tensor(12622.3418, grad_fn=<AddBackward0>)\n",
            "tensor(13191.4473, grad_fn=<AddBackward0>)\n",
            "tensor(13470.3184, grad_fn=<AddBackward0>)\n",
            "tensor(13540.5547, grad_fn=<AddBackward0>)\n",
            "tensor(13189.4160, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [210/469], Reconst Loss: 78.1407, KL Div: 24.9016\n",
            "tensor(13372.4805, grad_fn=<AddBackward0>)\n",
            "tensor(13213.8223, grad_fn=<AddBackward0>)\n",
            "tensor(13362.0371, grad_fn=<AddBackward0>)\n",
            "tensor(12701.2637, grad_fn=<AddBackward0>)\n",
            "tensor(13062.5820, grad_fn=<AddBackward0>)\n",
            "tensor(13537.4023, grad_fn=<AddBackward0>)\n",
            "tensor(13169.3477, grad_fn=<AddBackward0>)\n",
            "tensor(12980.4092, grad_fn=<AddBackward0>)\n",
            "tensor(13179.3066, grad_fn=<AddBackward0>)\n",
            "tensor(13167.0176, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [220/469], Reconst Loss: 77.2476, KL Div: 25.6197\n",
            "tensor(13086.0625, grad_fn=<AddBackward0>)\n",
            "tensor(13030.3125, grad_fn=<AddBackward0>)\n",
            "tensor(12991.5781, grad_fn=<AddBackward0>)\n",
            "tensor(13380.3213, grad_fn=<AddBackward0>)\n",
            "tensor(13356.8984, grad_fn=<AddBackward0>)\n",
            "tensor(13433.8887, grad_fn=<AddBackward0>)\n",
            "tensor(13084.0957, grad_fn=<AddBackward0>)\n",
            "tensor(13126.3027, grad_fn=<AddBackward0>)\n",
            "tensor(13307.2148, grad_fn=<AddBackward0>)\n",
            "tensor(13126.6904, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [230/469], Reconst Loss: 77.0093, KL Div: 25.5430\n",
            "tensor(13670.6406, grad_fn=<AddBackward0>)\n",
            "tensor(13091.3662, grad_fn=<AddBackward0>)\n",
            "tensor(13358.7061, grad_fn=<AddBackward0>)\n",
            "tensor(13178.6006, grad_fn=<AddBackward0>)\n",
            "tensor(13823.6855, grad_fn=<AddBackward0>)\n",
            "tensor(12654.1807, grad_fn=<AddBackward0>)\n",
            "tensor(13529.0244, grad_fn=<AddBackward0>)\n",
            "tensor(13290.5049, grad_fn=<AddBackward0>)\n",
            "tensor(13234.3252, grad_fn=<AddBackward0>)\n",
            "tensor(12964.1953, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [240/469], Reconst Loss: 76.0297, KL Div: 25.2531\n",
            "tensor(13424.1367, grad_fn=<AddBackward0>)\n",
            "tensor(13514.5156, grad_fn=<AddBackward0>)\n",
            "tensor(13033.0752, grad_fn=<AddBackward0>)\n",
            "tensor(13364.6172, grad_fn=<AddBackward0>)\n",
            "tensor(13519.6973, grad_fn=<AddBackward0>)\n",
            "tensor(13894.3105, grad_fn=<AddBackward0>)\n",
            "tensor(13341.4316, grad_fn=<AddBackward0>)\n",
            "tensor(13639.7959, grad_fn=<AddBackward0>)\n",
            "tensor(13380.6914, grad_fn=<AddBackward0>)\n",
            "tensor(13156.8633, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [250/469], Reconst Loss: 77.4405, KL Div: 25.3475\n",
            "tensor(13502.4199, grad_fn=<AddBackward0>)\n",
            "tensor(13831.2344, grad_fn=<AddBackward0>)\n",
            "tensor(13302.0977, grad_fn=<AddBackward0>)\n",
            "tensor(13061.7754, grad_fn=<AddBackward0>)\n",
            "tensor(12900.1963, grad_fn=<AddBackward0>)\n",
            "tensor(13068.1230, grad_fn=<AddBackward0>)\n",
            "tensor(13439.7178, grad_fn=<AddBackward0>)\n",
            "tensor(13510.8604, grad_fn=<AddBackward0>)\n",
            "tensor(13632.2148, grad_fn=<AddBackward0>)\n",
            "tensor(13065.3984, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [260/469], Reconst Loss: 77.0686, KL Div: 25.0048\n",
            "tensor(12871.4277, grad_fn=<AddBackward0>)\n",
            "tensor(13561.9922, grad_fn=<AddBackward0>)\n",
            "tensor(13631.8057, grad_fn=<AddBackward0>)\n",
            "tensor(13311.9316, grad_fn=<AddBackward0>)\n",
            "tensor(13032.9199, grad_fn=<AddBackward0>)\n",
            "tensor(13156.6660, grad_fn=<AddBackward0>)\n",
            "tensor(13200.1885, grad_fn=<AddBackward0>)\n",
            "tensor(13124.7695, grad_fn=<AddBackward0>)\n",
            "tensor(13103.2109, grad_fn=<AddBackward0>)\n",
            "tensor(13092.4287, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [270/469], Reconst Loss: 76.7007, KL Div: 25.5839\n",
            "tensor(13252.1045, grad_fn=<AddBackward0>)\n",
            "tensor(13382.1504, grad_fn=<AddBackward0>)\n",
            "tensor(13472.8691, grad_fn=<AddBackward0>)\n",
            "tensor(13283.7314, grad_fn=<AddBackward0>)\n",
            "tensor(13264.4727, grad_fn=<AddBackward0>)\n",
            "tensor(13678.3936, grad_fn=<AddBackward0>)\n",
            "tensor(13085.9160, grad_fn=<AddBackward0>)\n",
            "tensor(13328.2617, grad_fn=<AddBackward0>)\n",
            "tensor(12821.4844, grad_fn=<AddBackward0>)\n",
            "tensor(13221.3047, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [280/469], Reconst Loss: 78.3823, KL Div: 24.9091\n",
            "tensor(13452.0859, grad_fn=<AddBackward0>)\n",
            "tensor(13091.7637, grad_fn=<AddBackward0>)\n",
            "tensor(13967.1611, grad_fn=<AddBackward0>)\n",
            "tensor(13286.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13566.6748, grad_fn=<AddBackward0>)\n",
            "tensor(13386.8662, grad_fn=<AddBackward0>)\n",
            "tensor(13386.8750, grad_fn=<AddBackward0>)\n",
            "tensor(13250.8779, grad_fn=<AddBackward0>)\n",
            "tensor(13315.8613, grad_fn=<AddBackward0>)\n",
            "tensor(13002.8145, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [290/469], Reconst Loss: 75.9394, KL Div: 25.6450\n",
            "tensor(13364.9004, grad_fn=<AddBackward0>)\n",
            "tensor(13115.5146, grad_fn=<AddBackward0>)\n",
            "tensor(13314.6143, grad_fn=<AddBackward0>)\n",
            "tensor(13368.3496, grad_fn=<AddBackward0>)\n",
            "tensor(13259.0137, grad_fn=<AddBackward0>)\n",
            "tensor(12990.2285, grad_fn=<AddBackward0>)\n",
            "tensor(13548.0391, grad_fn=<AddBackward0>)\n",
            "tensor(13416.9443, grad_fn=<AddBackward0>)\n",
            "tensor(13370.6270, grad_fn=<AddBackward0>)\n",
            "tensor(13218.0859, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [300/469], Reconst Loss: 77.4998, KL Div: 25.7665\n",
            "tensor(13179.6924, grad_fn=<AddBackward0>)\n",
            "tensor(13611.9121, grad_fn=<AddBackward0>)\n",
            "tensor(13341.4053, grad_fn=<AddBackward0>)\n",
            "tensor(13523.5664, grad_fn=<AddBackward0>)\n",
            "tensor(13825.9658, grad_fn=<AddBackward0>)\n",
            "tensor(13337.7676, grad_fn=<AddBackward0>)\n",
            "tensor(12652.9648, grad_fn=<AddBackward0>)\n",
            "tensor(13232.4502, grad_fn=<AddBackward0>)\n",
            "tensor(13223.7012, grad_fn=<AddBackward0>)\n",
            "tensor(13350.0488, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [310/469], Reconst Loss: 79.0573, KL Div: 25.2400\n",
            "tensor(13238.3779, grad_fn=<AddBackward0>)\n",
            "tensor(13622.4590, grad_fn=<AddBackward0>)\n",
            "tensor(13203.6934, grad_fn=<AddBackward0>)\n",
            "tensor(12852.6846, grad_fn=<AddBackward0>)\n",
            "tensor(13423.4092, grad_fn=<AddBackward0>)\n",
            "tensor(13321.2217, grad_fn=<AddBackward0>)\n",
            "tensor(13334.7666, grad_fn=<AddBackward0>)\n",
            "tensor(13853.6055, grad_fn=<AddBackward0>)\n",
            "tensor(13057.6406, grad_fn=<AddBackward0>)\n",
            "tensor(12729.5879, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [320/469], Reconst Loss: 74.5816, KL Div: 24.8683\n",
            "tensor(13006.7812, grad_fn=<AddBackward0>)\n",
            "tensor(13451.1084, grad_fn=<AddBackward0>)\n",
            "tensor(13377.0156, grad_fn=<AddBackward0>)\n",
            "tensor(13199.4746, grad_fn=<AddBackward0>)\n",
            "tensor(13489.4414, grad_fn=<AddBackward0>)\n",
            "tensor(12999.7422, grad_fn=<AddBackward0>)\n",
            "tensor(13564.2344, grad_fn=<AddBackward0>)\n",
            "tensor(12829.2422, grad_fn=<AddBackward0>)\n",
            "tensor(13162.9209, grad_fn=<AddBackward0>)\n",
            "tensor(13643.7119, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [330/469], Reconst Loss: 81.3021, KL Div: 25.2894\n",
            "tensor(13382.8164, grad_fn=<AddBackward0>)\n",
            "tensor(13755.2646, grad_fn=<AddBackward0>)\n",
            "tensor(13186.4922, grad_fn=<AddBackward0>)\n",
            "tensor(13267.9023, grad_fn=<AddBackward0>)\n",
            "tensor(13577.3926, grad_fn=<AddBackward0>)\n",
            "tensor(13434.3398, grad_fn=<AddBackward0>)\n",
            "tensor(13186.2334, grad_fn=<AddBackward0>)\n",
            "tensor(13548.8379, grad_fn=<AddBackward0>)\n",
            "tensor(12820.2207, grad_fn=<AddBackward0>)\n",
            "tensor(13432.3799, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [340/469], Reconst Loss: 79.5620, KL Div: 25.3785\n",
            "tensor(12646.0225, grad_fn=<AddBackward0>)\n",
            "tensor(13391.7461, grad_fn=<AddBackward0>)\n",
            "tensor(12993.2168, grad_fn=<AddBackward0>)\n",
            "tensor(13313.9238, grad_fn=<AddBackward0>)\n",
            "tensor(13422.2480, grad_fn=<AddBackward0>)\n",
            "tensor(13339.3408, grad_fn=<AddBackward0>)\n",
            "tensor(13204.6953, grad_fn=<AddBackward0>)\n",
            "tensor(13458.7344, grad_fn=<AddBackward0>)\n",
            "tensor(13184.8867, grad_fn=<AddBackward0>)\n",
            "tensor(13426.3945, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [350/469], Reconst Loss: 79.6190, KL Div: 25.2747\n",
            "tensor(13189.2549, grad_fn=<AddBackward0>)\n",
            "tensor(13348.2451, grad_fn=<AddBackward0>)\n",
            "tensor(13398.5586, grad_fn=<AddBackward0>)\n",
            "tensor(13395.4316, grad_fn=<AddBackward0>)\n",
            "tensor(13456.9727, grad_fn=<AddBackward0>)\n",
            "tensor(13244.3340, grad_fn=<AddBackward0>)\n",
            "tensor(13439.2900, grad_fn=<AddBackward0>)\n",
            "tensor(13400.7646, grad_fn=<AddBackward0>)\n",
            "tensor(13057.9844, grad_fn=<AddBackward0>)\n",
            "tensor(13249.6953, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [360/469], Reconst Loss: 77.8996, KL Div: 25.6137\n",
            "tensor(13131.5781, grad_fn=<AddBackward0>)\n",
            "tensor(13462.1875, grad_fn=<AddBackward0>)\n",
            "tensor(12801.3975, grad_fn=<AddBackward0>)\n",
            "tensor(13256.6934, grad_fn=<AddBackward0>)\n",
            "tensor(13489.0547, grad_fn=<AddBackward0>)\n",
            "tensor(12847.5625, grad_fn=<AddBackward0>)\n",
            "tensor(13300.6367, grad_fn=<AddBackward0>)\n",
            "tensor(13279.0488, grad_fn=<AddBackward0>)\n",
            "tensor(13757.9219, grad_fn=<AddBackward0>)\n",
            "tensor(13621.0625, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [370/469], Reconst Loss: 80.5639, KL Div: 25.8506\n",
            "tensor(13725.5215, grad_fn=<AddBackward0>)\n",
            "tensor(13610.1758, grad_fn=<AddBackward0>)\n",
            "tensor(13461.8877, grad_fn=<AddBackward0>)\n",
            "tensor(13005.0039, grad_fn=<AddBackward0>)\n",
            "tensor(13414.4004, grad_fn=<AddBackward0>)\n",
            "tensor(13375.3262, grad_fn=<AddBackward0>)\n",
            "tensor(13560.8193, grad_fn=<AddBackward0>)\n",
            "tensor(13404.3838, grad_fn=<AddBackward0>)\n",
            "tensor(13598.5898, grad_fn=<AddBackward0>)\n",
            "tensor(13272.1357, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [380/469], Reconst Loss: 77.7343, KL Div: 25.9543\n",
            "tensor(13425.0693, grad_fn=<AddBackward0>)\n",
            "tensor(12815.3770, grad_fn=<AddBackward0>)\n",
            "tensor(13345.4121, grad_fn=<AddBackward0>)\n",
            "tensor(13697.0088, grad_fn=<AddBackward0>)\n",
            "tensor(12960.4004, grad_fn=<AddBackward0>)\n",
            "tensor(13461.4600, grad_fn=<AddBackward0>)\n",
            "tensor(13057.2480, grad_fn=<AddBackward0>)\n",
            "tensor(13011.0723, grad_fn=<AddBackward0>)\n",
            "tensor(13063.2285, grad_fn=<AddBackward0>)\n",
            "tensor(13068.2432, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [390/469], Reconst Loss: 77.1255, KL Div: 24.9702\n",
            "tensor(13147.0381, grad_fn=<AddBackward0>)\n",
            "tensor(13485.6484, grad_fn=<AddBackward0>)\n",
            "tensor(13398.4199, grad_fn=<AddBackward0>)\n",
            "tensor(13140.2598, grad_fn=<AddBackward0>)\n",
            "tensor(13775.3691, grad_fn=<AddBackward0>)\n",
            "tensor(13482.8867, grad_fn=<AddBackward0>)\n",
            "tensor(13392.9395, grad_fn=<AddBackward0>)\n",
            "tensor(13186.5605, grad_fn=<AddBackward0>)\n",
            "tensor(14151.0400, grad_fn=<AddBackward0>)\n",
            "tensor(13189.5332, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [400/469], Reconst Loss: 77.2652, KL Div: 25.7780\n",
            "tensor(13258.5713, grad_fn=<AddBackward0>)\n",
            "tensor(13678.0312, grad_fn=<AddBackward0>)\n",
            "tensor(13658.4150, grad_fn=<AddBackward0>)\n",
            "tensor(13214.6592, grad_fn=<AddBackward0>)\n",
            "tensor(13470.2188, grad_fn=<AddBackward0>)\n",
            "tensor(13024.0820, grad_fn=<AddBackward0>)\n",
            "tensor(13611.2617, grad_fn=<AddBackward0>)\n",
            "tensor(13624.3652, grad_fn=<AddBackward0>)\n",
            "tensor(13189.7676, grad_fn=<AddBackward0>)\n",
            "tensor(13515.0020, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [410/469], Reconst Loss: 80.0857, KL Div: 25.5003\n",
            "tensor(13708.6230, grad_fn=<AddBackward0>)\n",
            "tensor(12548.8877, grad_fn=<AddBackward0>)\n",
            "tensor(13703.2383, grad_fn=<AddBackward0>)\n",
            "tensor(13127.7334, grad_fn=<AddBackward0>)\n",
            "tensor(13193.4053, grad_fn=<AddBackward0>)\n",
            "tensor(13171.7500, grad_fn=<AddBackward0>)\n",
            "tensor(13306.2139, grad_fn=<AddBackward0>)\n",
            "tensor(13122.6982, grad_fn=<AddBackward0>)\n",
            "tensor(13472.6445, grad_fn=<AddBackward0>)\n",
            "tensor(13520.8564, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [420/469], Reconst Loss: 78.9760, KL Div: 26.6557\n",
            "tensor(13124.8730, grad_fn=<AddBackward0>)\n",
            "tensor(13451.9795, grad_fn=<AddBackward0>)\n",
            "tensor(12565.8926, grad_fn=<AddBackward0>)\n",
            "tensor(13343.0322, grad_fn=<AddBackward0>)\n",
            "tensor(13410.8408, grad_fn=<AddBackward0>)\n",
            "tensor(13445.3271, grad_fn=<AddBackward0>)\n",
            "tensor(12601.4355, grad_fn=<AddBackward0>)\n",
            "tensor(13288.5176, grad_fn=<AddBackward0>)\n",
            "tensor(13062.5801, grad_fn=<AddBackward0>)\n",
            "tensor(12738.8242, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [430/469], Reconst Loss: 75.0713, KL Div: 24.4508\n",
            "tensor(13684.9990, grad_fn=<AddBackward0>)\n",
            "tensor(13358.3594, grad_fn=<AddBackward0>)\n",
            "tensor(12732.2559, grad_fn=<AddBackward0>)\n",
            "tensor(13283.7334, grad_fn=<AddBackward0>)\n",
            "tensor(13132.4668, grad_fn=<AddBackward0>)\n",
            "tensor(12424.6924, grad_fn=<AddBackward0>)\n",
            "tensor(13221.1133, grad_fn=<AddBackward0>)\n",
            "tensor(13148.7812, grad_fn=<AddBackward0>)\n",
            "tensor(13278.3496, grad_fn=<AddBackward0>)\n",
            "tensor(13253.1270, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [440/469], Reconst Loss: 77.3499, KL Div: 26.1902\n",
            "tensor(13114.9873, grad_fn=<AddBackward0>)\n",
            "tensor(13066.8018, grad_fn=<AddBackward0>)\n",
            "tensor(13124.6465, grad_fn=<AddBackward0>)\n",
            "tensor(13514.0215, grad_fn=<AddBackward0>)\n",
            "tensor(13140.5830, grad_fn=<AddBackward0>)\n",
            "tensor(12913.2891, grad_fn=<AddBackward0>)\n",
            "tensor(12750.7725, grad_fn=<AddBackward0>)\n",
            "tensor(12909.7910, grad_fn=<AddBackward0>)\n",
            "tensor(13305.6699, grad_fn=<AddBackward0>)\n",
            "tensor(13105.0312, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [450/469], Reconst Loss: 78.1977, KL Div: 24.1854\n",
            "tensor(13376.8359, grad_fn=<AddBackward0>)\n",
            "tensor(13796.2344, grad_fn=<AddBackward0>)\n",
            "tensor(13753.4424, grad_fn=<AddBackward0>)\n",
            "tensor(13213.4648, grad_fn=<AddBackward0>)\n",
            "tensor(12926.4824, grad_fn=<AddBackward0>)\n",
            "tensor(13327.3984, grad_fn=<AddBackward0>)\n",
            "tensor(13323.7393, grad_fn=<AddBackward0>)\n",
            "tensor(13720.7031, grad_fn=<AddBackward0>)\n",
            "tensor(13420.0244, grad_fn=<AddBackward0>)\n",
            "tensor(13474.9375, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [460/469], Reconst Loss: 79.7327, KL Div: 25.5402\n",
            "tensor(12986.3955, grad_fn=<AddBackward0>)\n",
            "tensor(13171.2090, grad_fn=<AddBackward0>)\n",
            "tensor(13553.7236, grad_fn=<AddBackward0>)\n",
            "tensor(13537.7051, grad_fn=<AddBackward0>)\n",
            "tensor(12624.0371, grad_fn=<AddBackward0>)\n",
            "tensor(13380.6074, grad_fn=<AddBackward0>)\n",
            "tensor(12590.8887, grad_fn=<AddBackward0>)\n",
            "tensor(12971.3330, grad_fn=<AddBackward0>)\n",
            "tensor(9991.2275, grad_fn=<AddBackward0>)\n",
            "tensor(13500.4150, grad_fn=<AddBackward0>)\n",
            "tensor(13414.9463, grad_fn=<AddBackward0>)\n",
            "tensor(13688.2148, grad_fn=<AddBackward0>)\n",
            "tensor(13107.3428, grad_fn=<AddBackward0>)\n",
            "tensor(13358.0898, grad_fn=<AddBackward0>)\n",
            "tensor(12971.6426, grad_fn=<AddBackward0>)\n",
            "tensor(13773.2168, grad_fn=<AddBackward0>)\n",
            "tensor(13135.8438, grad_fn=<AddBackward0>)\n",
            "tensor(13176.1201, grad_fn=<AddBackward0>)\n",
            "tensor(13185.2627, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [10/469], Reconst Loss: 77.9495, KL Div: 25.0604\n",
            "tensor(13492.4004, grad_fn=<AddBackward0>)\n",
            "tensor(13870.2705, grad_fn=<AddBackward0>)\n",
            "tensor(13267.5312, grad_fn=<AddBackward0>)\n",
            "tensor(13326.2891, grad_fn=<AddBackward0>)\n",
            "tensor(13251.2812, grad_fn=<AddBackward0>)\n",
            "tensor(13194.2275, grad_fn=<AddBackward0>)\n",
            "tensor(12627.8506, grad_fn=<AddBackward0>)\n",
            "tensor(13213.6992, grad_fn=<AddBackward0>)\n",
            "tensor(13376.9912, grad_fn=<AddBackward0>)\n",
            "tensor(13236.9395, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [20/469], Reconst Loss: 77.9100, KL Div: 25.5036\n",
            "tensor(12950.3232, grad_fn=<AddBackward0>)\n",
            "tensor(13198.9775, grad_fn=<AddBackward0>)\n",
            "tensor(12826.2461, grad_fn=<AddBackward0>)\n",
            "tensor(13249.6172, grad_fn=<AddBackward0>)\n",
            "tensor(13074.8711, grad_fn=<AddBackward0>)\n",
            "tensor(13311.1289, grad_fn=<AddBackward0>)\n",
            "tensor(12848.8809, grad_fn=<AddBackward0>)\n",
            "tensor(13194.3828, grad_fn=<AddBackward0>)\n",
            "tensor(13327.9355, grad_fn=<AddBackward0>)\n",
            "tensor(12552.0449, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [30/469], Reconst Loss: 73.4945, KL Div: 24.5683\n",
            "tensor(13295.4092, grad_fn=<AddBackward0>)\n",
            "tensor(13310.9229, grad_fn=<AddBackward0>)\n",
            "tensor(13393.4492, grad_fn=<AddBackward0>)\n",
            "tensor(12592.8613, grad_fn=<AddBackward0>)\n",
            "tensor(13621.2930, grad_fn=<AddBackward0>)\n",
            "tensor(13412.2422, grad_fn=<AddBackward0>)\n",
            "tensor(12840.2148, grad_fn=<AddBackward0>)\n",
            "tensor(13028.2275, grad_fn=<AddBackward0>)\n",
            "tensor(13119.7500, grad_fn=<AddBackward0>)\n",
            "tensor(13318.0254, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [40/469], Reconst Loss: 78.2167, KL Div: 25.8304\n",
            "tensor(13140.6484, grad_fn=<AddBackward0>)\n",
            "tensor(13007.1582, grad_fn=<AddBackward0>)\n",
            "tensor(13253.0674, grad_fn=<AddBackward0>)\n",
            "tensor(13615.8115, grad_fn=<AddBackward0>)\n",
            "tensor(13626.4727, grad_fn=<AddBackward0>)\n",
            "tensor(13448.9922, grad_fn=<AddBackward0>)\n",
            "tensor(13463.1338, grad_fn=<AddBackward0>)\n",
            "tensor(12789.5850, grad_fn=<AddBackward0>)\n",
            "tensor(12801.7910, grad_fn=<AddBackward0>)\n",
            "tensor(13429.1494, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [50/469], Reconst Loss: 79.0936, KL Div: 25.8216\n",
            "tensor(13136.0332, grad_fn=<AddBackward0>)\n",
            "tensor(13525.8496, grad_fn=<AddBackward0>)\n",
            "tensor(13082.2646, grad_fn=<AddBackward0>)\n",
            "tensor(13370.9912, grad_fn=<AddBackward0>)\n",
            "tensor(12910.3916, grad_fn=<AddBackward0>)\n",
            "tensor(12887.8428, grad_fn=<AddBackward0>)\n",
            "tensor(13721.9111, grad_fn=<AddBackward0>)\n",
            "tensor(13727.0977, grad_fn=<AddBackward0>)\n",
            "tensor(13296.3203, grad_fn=<AddBackward0>)\n",
            "tensor(13648.6025, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [60/469], Reconst Loss: 80.7836, KL Div: 25.8461\n",
            "tensor(13504.0273, grad_fn=<AddBackward0>)\n",
            "tensor(13477.9844, grad_fn=<AddBackward0>)\n",
            "tensor(13713.9648, grad_fn=<AddBackward0>)\n",
            "tensor(13677.8154, grad_fn=<AddBackward0>)\n",
            "tensor(13332.8438, grad_fn=<AddBackward0>)\n",
            "tensor(13550.7383, grad_fn=<AddBackward0>)\n",
            "tensor(13211.2842, grad_fn=<AddBackward0>)\n",
            "tensor(12776.1182, grad_fn=<AddBackward0>)\n",
            "tensor(13798.0781, grad_fn=<AddBackward0>)\n",
            "tensor(13666.7031, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [70/469], Reconst Loss: 80.7735, KL Div: 25.9976\n",
            "tensor(13661.2051, grad_fn=<AddBackward0>)\n",
            "tensor(12685.7539, grad_fn=<AddBackward0>)\n",
            "tensor(12816.3945, grad_fn=<AddBackward0>)\n",
            "tensor(13691.2959, grad_fn=<AddBackward0>)\n",
            "tensor(13149.0449, grad_fn=<AddBackward0>)\n",
            "tensor(13256.0498, grad_fn=<AddBackward0>)\n",
            "tensor(13107.6123, grad_fn=<AddBackward0>)\n",
            "tensor(13289.0605, grad_fn=<AddBackward0>)\n",
            "tensor(13095.8047, grad_fn=<AddBackward0>)\n",
            "tensor(12896.6611, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [80/469], Reconst Loss: 75.5910, KL Div: 25.1642\n",
            "tensor(13078.8828, grad_fn=<AddBackward0>)\n",
            "tensor(12909.0117, grad_fn=<AddBackward0>)\n",
            "tensor(12777.3223, grad_fn=<AddBackward0>)\n",
            "tensor(13298.7246, grad_fn=<AddBackward0>)\n",
            "tensor(13738.4395, grad_fn=<AddBackward0>)\n",
            "tensor(13039.4688, grad_fn=<AddBackward0>)\n",
            "tensor(13413.7305, grad_fn=<AddBackward0>)\n",
            "tensor(12816.4277, grad_fn=<AddBackward0>)\n",
            "tensor(12922.4961, grad_fn=<AddBackward0>)\n",
            "tensor(12902.5518, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [90/469], Reconst Loss: 76.2395, KL Div: 24.5617\n",
            "tensor(13486.7002, grad_fn=<AddBackward0>)\n",
            "tensor(12816.5078, grad_fn=<AddBackward0>)\n",
            "tensor(12998.8867, grad_fn=<AddBackward0>)\n",
            "tensor(13334.8340, grad_fn=<AddBackward0>)\n",
            "tensor(13252.3203, grad_fn=<AddBackward0>)\n",
            "tensor(13088.4961, grad_fn=<AddBackward0>)\n",
            "tensor(12911.4092, grad_fn=<AddBackward0>)\n",
            "tensor(13808.1689, grad_fn=<AddBackward0>)\n",
            "tensor(13236.4629, grad_fn=<AddBackward0>)\n",
            "tensor(13474.1113, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [100/469], Reconst Loss: 79.6393, KL Div: 25.6272\n",
            "tensor(13587.1992, grad_fn=<AddBackward0>)\n",
            "tensor(13167.3574, grad_fn=<AddBackward0>)\n",
            "tensor(13183.0215, grad_fn=<AddBackward0>)\n",
            "tensor(13277.4287, grad_fn=<AddBackward0>)\n",
            "tensor(13826.2705, grad_fn=<AddBackward0>)\n",
            "tensor(12756.3408, grad_fn=<AddBackward0>)\n",
            "tensor(13570.3398, grad_fn=<AddBackward0>)\n",
            "tensor(13154.9648, grad_fn=<AddBackward0>)\n",
            "tensor(13175.8691, grad_fn=<AddBackward0>)\n",
            "tensor(13864.4990, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [110/469], Reconst Loss: 82.8446, KL Div: 25.4718\n",
            "tensor(12919.7891, grad_fn=<AddBackward0>)\n",
            "tensor(13308.3828, grad_fn=<AddBackward0>)\n",
            "tensor(13171.2051, grad_fn=<AddBackward0>)\n",
            "tensor(13255.4961, grad_fn=<AddBackward0>)\n",
            "tensor(13377.9551, grad_fn=<AddBackward0>)\n",
            "tensor(13237.0645, grad_fn=<AddBackward0>)\n",
            "tensor(12882.8125, grad_fn=<AddBackward0>)\n",
            "tensor(12558.9629, grad_fn=<AddBackward0>)\n",
            "tensor(13229.0684, grad_fn=<AddBackward0>)\n",
            "tensor(13009.1426, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [120/469], Reconst Loss: 76.7148, KL Div: 24.9191\n",
            "tensor(12901.0742, grad_fn=<AddBackward0>)\n",
            "tensor(13098.0684, grad_fn=<AddBackward0>)\n",
            "tensor(13453.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13473.7090, grad_fn=<AddBackward0>)\n",
            "tensor(13249.7422, grad_fn=<AddBackward0>)\n",
            "tensor(13081.3594, grad_fn=<AddBackward0>)\n",
            "tensor(13503.9199, grad_fn=<AddBackward0>)\n",
            "tensor(13066.5879, grad_fn=<AddBackward0>)\n",
            "tensor(13316.7236, grad_fn=<AddBackward0>)\n",
            "tensor(13069.4629, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [130/469], Reconst Loss: 76.4292, KL Div: 25.6760\n",
            "tensor(13126.2959, grad_fn=<AddBackward0>)\n",
            "tensor(13312.4785, grad_fn=<AddBackward0>)\n",
            "tensor(13184.0293, grad_fn=<AddBackward0>)\n",
            "tensor(12899.5605, grad_fn=<AddBackward0>)\n",
            "tensor(13508.2314, grad_fn=<AddBackward0>)\n",
            "tensor(12888.4355, grad_fn=<AddBackward0>)\n",
            "tensor(13419.4990, grad_fn=<AddBackward0>)\n",
            "tensor(12994.7744, grad_fn=<AddBackward0>)\n",
            "tensor(13452.2793, grad_fn=<AddBackward0>)\n",
            "tensor(14059.5371, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [140/469], Reconst Loss: 83.3377, KL Div: 26.5024\n",
            "tensor(13218.9727, grad_fn=<AddBackward0>)\n",
            "tensor(13781.6367, grad_fn=<AddBackward0>)\n",
            "tensor(13400.2041, grad_fn=<AddBackward0>)\n",
            "tensor(13690.4180, grad_fn=<AddBackward0>)\n",
            "tensor(13404.7383, grad_fn=<AddBackward0>)\n",
            "tensor(13019.8662, grad_fn=<AddBackward0>)\n",
            "tensor(13971.1250, grad_fn=<AddBackward0>)\n",
            "tensor(12804.6582, grad_fn=<AddBackward0>)\n",
            "tensor(13424.7158, grad_fn=<AddBackward0>)\n",
            "tensor(13527.3613, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [150/469], Reconst Loss: 80.4126, KL Div: 25.2699\n",
            "tensor(12927.2217, grad_fn=<AddBackward0>)\n",
            "tensor(13251.1162, grad_fn=<AddBackward0>)\n",
            "tensor(13216.0215, grad_fn=<AddBackward0>)\n",
            "tensor(13351.0479, grad_fn=<AddBackward0>)\n",
            "tensor(12978.9199, grad_fn=<AddBackward0>)\n",
            "tensor(12767.1602, grad_fn=<AddBackward0>)\n",
            "tensor(13679.1504, grad_fn=<AddBackward0>)\n",
            "tensor(13237.9336, grad_fn=<AddBackward0>)\n",
            "tensor(13016.9922, grad_fn=<AddBackward0>)\n",
            "tensor(13068.4062, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [160/469], Reconst Loss: 77.1417, KL Div: 24.9552\n",
            "tensor(13188.9082, grad_fn=<AddBackward0>)\n",
            "tensor(13692.5596, grad_fn=<AddBackward0>)\n",
            "tensor(13317.3848, grad_fn=<AddBackward0>)\n",
            "tensor(12982.9141, grad_fn=<AddBackward0>)\n",
            "tensor(12548.3838, grad_fn=<AddBackward0>)\n",
            "tensor(13431.2725, grad_fn=<AddBackward0>)\n",
            "tensor(13403.2461, grad_fn=<AddBackward0>)\n",
            "tensor(13089.4844, grad_fn=<AddBackward0>)\n",
            "tensor(12920.5723, grad_fn=<AddBackward0>)\n",
            "tensor(13810.5547, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [170/469], Reconst Loss: 82.0444, KL Div: 25.8505\n",
            "tensor(13245.2500, grad_fn=<AddBackward0>)\n",
            "tensor(12847.1357, grad_fn=<AddBackward0>)\n",
            "tensor(13577.1953, grad_fn=<AddBackward0>)\n",
            "tensor(13682.2158, grad_fn=<AddBackward0>)\n",
            "tensor(13155.4473, grad_fn=<AddBackward0>)\n",
            "tensor(13478.6152, grad_fn=<AddBackward0>)\n",
            "tensor(13385.8760, grad_fn=<AddBackward0>)\n",
            "tensor(12724.2871, grad_fn=<AddBackward0>)\n",
            "tensor(13331.6084, grad_fn=<AddBackward0>)\n",
            "tensor(12790.5410, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [180/469], Reconst Loss: 75.2243, KL Div: 24.7018\n",
            "tensor(13265.6543, grad_fn=<AddBackward0>)\n",
            "tensor(13284.8936, grad_fn=<AddBackward0>)\n",
            "tensor(13253.5488, grad_fn=<AddBackward0>)\n",
            "tensor(13279.5801, grad_fn=<AddBackward0>)\n",
            "tensor(13451.6543, grad_fn=<AddBackward0>)\n",
            "tensor(13316.5430, grad_fn=<AddBackward0>)\n",
            "tensor(13596.8818, grad_fn=<AddBackward0>)\n",
            "tensor(13378.9512, grad_fn=<AddBackward0>)\n",
            "tensor(13500.5586, grad_fn=<AddBackward0>)\n",
            "tensor(13370.9297, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [190/469], Reconst Loss: 79.0594, KL Div: 25.4010\n",
            "tensor(13250.9512, grad_fn=<AddBackward0>)\n",
            "tensor(13501.1777, grad_fn=<AddBackward0>)\n",
            "tensor(13363.6777, grad_fn=<AddBackward0>)\n",
            "tensor(13292.8691, grad_fn=<AddBackward0>)\n",
            "tensor(13366.8877, grad_fn=<AddBackward0>)\n",
            "tensor(13521.1006, grad_fn=<AddBackward0>)\n",
            "tensor(13339.1602, grad_fn=<AddBackward0>)\n",
            "tensor(13747.9404, grad_fn=<AddBackward0>)\n",
            "tensor(13433.5977, grad_fn=<AddBackward0>)\n",
            "tensor(13250.7578, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [200/469], Reconst Loss: 77.6934, KL Div: 25.8282\n",
            "tensor(13465.2949, grad_fn=<AddBackward0>)\n",
            "tensor(13512.5664, grad_fn=<AddBackward0>)\n",
            "tensor(13538.0996, grad_fn=<AddBackward0>)\n",
            "tensor(13526.1367, grad_fn=<AddBackward0>)\n",
            "tensor(12980.0117, grad_fn=<AddBackward0>)\n",
            "tensor(13054.0352, grad_fn=<AddBackward0>)\n",
            "tensor(13131.9004, grad_fn=<AddBackward0>)\n",
            "tensor(13128.6104, grad_fn=<AddBackward0>)\n",
            "tensor(13217.5098, grad_fn=<AddBackward0>)\n",
            "tensor(13612.6250, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [210/469], Reconst Loss: 80.9790, KL Div: 25.3697\n",
            "tensor(13162.9189, grad_fn=<AddBackward0>)\n",
            "tensor(13352.9961, grad_fn=<AddBackward0>)\n",
            "tensor(12866.5146, grad_fn=<AddBackward0>)\n",
            "tensor(12923.5391, grad_fn=<AddBackward0>)\n",
            "tensor(13426.9580, grad_fn=<AddBackward0>)\n",
            "tensor(13431.0566, grad_fn=<AddBackward0>)\n",
            "tensor(13018.2637, grad_fn=<AddBackward0>)\n",
            "tensor(13087.6426, grad_fn=<AddBackward0>)\n",
            "tensor(13353.4697, grad_fn=<AddBackward0>)\n",
            "tensor(12565.9580, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [220/469], Reconst Loss: 73.8815, KL Div: 24.2901\n",
            "tensor(13755.1816, grad_fn=<AddBackward0>)\n",
            "tensor(13623.0078, grad_fn=<AddBackward0>)\n",
            "tensor(13125.3408, grad_fn=<AddBackward0>)\n",
            "tensor(13067.6934, grad_fn=<AddBackward0>)\n",
            "tensor(13058.2480, grad_fn=<AddBackward0>)\n",
            "tensor(13736.4980, grad_fn=<AddBackward0>)\n",
            "tensor(12825.2520, grad_fn=<AddBackward0>)\n",
            "tensor(12880.5049, grad_fn=<AddBackward0>)\n",
            "tensor(13011.5352, grad_fn=<AddBackward0>)\n",
            "tensor(13421.5918, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [230/469], Reconst Loss: 78.9128, KL Div: 25.9434\n",
            "tensor(13990.4893, grad_fn=<AddBackward0>)\n",
            "tensor(13231.1699, grad_fn=<AddBackward0>)\n",
            "tensor(13508.2734, grad_fn=<AddBackward0>)\n",
            "tensor(13033.7871, grad_fn=<AddBackward0>)\n",
            "tensor(13238.5830, grad_fn=<AddBackward0>)\n",
            "tensor(13320.9424, grad_fn=<AddBackward0>)\n",
            "tensor(13926.8418, grad_fn=<AddBackward0>)\n",
            "tensor(13708.8174, grad_fn=<AddBackward0>)\n",
            "tensor(13000.2080, grad_fn=<AddBackward0>)\n",
            "tensor(13172.3223, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [240/469], Reconst Loss: 77.3592, KL Div: 25.5495\n",
            "tensor(13574.0986, grad_fn=<AddBackward0>)\n",
            "tensor(12695.2236, grad_fn=<AddBackward0>)\n",
            "tensor(12875.5312, grad_fn=<AddBackward0>)\n",
            "tensor(13005.1182, grad_fn=<AddBackward0>)\n",
            "tensor(13294.6113, grad_fn=<AddBackward0>)\n",
            "tensor(13582.0898, grad_fn=<AddBackward0>)\n",
            "tensor(13296.6787, grad_fn=<AddBackward0>)\n",
            "tensor(13492.5625, grad_fn=<AddBackward0>)\n",
            "tensor(13411.8164, grad_fn=<AddBackward0>)\n",
            "tensor(13267.1797, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [250/469], Reconst Loss: 77.9878, KL Div: 25.6621\n",
            "tensor(12822.1143, grad_fn=<AddBackward0>)\n",
            "tensor(13400.6973, grad_fn=<AddBackward0>)\n",
            "tensor(13303.5957, grad_fn=<AddBackward0>)\n",
            "tensor(13435.9941, grad_fn=<AddBackward0>)\n",
            "tensor(13012.1943, grad_fn=<AddBackward0>)\n",
            "tensor(13164.5293, grad_fn=<AddBackward0>)\n",
            "tensor(13268.1162, grad_fn=<AddBackward0>)\n",
            "tensor(13182.7363, grad_fn=<AddBackward0>)\n",
            "tensor(13438.1504, grad_fn=<AddBackward0>)\n",
            "tensor(13368.6768, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [260/469], Reconst Loss: 79.0665, KL Div: 25.3763\n",
            "tensor(13462.0645, grad_fn=<AddBackward0>)\n",
            "tensor(12868.9609, grad_fn=<AddBackward0>)\n",
            "tensor(13141.2480, grad_fn=<AddBackward0>)\n",
            "tensor(13510.0918, grad_fn=<AddBackward0>)\n",
            "tensor(13436.8184, grad_fn=<AddBackward0>)\n",
            "tensor(13414.3086, grad_fn=<AddBackward0>)\n",
            "tensor(13279.0791, grad_fn=<AddBackward0>)\n",
            "tensor(12949.7139, grad_fn=<AddBackward0>)\n",
            "tensor(13086.1846, grad_fn=<AddBackward0>)\n",
            "tensor(12722.5674, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [270/469], Reconst Loss: 74.9066, KL Div: 24.4884\n",
            "tensor(13408.4141, grad_fn=<AddBackward0>)\n",
            "tensor(13149.4375, grad_fn=<AddBackward0>)\n",
            "tensor(13440.3320, grad_fn=<AddBackward0>)\n",
            "tensor(13095.3223, grad_fn=<AddBackward0>)\n",
            "tensor(13115.2812, grad_fn=<AddBackward0>)\n",
            "tensor(13317.0674, grad_fn=<AddBackward0>)\n",
            "tensor(13475.8086, grad_fn=<AddBackward0>)\n",
            "tensor(13071.5322, grad_fn=<AddBackward0>)\n",
            "tensor(13488.1211, grad_fn=<AddBackward0>)\n",
            "tensor(13126.3428, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [280/469], Reconst Loss: 77.3882, KL Div: 25.1614\n",
            "tensor(13030.1328, grad_fn=<AddBackward0>)\n",
            "tensor(13474.0723, grad_fn=<AddBackward0>)\n",
            "tensor(13204.3652, grad_fn=<AddBackward0>)\n",
            "tensor(13164.8486, grad_fn=<AddBackward0>)\n",
            "tensor(12803.8369, grad_fn=<AddBackward0>)\n",
            "tensor(12650.7275, grad_fn=<AddBackward0>)\n",
            "tensor(13256.8242, grad_fn=<AddBackward0>)\n",
            "tensor(13537.8330, grad_fn=<AddBackward0>)\n",
            "tensor(12889.1426, grad_fn=<AddBackward0>)\n",
            "tensor(13568.1543, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [290/469], Reconst Loss: 80.1370, KL Div: 25.8642\n",
            "tensor(13348.0020, grad_fn=<AddBackward0>)\n",
            "tensor(13514.9590, grad_fn=<AddBackward0>)\n",
            "tensor(13077.4092, grad_fn=<AddBackward0>)\n",
            "tensor(13840.9639, grad_fn=<AddBackward0>)\n",
            "tensor(13092.2285, grad_fn=<AddBackward0>)\n",
            "tensor(13217.1934, grad_fn=<AddBackward0>)\n",
            "tensor(13037.2266, grad_fn=<AddBackward0>)\n",
            "tensor(13188.1367, grad_fn=<AddBackward0>)\n",
            "tensor(13670.4307, grad_fn=<AddBackward0>)\n",
            "tensor(12614.2998, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [300/469], Reconst Loss: 73.7212, KL Div: 24.8280\n",
            "tensor(13220.6973, grad_fn=<AddBackward0>)\n",
            "tensor(13482.2959, grad_fn=<AddBackward0>)\n",
            "tensor(12837.9609, grad_fn=<AddBackward0>)\n",
            "tensor(13028.3154, grad_fn=<AddBackward0>)\n",
            "tensor(13305.7461, grad_fn=<AddBackward0>)\n",
            "tensor(12774.0303, grad_fn=<AddBackward0>)\n",
            "tensor(13115.2471, grad_fn=<AddBackward0>)\n",
            "tensor(12955.9414, grad_fn=<AddBackward0>)\n",
            "tensor(13378.3125, grad_fn=<AddBackward0>)\n",
            "tensor(13105.4268, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [310/469], Reconst Loss: 77.6852, KL Div: 24.7010\n",
            "tensor(13197.2578, grad_fn=<AddBackward0>)\n",
            "tensor(13124.4971, grad_fn=<AddBackward0>)\n",
            "tensor(13436.7061, grad_fn=<AddBackward0>)\n",
            "tensor(13089.3750, grad_fn=<AddBackward0>)\n",
            "tensor(13776.7305, grad_fn=<AddBackward0>)\n",
            "tensor(13146.9365, grad_fn=<AddBackward0>)\n",
            "tensor(12697.4062, grad_fn=<AddBackward0>)\n",
            "tensor(13074.8564, grad_fn=<AddBackward0>)\n",
            "tensor(13129.5703, grad_fn=<AddBackward0>)\n",
            "tensor(13238.1387, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [320/469], Reconst Loss: 77.4025, KL Div: 26.0205\n",
            "tensor(13334.0127, grad_fn=<AddBackward0>)\n",
            "tensor(13635.3721, grad_fn=<AddBackward0>)\n",
            "tensor(13099.2129, grad_fn=<AddBackward0>)\n",
            "tensor(13200.5947, grad_fn=<AddBackward0>)\n",
            "tensor(12770.9141, grad_fn=<AddBackward0>)\n",
            "tensor(13123.1191, grad_fn=<AddBackward0>)\n",
            "tensor(13334.3271, grad_fn=<AddBackward0>)\n",
            "tensor(12676.3477, grad_fn=<AddBackward0>)\n",
            "tensor(13694.6152, grad_fn=<AddBackward0>)\n",
            "tensor(13296.3340, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [330/469], Reconst Loss: 78.3332, KL Div: 25.5444\n",
            "tensor(13196.9355, grad_fn=<AddBackward0>)\n",
            "tensor(12907.7139, grad_fn=<AddBackward0>)\n",
            "tensor(13814.0889, grad_fn=<AddBackward0>)\n",
            "tensor(13372.4180, grad_fn=<AddBackward0>)\n",
            "tensor(13572.2754, grad_fn=<AddBackward0>)\n",
            "tensor(13111.9307, grad_fn=<AddBackward0>)\n",
            "tensor(13008.7695, grad_fn=<AddBackward0>)\n",
            "tensor(13360.4590, grad_fn=<AddBackward0>)\n",
            "tensor(12900.8613, grad_fn=<AddBackward0>)\n",
            "tensor(13427.3594, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [340/469], Reconst Loss: 79.0572, KL Div: 25.8441\n",
            "tensor(13629.4756, grad_fn=<AddBackward0>)\n",
            "tensor(13648.9453, grad_fn=<AddBackward0>)\n",
            "tensor(13371.0010, grad_fn=<AddBackward0>)\n",
            "tensor(13015.5742, grad_fn=<AddBackward0>)\n",
            "tensor(13228.7168, grad_fn=<AddBackward0>)\n",
            "tensor(13292.1533, grad_fn=<AddBackward0>)\n",
            "tensor(13077.6094, grad_fn=<AddBackward0>)\n",
            "tensor(13028.9648, grad_fn=<AddBackward0>)\n",
            "tensor(13460.9316, grad_fn=<AddBackward0>)\n",
            "tensor(13158.0537, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [350/469], Reconst Loss: 76.9355, KL Div: 25.8618\n",
            "tensor(12613.1680, grad_fn=<AddBackward0>)\n",
            "tensor(13260.4707, grad_fn=<AddBackward0>)\n",
            "tensor(12937.7949, grad_fn=<AddBackward0>)\n",
            "tensor(13496.3584, grad_fn=<AddBackward0>)\n",
            "tensor(13041.7588, grad_fn=<AddBackward0>)\n",
            "tensor(13659.5264, grad_fn=<AddBackward0>)\n",
            "tensor(13558.3076, grad_fn=<AddBackward0>)\n",
            "tensor(13585.3408, grad_fn=<AddBackward0>)\n",
            "tensor(13447.4551, grad_fn=<AddBackward0>)\n",
            "tensor(13752.3867, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [360/469], Reconst Loss: 81.6320, KL Div: 25.8086\n",
            "tensor(13372.4375, grad_fn=<AddBackward0>)\n",
            "tensor(12966.6152, grad_fn=<AddBackward0>)\n",
            "tensor(12910.1025, grad_fn=<AddBackward0>)\n",
            "tensor(13742.8799, grad_fn=<AddBackward0>)\n",
            "tensor(13192.5195, grad_fn=<AddBackward0>)\n",
            "tensor(13582.1152, grad_fn=<AddBackward0>)\n",
            "tensor(13127.3105, grad_fn=<AddBackward0>)\n",
            "tensor(13186.1650, grad_fn=<AddBackward0>)\n",
            "tensor(13232.5645, grad_fn=<AddBackward0>)\n",
            "tensor(13048.0840, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [370/469], Reconst Loss: 76.6700, KL Div: 25.2681\n",
            "tensor(13103.7822, grad_fn=<AddBackward0>)\n",
            "tensor(13146.1553, grad_fn=<AddBackward0>)\n",
            "tensor(12681.3301, grad_fn=<AddBackward0>)\n",
            "tensor(13534.7598, grad_fn=<AddBackward0>)\n",
            "tensor(13265.1963, grad_fn=<AddBackward0>)\n",
            "tensor(13370.3555, grad_fn=<AddBackward0>)\n",
            "tensor(13444.3027, grad_fn=<AddBackward0>)\n",
            "tensor(13408.2109, grad_fn=<AddBackward0>)\n",
            "tensor(13902.6924, grad_fn=<AddBackward0>)\n",
            "tensor(13907.0684, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [380/469], Reconst Loss: 82.5478, KL Div: 26.1011\n",
            "tensor(13207.6934, grad_fn=<AddBackward0>)\n",
            "tensor(13565.8389, grad_fn=<AddBackward0>)\n",
            "tensor(13183.2314, grad_fn=<AddBackward0>)\n",
            "tensor(12820.2920, grad_fn=<AddBackward0>)\n",
            "tensor(13187.2197, grad_fn=<AddBackward0>)\n",
            "tensor(13268.5469, grad_fn=<AddBackward0>)\n",
            "tensor(13068.3467, grad_fn=<AddBackward0>)\n",
            "tensor(13577.8135, grad_fn=<AddBackward0>)\n",
            "tensor(13774.3721, grad_fn=<AddBackward0>)\n",
            "tensor(13395.8115, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [390/469], Reconst Loss: 79.4898, KL Div: 25.1650\n",
            "tensor(13169.8428, grad_fn=<AddBackward0>)\n",
            "tensor(13195.1016, grad_fn=<AddBackward0>)\n",
            "tensor(12927.7656, grad_fn=<AddBackward0>)\n",
            "tensor(13306.5449, grad_fn=<AddBackward0>)\n",
            "tensor(13552.3164, grad_fn=<AddBackward0>)\n",
            "tensor(13320.6777, grad_fn=<AddBackward0>)\n",
            "tensor(13281.9385, grad_fn=<AddBackward0>)\n",
            "tensor(13278.7051, grad_fn=<AddBackward0>)\n",
            "tensor(13283.2852, grad_fn=<AddBackward0>)\n",
            "tensor(13544.2383, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [400/469], Reconst Loss: 79.5071, KL Div: 26.3072\n",
            "tensor(13571.4785, grad_fn=<AddBackward0>)\n",
            "tensor(13495.8096, grad_fn=<AddBackward0>)\n",
            "tensor(13564.0322, grad_fn=<AddBackward0>)\n",
            "tensor(13453.3027, grad_fn=<AddBackward0>)\n",
            "tensor(13530.1621, grad_fn=<AddBackward0>)\n",
            "tensor(13452.5898, grad_fn=<AddBackward0>)\n",
            "tensor(13313.9395, grad_fn=<AddBackward0>)\n",
            "tensor(13066.1797, grad_fn=<AddBackward0>)\n",
            "tensor(13323.5156, grad_fn=<AddBackward0>)\n",
            "tensor(13333.2090, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [410/469], Reconst Loss: 78.6882, KL Div: 25.4775\n",
            "tensor(13092.5918, grad_fn=<AddBackward0>)\n",
            "tensor(13245.3320, grad_fn=<AddBackward0>)\n",
            "tensor(13412.7188, grad_fn=<AddBackward0>)\n",
            "tensor(13508.7598, grad_fn=<AddBackward0>)\n",
            "tensor(13485.3184, grad_fn=<AddBackward0>)\n",
            "tensor(13325.3613, grad_fn=<AddBackward0>)\n",
            "tensor(13418.8564, grad_fn=<AddBackward0>)\n",
            "tensor(12986.4688, grad_fn=<AddBackward0>)\n",
            "tensor(13756.6045, grad_fn=<AddBackward0>)\n",
            "tensor(13522.5176, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [420/469], Reconst Loss: 80.1248, KL Div: 25.5199\n",
            "tensor(12725.4590, grad_fn=<AddBackward0>)\n",
            "tensor(13212.1582, grad_fn=<AddBackward0>)\n",
            "tensor(13465.5352, grad_fn=<AddBackward0>)\n",
            "tensor(13654.1377, grad_fn=<AddBackward0>)\n",
            "tensor(12990.1680, grad_fn=<AddBackward0>)\n",
            "tensor(13233.2832, grad_fn=<AddBackward0>)\n",
            "tensor(13399.0742, grad_fn=<AddBackward0>)\n",
            "tensor(13427.1914, grad_fn=<AddBackward0>)\n",
            "tensor(12781.8115, grad_fn=<AddBackward0>)\n",
            "tensor(13167.3535, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [430/469], Reconst Loss: 77.4855, KL Div: 25.3845\n",
            "tensor(13427.6523, grad_fn=<AddBackward0>)\n",
            "tensor(13813.4482, grad_fn=<AddBackward0>)\n",
            "tensor(13226.3242, grad_fn=<AddBackward0>)\n",
            "tensor(13198.6172, grad_fn=<AddBackward0>)\n",
            "tensor(12876.5713, grad_fn=<AddBackward0>)\n",
            "tensor(13393.6621, grad_fn=<AddBackward0>)\n",
            "tensor(13189.3857, grad_fn=<AddBackward0>)\n",
            "tensor(13213.1172, grad_fn=<AddBackward0>)\n",
            "tensor(13653.4941, grad_fn=<AddBackward0>)\n",
            "tensor(13226.3057, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [440/469], Reconst Loss: 78.1536, KL Div: 25.1769\n",
            "tensor(13231.6025, grad_fn=<AddBackward0>)\n",
            "tensor(13581.3262, grad_fn=<AddBackward0>)\n",
            "tensor(13108.7188, grad_fn=<AddBackward0>)\n",
            "tensor(12791.5215, grad_fn=<AddBackward0>)\n",
            "tensor(13022.4863, grad_fn=<AddBackward0>)\n",
            "tensor(13104.5537, grad_fn=<AddBackward0>)\n",
            "tensor(12833.1553, grad_fn=<AddBackward0>)\n",
            "tensor(13521.9316, grad_fn=<AddBackward0>)\n",
            "tensor(13637.8438, grad_fn=<AddBackward0>)\n",
            "tensor(13160.3867, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [450/469], Reconst Loss: 77.7037, KL Div: 25.1118\n",
            "tensor(12947.1514, grad_fn=<AddBackward0>)\n",
            "tensor(13255.9170, grad_fn=<AddBackward0>)\n",
            "tensor(13499.6660, grad_fn=<AddBackward0>)\n",
            "tensor(13181.3828, grad_fn=<AddBackward0>)\n",
            "tensor(13573.5527, grad_fn=<AddBackward0>)\n",
            "tensor(13439.7861, grad_fn=<AddBackward0>)\n",
            "tensor(13471.0879, grad_fn=<AddBackward0>)\n",
            "tensor(13362.0908, grad_fn=<AddBackward0>)\n",
            "tensor(13316.3203, grad_fn=<AddBackward0>)\n",
            "tensor(13553.2607, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [460/469], Reconst Loss: 80.0188, KL Div: 25.8661\n",
            "tensor(14143.9795, grad_fn=<AddBackward0>)\n",
            "tensor(13165.8809, grad_fn=<AddBackward0>)\n",
            "tensor(13405.1631, grad_fn=<AddBackward0>)\n",
            "tensor(13095.3496, grad_fn=<AddBackward0>)\n",
            "tensor(13746.5127, grad_fn=<AddBackward0>)\n",
            "tensor(13391.7168, grad_fn=<AddBackward0>)\n",
            "tensor(13527.9199, grad_fn=<AddBackward0>)\n",
            "tensor(13497.2861, grad_fn=<AddBackward0>)\n",
            "tensor(10369.4561, grad_fn=<AddBackward0>)\n",
            "tensor(13300.0869, grad_fn=<AddBackward0>)\n",
            "tensor(13216.0303, grad_fn=<AddBackward0>)\n",
            "tensor(13315.6279, grad_fn=<AddBackward0>)\n",
            "tensor(13114.6035, grad_fn=<AddBackward0>)\n",
            "tensor(13513.0430, grad_fn=<AddBackward0>)\n",
            "tensor(13323.2959, grad_fn=<AddBackward0>)\n",
            "tensor(13387.8145, grad_fn=<AddBackward0>)\n",
            "tensor(12946.7598, grad_fn=<AddBackward0>)\n",
            "tensor(13633.1338, grad_fn=<AddBackward0>)\n",
            "tensor(12870.9990, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [10/469], Reconst Loss: 75.0198, KL Div: 25.5348\n",
            "tensor(12858.4502, grad_fn=<AddBackward0>)\n",
            "tensor(13493.7715, grad_fn=<AddBackward0>)\n",
            "tensor(13153.1729, grad_fn=<AddBackward0>)\n",
            "tensor(13149.3096, grad_fn=<AddBackward0>)\n",
            "tensor(13269.7832, grad_fn=<AddBackward0>)\n",
            "tensor(12897.1113, grad_fn=<AddBackward0>)\n",
            "tensor(13399.1387, grad_fn=<AddBackward0>)\n",
            "tensor(13283.1191, grad_fn=<AddBackward0>)\n",
            "tensor(13114.3242, grad_fn=<AddBackward0>)\n",
            "tensor(13279.5029, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [20/469], Reconst Loss: 78.8097, KL Div: 24.9364\n",
            "tensor(13413.0488, grad_fn=<AddBackward0>)\n",
            "tensor(13659.5273, grad_fn=<AddBackward0>)\n",
            "tensor(13265.6748, grad_fn=<AddBackward0>)\n",
            "tensor(12982.1855, grad_fn=<AddBackward0>)\n",
            "tensor(12666.6260, grad_fn=<AddBackward0>)\n",
            "tensor(13795.1289, grad_fn=<AddBackward0>)\n",
            "tensor(13028.7715, grad_fn=<AddBackward0>)\n",
            "tensor(13180.8047, grad_fn=<AddBackward0>)\n",
            "tensor(13294.5039, grad_fn=<AddBackward0>)\n",
            "tensor(13822.5742, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [30/469], Reconst Loss: 82.4307, KL Div: 25.5581\n",
            "tensor(13168.6426, grad_fn=<AddBackward0>)\n",
            "tensor(13302.5215, grad_fn=<AddBackward0>)\n",
            "tensor(12844.5039, grad_fn=<AddBackward0>)\n",
            "tensor(13465.2764, grad_fn=<AddBackward0>)\n",
            "tensor(13298.9678, grad_fn=<AddBackward0>)\n",
            "tensor(13184.8447, grad_fn=<AddBackward0>)\n",
            "tensor(13214.6768, grad_fn=<AddBackward0>)\n",
            "tensor(13286.6689, grad_fn=<AddBackward0>)\n",
            "tensor(13337.0752, grad_fn=<AddBackward0>)\n",
            "tensor(13353.9609, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [40/469], Reconst Loss: 78.4614, KL Div: 25.8664\n",
            "tensor(12464.7051, grad_fn=<AddBackward0>)\n",
            "tensor(13127.5117, grad_fn=<AddBackward0>)\n",
            "tensor(12984.3818, grad_fn=<AddBackward0>)\n",
            "tensor(13339.5977, grad_fn=<AddBackward0>)\n",
            "tensor(13390.6455, grad_fn=<AddBackward0>)\n",
            "tensor(13130.1113, grad_fn=<AddBackward0>)\n",
            "tensor(12942.5049, grad_fn=<AddBackward0>)\n",
            "tensor(14017.2100, grad_fn=<AddBackward0>)\n",
            "tensor(13210.0176, grad_fn=<AddBackward0>)\n",
            "tensor(13110.3887, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [50/469], Reconst Loss: 76.7538, KL Div: 25.6711\n",
            "tensor(13518.9541, grad_fn=<AddBackward0>)\n",
            "tensor(13677.2246, grad_fn=<AddBackward0>)\n",
            "tensor(12958.9902, grad_fn=<AddBackward0>)\n",
            "tensor(13460.5332, grad_fn=<AddBackward0>)\n",
            "tensor(13192.7100, grad_fn=<AddBackward0>)\n",
            "tensor(13194.8115, grad_fn=<AddBackward0>)\n",
            "tensor(12884.2852, grad_fn=<AddBackward0>)\n",
            "tensor(13579.7539, grad_fn=<AddBackward0>)\n",
            "tensor(13251.8389, grad_fn=<AddBackward0>)\n",
            "tensor(12964.4971, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [60/469], Reconst Loss: 76.2378, KL Div: 25.0473\n",
            "tensor(13218.9805, grad_fn=<AddBackward0>)\n",
            "tensor(13032.8242, grad_fn=<AddBackward0>)\n",
            "tensor(13314.6582, grad_fn=<AddBackward0>)\n",
            "tensor(13104.4355, grad_fn=<AddBackward0>)\n",
            "tensor(13426.4062, grad_fn=<AddBackward0>)\n",
            "tensor(13437.1387, grad_fn=<AddBackward0>)\n",
            "tensor(12393.1074, grad_fn=<AddBackward0>)\n",
            "tensor(13296.9990, grad_fn=<AddBackward0>)\n",
            "tensor(12715.4072, grad_fn=<AddBackward0>)\n",
            "tensor(13777.3477, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [70/469], Reconst Loss: 80.7667, KL Div: 26.8688\n",
            "tensor(13355.7070, grad_fn=<AddBackward0>)\n",
            "tensor(13288.8945, grad_fn=<AddBackward0>)\n",
            "tensor(12794.3926, grad_fn=<AddBackward0>)\n",
            "tensor(13120.6562, grad_fn=<AddBackward0>)\n",
            "tensor(13077.6924, grad_fn=<AddBackward0>)\n",
            "tensor(12839.9102, grad_fn=<AddBackward0>)\n",
            "tensor(13437.2236, grad_fn=<AddBackward0>)\n",
            "tensor(13209.1543, grad_fn=<AddBackward0>)\n",
            "tensor(13391.3926, grad_fn=<AddBackward0>)\n",
            "tensor(13548.9023, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [80/469], Reconst Loss: 80.1288, KL Div: 25.7220\n",
            "tensor(13365.5859, grad_fn=<AddBackward0>)\n",
            "tensor(12792.0898, grad_fn=<AddBackward0>)\n",
            "tensor(13022.2969, grad_fn=<AddBackward0>)\n",
            "tensor(13401.5176, grad_fn=<AddBackward0>)\n",
            "tensor(12886.2109, grad_fn=<AddBackward0>)\n",
            "tensor(13266.8213, grad_fn=<AddBackward0>)\n",
            "tensor(13577.4092, grad_fn=<AddBackward0>)\n",
            "tensor(13148.5020, grad_fn=<AddBackward0>)\n",
            "tensor(13096.4414, grad_fn=<AddBackward0>)\n",
            "tensor(13358.0625, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [90/469], Reconst Loss: 79.0245, KL Div: 25.3354\n",
            "tensor(13140.3926, grad_fn=<AddBackward0>)\n",
            "tensor(13250.8506, grad_fn=<AddBackward0>)\n",
            "tensor(13567.3730, grad_fn=<AddBackward0>)\n",
            "tensor(12789.1172, grad_fn=<AddBackward0>)\n",
            "tensor(13269.2031, grad_fn=<AddBackward0>)\n",
            "tensor(12911.8066, grad_fn=<AddBackward0>)\n",
            "tensor(12879.8271, grad_fn=<AddBackward0>)\n",
            "tensor(13392.1143, grad_fn=<AddBackward0>)\n",
            "tensor(13250.7275, grad_fn=<AddBackward0>)\n",
            "tensor(13453.2520, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [100/469], Reconst Loss: 79.1144, KL Div: 25.9891\n",
            "tensor(13878.6230, grad_fn=<AddBackward0>)\n",
            "tensor(13799.9424, grad_fn=<AddBackward0>)\n",
            "tensor(13266.0391, grad_fn=<AddBackward0>)\n",
            "tensor(13341.8564, grad_fn=<AddBackward0>)\n",
            "tensor(13221.4600, grad_fn=<AddBackward0>)\n",
            "tensor(13682.5098, grad_fn=<AddBackward0>)\n",
            "tensor(13330.7041, grad_fn=<AddBackward0>)\n",
            "tensor(13214.7686, grad_fn=<AddBackward0>)\n",
            "tensor(12880.2949, grad_fn=<AddBackward0>)\n",
            "tensor(13465.7793, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [110/469], Reconst Loss: 79.4635, KL Div: 25.7379\n",
            "tensor(13146.4883, grad_fn=<AddBackward0>)\n",
            "tensor(13926.3516, grad_fn=<AddBackward0>)\n",
            "tensor(13657.0889, grad_fn=<AddBackward0>)\n",
            "tensor(13323.9795, grad_fn=<AddBackward0>)\n",
            "tensor(12770.4902, grad_fn=<AddBackward0>)\n",
            "tensor(14256.6582, grad_fn=<AddBackward0>)\n",
            "tensor(13156.2314, grad_fn=<AddBackward0>)\n",
            "tensor(13538.3828, grad_fn=<AddBackward0>)\n",
            "tensor(13591.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13000.8770, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [120/469], Reconst Loss: 76.1995, KL Div: 25.3698\n",
            "tensor(13489.5430, grad_fn=<AddBackward0>)\n",
            "tensor(12883.3242, grad_fn=<AddBackward0>)\n",
            "tensor(13029.5420, grad_fn=<AddBackward0>)\n",
            "tensor(13389.3945, grad_fn=<AddBackward0>)\n",
            "tensor(13567.9434, grad_fn=<AddBackward0>)\n",
            "tensor(14032.9482, grad_fn=<AddBackward0>)\n",
            "tensor(13403.6572, grad_fn=<AddBackward0>)\n",
            "tensor(13377.6660, grad_fn=<AddBackward0>)\n",
            "tensor(13443.9551, grad_fn=<AddBackward0>)\n",
            "tensor(13114.3701, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [130/469], Reconst Loss: 77.1818, KL Div: 25.2742\n",
            "tensor(13457.4453, grad_fn=<AddBackward0>)\n",
            "tensor(12916.8545, grad_fn=<AddBackward0>)\n",
            "tensor(13068.7891, grad_fn=<AddBackward0>)\n",
            "tensor(13985.8926, grad_fn=<AddBackward0>)\n",
            "tensor(13599.3477, grad_fn=<AddBackward0>)\n",
            "tensor(13466.2393, grad_fn=<AddBackward0>)\n",
            "tensor(13451.2871, grad_fn=<AddBackward0>)\n",
            "tensor(12892.7178, grad_fn=<AddBackward0>)\n",
            "tensor(13752.4893, grad_fn=<AddBackward0>)\n",
            "tensor(13157.6562, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [140/469], Reconst Loss: 77.5575, KL Div: 25.2367\n",
            "tensor(13182.4785, grad_fn=<AddBackward0>)\n",
            "tensor(13424.9980, grad_fn=<AddBackward0>)\n",
            "tensor(13413.0742, grad_fn=<AddBackward0>)\n",
            "tensor(13509.4453, grad_fn=<AddBackward0>)\n",
            "tensor(13326.2725, grad_fn=<AddBackward0>)\n",
            "tensor(13278.9688, grad_fn=<AddBackward0>)\n",
            "tensor(13506.6592, grad_fn=<AddBackward0>)\n",
            "tensor(13740.9316, grad_fn=<AddBackward0>)\n",
            "tensor(13483.6143, grad_fn=<AddBackward0>)\n",
            "tensor(13077.3281, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [150/469], Reconst Loss: 76.8670, KL Div: 25.2996\n",
            "tensor(13078.6738, grad_fn=<AddBackward0>)\n",
            "tensor(13558.0195, grad_fn=<AddBackward0>)\n",
            "tensor(13700.3887, grad_fn=<AddBackward0>)\n",
            "tensor(13237.4648, grad_fn=<AddBackward0>)\n",
            "tensor(13406.1289, grad_fn=<AddBackward0>)\n",
            "tensor(12908.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13356.3027, grad_fn=<AddBackward0>)\n",
            "tensor(13702.1953, grad_fn=<AddBackward0>)\n",
            "tensor(13492.2451, grad_fn=<AddBackward0>)\n",
            "tensor(13326.6680, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [160/469], Reconst Loss: 78.8270, KL Div: 25.2876\n",
            "tensor(12766.7139, grad_fn=<AddBackward0>)\n",
            "tensor(13450.8320, grad_fn=<AddBackward0>)\n",
            "tensor(13518.9131, grad_fn=<AddBackward0>)\n",
            "tensor(13245.7402, grad_fn=<AddBackward0>)\n",
            "tensor(13452.5000, grad_fn=<AddBackward0>)\n",
            "tensor(13197.6777, grad_fn=<AddBackward0>)\n",
            "tensor(13035.6699, grad_fn=<AddBackward0>)\n",
            "tensor(12912.9512, grad_fn=<AddBackward0>)\n",
            "tensor(13538.6309, grad_fn=<AddBackward0>)\n",
            "tensor(13272.9727, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [170/469], Reconst Loss: 77.9732, KL Div: 25.7219\n",
            "tensor(13356.1201, grad_fn=<AddBackward0>)\n",
            "tensor(13272.5791, grad_fn=<AddBackward0>)\n",
            "tensor(12902.9707, grad_fn=<AddBackward0>)\n",
            "tensor(13967.3428, grad_fn=<AddBackward0>)\n",
            "tensor(13065.5811, grad_fn=<AddBackward0>)\n",
            "tensor(12881.0029, grad_fn=<AddBackward0>)\n",
            "tensor(13123.6230, grad_fn=<AddBackward0>)\n",
            "tensor(12969.6523, grad_fn=<AddBackward0>)\n",
            "tensor(13421.8535, grad_fn=<AddBackward0>)\n",
            "tensor(13565.9883, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [180/469], Reconst Loss: 80.6217, KL Div: 25.3626\n",
            "tensor(13335.4824, grad_fn=<AddBackward0>)\n",
            "tensor(13075.2705, grad_fn=<AddBackward0>)\n",
            "tensor(13100.7441, grad_fn=<AddBackward0>)\n",
            "tensor(13344.0430, grad_fn=<AddBackward0>)\n",
            "tensor(12964.7168, grad_fn=<AddBackward0>)\n",
            "tensor(13157.6592, grad_fn=<AddBackward0>)\n",
            "tensor(13278.4639, grad_fn=<AddBackward0>)\n",
            "tensor(12970.2588, grad_fn=<AddBackward0>)\n",
            "tensor(13416.0547, grad_fn=<AddBackward0>)\n",
            "tensor(13494.9219, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [190/469], Reconst Loss: 80.0899, KL Div: 25.3392\n",
            "tensor(13273.2930, grad_fn=<AddBackward0>)\n",
            "tensor(13018.8057, grad_fn=<AddBackward0>)\n",
            "tensor(13003.8008, grad_fn=<AddBackward0>)\n",
            "tensor(13125.0557, grad_fn=<AddBackward0>)\n",
            "tensor(13847.3955, grad_fn=<AddBackward0>)\n",
            "tensor(12641.3828, grad_fn=<AddBackward0>)\n",
            "tensor(13641.6250, grad_fn=<AddBackward0>)\n",
            "tensor(13251.4688, grad_fn=<AddBackward0>)\n",
            "tensor(13312.5703, grad_fn=<AddBackward0>)\n",
            "tensor(13184.4912, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [200/469], Reconst Loss: 77.8179, KL Div: 25.1859\n",
            "tensor(12865.2188, grad_fn=<AddBackward0>)\n",
            "tensor(13437.8613, grad_fn=<AddBackward0>)\n",
            "tensor(13117.1465, grad_fn=<AddBackward0>)\n",
            "tensor(12731.7373, grad_fn=<AddBackward0>)\n",
            "tensor(13177.2070, grad_fn=<AddBackward0>)\n",
            "tensor(12976.0596, grad_fn=<AddBackward0>)\n",
            "tensor(13104.5791, grad_fn=<AddBackward0>)\n",
            "tensor(13182.7588, grad_fn=<AddBackward0>)\n",
            "tensor(12945.5742, grad_fn=<AddBackward0>)\n",
            "tensor(13738.3125, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [210/469], Reconst Loss: 81.5879, KL Div: 25.7427\n",
            "tensor(13225.2012, grad_fn=<AddBackward0>)\n",
            "tensor(13348.8564, grad_fn=<AddBackward0>)\n",
            "tensor(13169.7725, grad_fn=<AddBackward0>)\n",
            "tensor(12891.0264, grad_fn=<AddBackward0>)\n",
            "tensor(13108.9619, grad_fn=<AddBackward0>)\n",
            "tensor(13406.9277, grad_fn=<AddBackward0>)\n",
            "tensor(12991.5020, grad_fn=<AddBackward0>)\n",
            "tensor(13242.5049, grad_fn=<AddBackward0>)\n",
            "tensor(13157.8584, grad_fn=<AddBackward0>)\n",
            "tensor(12724.3203, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [220/469], Reconst Loss: 74.5135, KL Div: 24.8952\n",
            "tensor(13139.8984, grad_fn=<AddBackward0>)\n",
            "tensor(13579.0469, grad_fn=<AddBackward0>)\n",
            "tensor(13070.7568, grad_fn=<AddBackward0>)\n",
            "tensor(13204.8809, grad_fn=<AddBackward0>)\n",
            "tensor(13544.6895, grad_fn=<AddBackward0>)\n",
            "tensor(13318.7188, grad_fn=<AddBackward0>)\n",
            "tensor(13750.9512, grad_fn=<AddBackward0>)\n",
            "tensor(12720.7109, grad_fn=<AddBackward0>)\n",
            "tensor(13228.5361, grad_fn=<AddBackward0>)\n",
            "tensor(13655.6133, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [230/469], Reconst Loss: 81.1952, KL Div: 25.4893\n",
            "tensor(13471.0508, grad_fn=<AddBackward0>)\n",
            "tensor(13687.7344, grad_fn=<AddBackward0>)\n",
            "tensor(13001.9531, grad_fn=<AddBackward0>)\n",
            "tensor(13220.0898, grad_fn=<AddBackward0>)\n",
            "tensor(13698.9531, grad_fn=<AddBackward0>)\n",
            "tensor(13337.6250, grad_fn=<AddBackward0>)\n",
            "tensor(13197.6699, grad_fn=<AddBackward0>)\n",
            "tensor(12791.7051, grad_fn=<AddBackward0>)\n",
            "tensor(13538.5586, grad_fn=<AddBackward0>)\n",
            "tensor(13247.2988, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [240/469], Reconst Loss: 77.2569, KL Div: 26.2377\n",
            "tensor(13724.7695, grad_fn=<AddBackward0>)\n",
            "tensor(13572.4404, grad_fn=<AddBackward0>)\n",
            "tensor(13234.5615, grad_fn=<AddBackward0>)\n",
            "tensor(12829.9854, grad_fn=<AddBackward0>)\n",
            "tensor(12881.0098, grad_fn=<AddBackward0>)\n",
            "tensor(13455.9922, grad_fn=<AddBackward0>)\n",
            "tensor(12948.7598, grad_fn=<AddBackward0>)\n",
            "tensor(13468.3008, grad_fn=<AddBackward0>)\n",
            "tensor(12599.8975, grad_fn=<AddBackward0>)\n",
            "tensor(13717.3555, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [250/469], Reconst Loss: 80.7890, KL Div: 26.3778\n",
            "tensor(13421.6777, grad_fn=<AddBackward0>)\n",
            "tensor(13154.0039, grad_fn=<AddBackward0>)\n",
            "tensor(13226.5693, grad_fn=<AddBackward0>)\n",
            "tensor(13254.8047, grad_fn=<AddBackward0>)\n",
            "tensor(12887.5498, grad_fn=<AddBackward0>)\n",
            "tensor(12844.5645, grad_fn=<AddBackward0>)\n",
            "tensor(12780.2930, grad_fn=<AddBackward0>)\n",
            "tensor(13211.4971, grad_fn=<AddBackward0>)\n",
            "tensor(13059.8242, grad_fn=<AddBackward0>)\n",
            "tensor(13108.3877, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [260/469], Reconst Loss: 76.5004, KL Div: 25.9089\n",
            "tensor(13585.9014, grad_fn=<AddBackward0>)\n",
            "tensor(12922.6582, grad_fn=<AddBackward0>)\n",
            "tensor(13230.4160, grad_fn=<AddBackward0>)\n",
            "tensor(13327.9199, grad_fn=<AddBackward0>)\n",
            "tensor(13199.1250, grad_fn=<AddBackward0>)\n",
            "tensor(12953.9639, grad_fn=<AddBackward0>)\n",
            "tensor(13318.5078, grad_fn=<AddBackward0>)\n",
            "tensor(12863.9160, grad_fn=<AddBackward0>)\n",
            "tensor(13102.4902, grad_fn=<AddBackward0>)\n",
            "tensor(12846.0869, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [270/469], Reconst Loss: 75.6402, KL Div: 24.7199\n",
            "tensor(13208.0449, grad_fn=<AddBackward0>)\n",
            "tensor(13736.1289, grad_fn=<AddBackward0>)\n",
            "tensor(13436.0879, grad_fn=<AddBackward0>)\n",
            "tensor(13331.9160, grad_fn=<AddBackward0>)\n",
            "tensor(12768.9160, grad_fn=<AddBackward0>)\n",
            "tensor(12962.9307, grad_fn=<AddBackward0>)\n",
            "tensor(13166.6953, grad_fn=<AddBackward0>)\n",
            "tensor(13523.0430, grad_fn=<AddBackward0>)\n",
            "tensor(12875.8066, grad_fn=<AddBackward0>)\n",
            "tensor(12835.1201, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [280/469], Reconst Loss: 75.1541, KL Div: 25.1203\n",
            "tensor(12816.1416, grad_fn=<AddBackward0>)\n",
            "tensor(13024.3408, grad_fn=<AddBackward0>)\n",
            "tensor(13699.0039, grad_fn=<AddBackward0>)\n",
            "tensor(13642.1367, grad_fn=<AddBackward0>)\n",
            "tensor(12735.4639, grad_fn=<AddBackward0>)\n",
            "tensor(12796.7510, grad_fn=<AddBackward0>)\n",
            "tensor(13433.0430, grad_fn=<AddBackward0>)\n",
            "tensor(13632.7178, grad_fn=<AddBackward0>)\n",
            "tensor(13580.9404, grad_fn=<AddBackward0>)\n",
            "tensor(13628.3057, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [290/469], Reconst Loss: 81.4377, KL Div: 25.0334\n",
            "tensor(13212.3125, grad_fn=<AddBackward0>)\n",
            "tensor(13523.4414, grad_fn=<AddBackward0>)\n",
            "tensor(12998.7363, grad_fn=<AddBackward0>)\n",
            "tensor(13655.6143, grad_fn=<AddBackward0>)\n",
            "tensor(13105.3857, grad_fn=<AddBackward0>)\n",
            "tensor(13389.9160, grad_fn=<AddBackward0>)\n",
            "tensor(13319.1348, grad_fn=<AddBackward0>)\n",
            "tensor(13386.4053, grad_fn=<AddBackward0>)\n",
            "tensor(12598.4473, grad_fn=<AddBackward0>)\n",
            "tensor(13530.3828, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [300/469], Reconst Loss: 79.3811, KL Div: 26.3250\n",
            "tensor(13648.1074, grad_fn=<AddBackward0>)\n",
            "tensor(13635.7295, grad_fn=<AddBackward0>)\n",
            "tensor(12958.4912, grad_fn=<AddBackward0>)\n",
            "tensor(13216.2109, grad_fn=<AddBackward0>)\n",
            "tensor(12888.5469, grad_fn=<AddBackward0>)\n",
            "tensor(13508.5000, grad_fn=<AddBackward0>)\n",
            "tensor(12579.0684, grad_fn=<AddBackward0>)\n",
            "tensor(13331.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13258.7441, grad_fn=<AddBackward0>)\n",
            "tensor(13419.5664, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [310/469], Reconst Loss: 78.5916, KL Div: 26.2487\n",
            "tensor(13408.7480, grad_fn=<AddBackward0>)\n",
            "tensor(13103.8594, grad_fn=<AddBackward0>)\n",
            "tensor(13373.5938, grad_fn=<AddBackward0>)\n",
            "tensor(13258.5498, grad_fn=<AddBackward0>)\n",
            "tensor(13275.7471, grad_fn=<AddBackward0>)\n",
            "tensor(13075.4873, grad_fn=<AddBackward0>)\n",
            "tensor(13236.4707, grad_fn=<AddBackward0>)\n",
            "tensor(13021.2051, grad_fn=<AddBackward0>)\n",
            "tensor(12803.7441, grad_fn=<AddBackward0>)\n",
            "tensor(13080.5908, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [320/469], Reconst Loss: 77.0697, KL Div: 25.1224\n",
            "tensor(12930.8232, grad_fn=<AddBackward0>)\n",
            "tensor(13343.6738, grad_fn=<AddBackward0>)\n",
            "tensor(12601.9609, grad_fn=<AddBackward0>)\n",
            "tensor(13029.5283, grad_fn=<AddBackward0>)\n",
            "tensor(13154.5918, grad_fn=<AddBackward0>)\n",
            "tensor(13517.7207, grad_fn=<AddBackward0>)\n",
            "tensor(12804.5205, grad_fn=<AddBackward0>)\n",
            "tensor(13535.5234, grad_fn=<AddBackward0>)\n",
            "tensor(13155.5645, grad_fn=<AddBackward0>)\n",
            "tensor(12779.0039, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [330/469], Reconst Loss: 74.7743, KL Div: 25.0617\n",
            "tensor(13122.6074, grad_fn=<AddBackward0>)\n",
            "tensor(13263.0938, grad_fn=<AddBackward0>)\n",
            "tensor(13224.0898, grad_fn=<AddBackward0>)\n",
            "tensor(13052.9707, grad_fn=<AddBackward0>)\n",
            "tensor(13217.5410, grad_fn=<AddBackward0>)\n",
            "tensor(13235.5762, grad_fn=<AddBackward0>)\n",
            "tensor(13429.3066, grad_fn=<AddBackward0>)\n",
            "tensor(13577.1855, grad_fn=<AddBackward0>)\n",
            "tensor(12541.1963, grad_fn=<AddBackward0>)\n",
            "tensor(13207.3516, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [340/469], Reconst Loss: 78.0828, KL Div: 25.0997\n",
            "tensor(13154.5039, grad_fn=<AddBackward0>)\n",
            "tensor(13169.9150, grad_fn=<AddBackward0>)\n",
            "tensor(13530.7490, grad_fn=<AddBackward0>)\n",
            "tensor(13546.7324, grad_fn=<AddBackward0>)\n",
            "tensor(13716.5215, grad_fn=<AddBackward0>)\n",
            "tensor(13314.0898, grad_fn=<AddBackward0>)\n",
            "tensor(13427.4883, grad_fn=<AddBackward0>)\n",
            "tensor(12714.2676, grad_fn=<AddBackward0>)\n",
            "tensor(13617.0566, grad_fn=<AddBackward0>)\n",
            "tensor(13102.2227, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [350/469], Reconst Loss: 76.5134, KL Div: 25.8477\n",
            "tensor(13210.9619, grad_fn=<AddBackward0>)\n",
            "tensor(12746.6299, grad_fn=<AddBackward0>)\n",
            "tensor(12953.6426, grad_fn=<AddBackward0>)\n",
            "tensor(12891.7451, grad_fn=<AddBackward0>)\n",
            "tensor(13244.7881, grad_fn=<AddBackward0>)\n",
            "tensor(13208.0586, grad_fn=<AddBackward0>)\n",
            "tensor(13180.8477, grad_fn=<AddBackward0>)\n",
            "tensor(13389.4727, grad_fn=<AddBackward0>)\n",
            "tensor(13520.9600, grad_fn=<AddBackward0>)\n",
            "tensor(12738.9082, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [360/469], Reconst Loss: 74.6458, KL Div: 24.8769\n",
            "tensor(12535.1094, grad_fn=<AddBackward0>)\n",
            "tensor(13195.3115, grad_fn=<AddBackward0>)\n",
            "tensor(13489.3867, grad_fn=<AddBackward0>)\n",
            "tensor(13032.8369, grad_fn=<AddBackward0>)\n",
            "tensor(13024.5166, grad_fn=<AddBackward0>)\n",
            "tensor(13560.2324, grad_fn=<AddBackward0>)\n",
            "tensor(13200.9043, grad_fn=<AddBackward0>)\n",
            "tensor(13117.5029, grad_fn=<AddBackward0>)\n",
            "tensor(13124.4326, grad_fn=<AddBackward0>)\n",
            "tensor(13147.3672, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [370/469], Reconst Loss: 77.7135, KL Div: 25.0003\n",
            "tensor(13526.4072, grad_fn=<AddBackward0>)\n",
            "tensor(12994.4062, grad_fn=<AddBackward0>)\n",
            "tensor(13644.4678, grad_fn=<AddBackward0>)\n",
            "tensor(13256.5547, grad_fn=<AddBackward0>)\n",
            "tensor(13735.8057, grad_fn=<AddBackward0>)\n",
            "tensor(13373.5166, grad_fn=<AddBackward0>)\n",
            "tensor(12901.3330, grad_fn=<AddBackward0>)\n",
            "tensor(13138.1504, grad_fn=<AddBackward0>)\n",
            "tensor(13694.3252, grad_fn=<AddBackward0>)\n",
            "tensor(13465.2031, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [380/469], Reconst Loss: 80.1499, KL Div: 25.0470\n",
            "tensor(12900.8184, grad_fn=<AddBackward0>)\n",
            "tensor(13528.5820, grad_fn=<AddBackward0>)\n",
            "tensor(13725.4727, grad_fn=<AddBackward0>)\n",
            "tensor(12826.4443, grad_fn=<AddBackward0>)\n",
            "tensor(12753.8340, grad_fn=<AddBackward0>)\n",
            "tensor(13552.4443, grad_fn=<AddBackward0>)\n",
            "tensor(13416.9248, grad_fn=<AddBackward0>)\n",
            "tensor(13201.6445, grad_fn=<AddBackward0>)\n",
            "tensor(12986.6670, grad_fn=<AddBackward0>)\n",
            "tensor(13727.3848, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [390/469], Reconst Loss: 81.6324, KL Div: 25.6128\n",
            "tensor(12982.3506, grad_fn=<AddBackward0>)\n",
            "tensor(13837.7568, grad_fn=<AddBackward0>)\n",
            "tensor(13181.6641, grad_fn=<AddBackward0>)\n",
            "tensor(13327.5352, grad_fn=<AddBackward0>)\n",
            "tensor(13084.3633, grad_fn=<AddBackward0>)\n",
            "tensor(13492.4727, grad_fn=<AddBackward0>)\n",
            "tensor(13311.2324, grad_fn=<AddBackward0>)\n",
            "tensor(13569.3340, grad_fn=<AddBackward0>)\n",
            "tensor(13176.4365, grad_fn=<AddBackward0>)\n",
            "tensor(13620.5059, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [400/469], Reconst Loss: 79.9941, KL Div: 26.4161\n",
            "tensor(12850.5645, grad_fn=<AddBackward0>)\n",
            "tensor(13545.8340, grad_fn=<AddBackward0>)\n",
            "tensor(13247.8135, grad_fn=<AddBackward0>)\n",
            "tensor(13323.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13481.0293, grad_fn=<AddBackward0>)\n",
            "tensor(13502.5811, grad_fn=<AddBackward0>)\n",
            "tensor(12994.4736, grad_fn=<AddBackward0>)\n",
            "tensor(13154.5469, grad_fn=<AddBackward0>)\n",
            "tensor(13409.6504, grad_fn=<AddBackward0>)\n",
            "tensor(13113.6416, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [410/469], Reconst Loss: 77.6131, KL Div: 24.8372\n",
            "tensor(13591.0410, grad_fn=<AddBackward0>)\n",
            "tensor(13359.8418, grad_fn=<AddBackward0>)\n",
            "tensor(13546.6152, grad_fn=<AddBackward0>)\n",
            "tensor(13473.7754, grad_fn=<AddBackward0>)\n",
            "tensor(12752.8691, grad_fn=<AddBackward0>)\n",
            "tensor(13422.9258, grad_fn=<AddBackward0>)\n",
            "tensor(13306.9629, grad_fn=<AddBackward0>)\n",
            "tensor(13000.3125, grad_fn=<AddBackward0>)\n",
            "tensor(13267.7383, grad_fn=<AddBackward0>)\n",
            "tensor(13362.9082, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [420/469], Reconst Loss: 78.3936, KL Div: 26.0041\n",
            "tensor(13263.7119, grad_fn=<AddBackward0>)\n",
            "tensor(13621.0996, grad_fn=<AddBackward0>)\n",
            "tensor(13613.3555, grad_fn=<AddBackward0>)\n",
            "tensor(13474.2578, grad_fn=<AddBackward0>)\n",
            "tensor(13854.4385, grad_fn=<AddBackward0>)\n",
            "tensor(13149.3223, grad_fn=<AddBackward0>)\n",
            "tensor(13429.3857, grad_fn=<AddBackward0>)\n",
            "tensor(13368.3174, grad_fn=<AddBackward0>)\n",
            "tensor(13721.9922, grad_fn=<AddBackward0>)\n",
            "tensor(13208.3965, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [430/469], Reconst Loss: 77.3276, KL Div: 25.8630\n",
            "tensor(13463.4619, grad_fn=<AddBackward0>)\n",
            "tensor(13336.4199, grad_fn=<AddBackward0>)\n",
            "tensor(13277.3340, grad_fn=<AddBackward0>)\n",
            "tensor(13124.8223, grad_fn=<AddBackward0>)\n",
            "tensor(12701.0654, grad_fn=<AddBackward0>)\n",
            "tensor(13782.3838, grad_fn=<AddBackward0>)\n",
            "tensor(13130.5908, grad_fn=<AddBackward0>)\n",
            "tensor(12528.8066, grad_fn=<AddBackward0>)\n",
            "tensor(13034.8750, grad_fn=<AddBackward0>)\n",
            "tensor(13143.5713, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [440/469], Reconst Loss: 77.3263, KL Div: 25.3578\n",
            "tensor(13610.8545, grad_fn=<AddBackward0>)\n",
            "tensor(13050.8682, grad_fn=<AddBackward0>)\n",
            "tensor(13335.0234, grad_fn=<AddBackward0>)\n",
            "tensor(12610.8975, grad_fn=<AddBackward0>)\n",
            "tensor(13342.9814, grad_fn=<AddBackward0>)\n",
            "tensor(13590.9404, grad_fn=<AddBackward0>)\n",
            "tensor(12967.4102, grad_fn=<AddBackward0>)\n",
            "tensor(13349.4668, grad_fn=<AddBackward0>)\n",
            "tensor(13492.7334, grad_fn=<AddBackward0>)\n",
            "tensor(13223.9580, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [450/469], Reconst Loss: 77.3721, KL Div: 25.9401\n",
            "tensor(13069.8857, grad_fn=<AddBackward0>)\n",
            "tensor(13405.2734, grad_fn=<AddBackward0>)\n",
            "tensor(13088.8232, grad_fn=<AddBackward0>)\n",
            "tensor(12729.2266, grad_fn=<AddBackward0>)\n",
            "tensor(13233.5264, grad_fn=<AddBackward0>)\n",
            "tensor(13393.4150, grad_fn=<AddBackward0>)\n",
            "tensor(13106.9473, grad_fn=<AddBackward0>)\n",
            "tensor(12854.5273, grad_fn=<AddBackward0>)\n",
            "tensor(13183.7559, grad_fn=<AddBackward0>)\n",
            "tensor(13575.2314, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [460/469], Reconst Loss: 80.2723, KL Div: 25.7842\n",
            "tensor(12928.1035, grad_fn=<AddBackward0>)\n",
            "tensor(13228.9004, grad_fn=<AddBackward0>)\n",
            "tensor(13300.3350, grad_fn=<AddBackward0>)\n",
            "tensor(13194.9395, grad_fn=<AddBackward0>)\n",
            "tensor(13079.1797, grad_fn=<AddBackward0>)\n",
            "tensor(12894.4629, grad_fn=<AddBackward0>)\n",
            "tensor(13583.0059, grad_fn=<AddBackward0>)\n",
            "tensor(14226.9668, grad_fn=<AddBackward0>)\n",
            "tensor(10253.6738, grad_fn=<AddBackward0>)\n",
            "tensor(13005.5039, grad_fn=<AddBackward0>)\n",
            "tensor(13061.9336, grad_fn=<AddBackward0>)\n",
            "tensor(13091.6006, grad_fn=<AddBackward0>)\n",
            "tensor(13252.2363, grad_fn=<AddBackward0>)\n",
            "tensor(13608.8633, grad_fn=<AddBackward0>)\n",
            "tensor(13393.3564, grad_fn=<AddBackward0>)\n",
            "tensor(12830.8955, grad_fn=<AddBackward0>)\n",
            "tensor(13011.6777, grad_fn=<AddBackward0>)\n",
            "tensor(13136.2285, grad_fn=<AddBackward0>)\n",
            "tensor(13346.6572, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [10/469], Reconst Loss: 78.8249, KL Div: 25.4459\n",
            "tensor(13095.0488, grad_fn=<AddBackward0>)\n",
            "tensor(13346.7969, grad_fn=<AddBackward0>)\n",
            "tensor(13016.8086, grad_fn=<AddBackward0>)\n",
            "tensor(12684.3770, grad_fn=<AddBackward0>)\n",
            "tensor(13140.4482, grad_fn=<AddBackward0>)\n",
            "tensor(13657., grad_fn=<AddBackward0>)\n",
            "tensor(13170.4229, grad_fn=<AddBackward0>)\n",
            "tensor(13245.0547, grad_fn=<AddBackward0>)\n",
            "tensor(13226.7422, grad_fn=<AddBackward0>)\n",
            "tensor(12958.7832, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [20/469], Reconst Loss: 75.6495, KL Div: 25.5910\n",
            "tensor(12949.4131, grad_fn=<AddBackward0>)\n",
            "tensor(13009.6777, grad_fn=<AddBackward0>)\n",
            "tensor(13561.9824, grad_fn=<AddBackward0>)\n",
            "tensor(13218.2324, grad_fn=<AddBackward0>)\n",
            "tensor(13064.9727, grad_fn=<AddBackward0>)\n",
            "tensor(12676.3623, grad_fn=<AddBackward0>)\n",
            "tensor(13268.4658, grad_fn=<AddBackward0>)\n",
            "tensor(13612.6260, grad_fn=<AddBackward0>)\n",
            "tensor(13103.9844, grad_fn=<AddBackward0>)\n",
            "tensor(13011.5039, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [30/469], Reconst Loss: 76.9035, KL Div: 24.7489\n",
            "tensor(13280.6074, grad_fn=<AddBackward0>)\n",
            "tensor(13507.0352, grad_fn=<AddBackward0>)\n",
            "tensor(13372.4512, grad_fn=<AddBackward0>)\n",
            "tensor(12934.1396, grad_fn=<AddBackward0>)\n",
            "tensor(13077.6387, grad_fn=<AddBackward0>)\n",
            "tensor(13294.8945, grad_fn=<AddBackward0>)\n",
            "tensor(13082.5830, grad_fn=<AddBackward0>)\n",
            "tensor(13398.5342, grad_fn=<AddBackward0>)\n",
            "tensor(13797.6436, grad_fn=<AddBackward0>)\n",
            "tensor(13037.8086, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [40/469], Reconst Loss: 76.6224, KL Div: 25.2355\n",
            "tensor(13438.2246, grad_fn=<AddBackward0>)\n",
            "tensor(13074.1250, grad_fn=<AddBackward0>)\n",
            "tensor(13374.1270, grad_fn=<AddBackward0>)\n",
            "tensor(13201.5479, grad_fn=<AddBackward0>)\n",
            "tensor(13577.8672, grad_fn=<AddBackward0>)\n",
            "tensor(13233.2275, grad_fn=<AddBackward0>)\n",
            "tensor(13373.6934, grad_fn=<AddBackward0>)\n",
            "tensor(13523.8057, grad_fn=<AddBackward0>)\n",
            "tensor(12592.8057, grad_fn=<AddBackward0>)\n",
            "tensor(13246.1953, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [50/469], Reconst Loss: 77.9365, KL Div: 25.5494\n",
            "tensor(13663.2861, grad_fn=<AddBackward0>)\n",
            "tensor(13074.4023, grad_fn=<AddBackward0>)\n",
            "tensor(12938.5879, grad_fn=<AddBackward0>)\n",
            "tensor(13761.8975, grad_fn=<AddBackward0>)\n",
            "tensor(12917.7842, grad_fn=<AddBackward0>)\n",
            "tensor(12603.0811, grad_fn=<AddBackward0>)\n",
            "tensor(13319.6621, grad_fn=<AddBackward0>)\n",
            "tensor(13462.4902, grad_fn=<AddBackward0>)\n",
            "tensor(13033.7217, grad_fn=<AddBackward0>)\n",
            "tensor(12906.4619, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [60/469], Reconst Loss: 75.5950, KL Div: 25.2367\n",
            "tensor(13169.3057, grad_fn=<AddBackward0>)\n",
            "tensor(13513.0117, grad_fn=<AddBackward0>)\n",
            "tensor(13612.0498, grad_fn=<AddBackward0>)\n",
            "tensor(12733.1191, grad_fn=<AddBackward0>)\n",
            "tensor(13620.1016, grad_fn=<AddBackward0>)\n",
            "tensor(12837.6914, grad_fn=<AddBackward0>)\n",
            "tensor(13352.8115, grad_fn=<AddBackward0>)\n",
            "tensor(13409.7051, grad_fn=<AddBackward0>)\n",
            "tensor(13595.3467, grad_fn=<AddBackward0>)\n",
            "tensor(13216.5078, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [70/469], Reconst Loss: 77.3880, KL Div: 25.8660\n",
            "tensor(13103.9922, grad_fn=<AddBackward0>)\n",
            "tensor(12963.9883, grad_fn=<AddBackward0>)\n",
            "tensor(12992.2393, grad_fn=<AddBackward0>)\n",
            "tensor(13467.1211, grad_fn=<AddBackward0>)\n",
            "tensor(13423.4189, grad_fn=<AddBackward0>)\n",
            "tensor(13633.1191, grad_fn=<AddBackward0>)\n",
            "tensor(13191.5811, grad_fn=<AddBackward0>)\n",
            "tensor(13127.8955, grad_fn=<AddBackward0>)\n",
            "tensor(13191.0127, grad_fn=<AddBackward0>)\n",
            "tensor(13169.8301, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [80/469], Reconst Loss: 77.2577, KL Div: 25.6316\n",
            "tensor(13373.1172, grad_fn=<AddBackward0>)\n",
            "tensor(13587.3027, grad_fn=<AddBackward0>)\n",
            "tensor(13922.7705, grad_fn=<AddBackward0>)\n",
            "tensor(13358.7236, grad_fn=<AddBackward0>)\n",
            "tensor(13292.3848, grad_fn=<AddBackward0>)\n",
            "tensor(13416.2734, grad_fn=<AddBackward0>)\n",
            "tensor(13220.0645, grad_fn=<AddBackward0>)\n",
            "tensor(13208.2900, grad_fn=<AddBackward0>)\n",
            "tensor(12896.0166, grad_fn=<AddBackward0>)\n",
            "tensor(13524.8926, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [90/469], Reconst Loss: 79.9554, KL Div: 25.7079\n",
            "tensor(13527.6533, grad_fn=<AddBackward0>)\n",
            "tensor(13214.1465, grad_fn=<AddBackward0>)\n",
            "tensor(13255.3164, grad_fn=<AddBackward0>)\n",
            "tensor(13292.0010, grad_fn=<AddBackward0>)\n",
            "tensor(13528.3389, grad_fn=<AddBackward0>)\n",
            "tensor(13167.8174, grad_fn=<AddBackward0>)\n",
            "tensor(13550.0449, grad_fn=<AddBackward0>)\n",
            "tensor(13063.2363, grad_fn=<AddBackward0>)\n",
            "tensor(12967.2539, grad_fn=<AddBackward0>)\n",
            "tensor(13138.1191, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [100/469], Reconst Loss: 76.9610, KL Div: 25.6806\n",
            "tensor(13078.5820, grad_fn=<AddBackward0>)\n",
            "tensor(13617.5127, grad_fn=<AddBackward0>)\n",
            "tensor(12971.0068, grad_fn=<AddBackward0>)\n",
            "tensor(13361.6025, grad_fn=<AddBackward0>)\n",
            "tensor(13285.4990, grad_fn=<AddBackward0>)\n",
            "tensor(12947.9121, grad_fn=<AddBackward0>)\n",
            "tensor(13017.4893, grad_fn=<AddBackward0>)\n",
            "tensor(13532.6445, grad_fn=<AddBackward0>)\n",
            "tensor(12851.8203, grad_fn=<AddBackward0>)\n",
            "tensor(13711.9277, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [110/469], Reconst Loss: 81.3674, KL Div: 25.7571\n",
            "tensor(13395.3418, grad_fn=<AddBackward0>)\n",
            "tensor(13344.3418, grad_fn=<AddBackward0>)\n",
            "tensor(13147.1191, grad_fn=<AddBackward0>)\n",
            "tensor(13288.6201, grad_fn=<AddBackward0>)\n",
            "tensor(13510.5879, grad_fn=<AddBackward0>)\n",
            "tensor(13423.8037, grad_fn=<AddBackward0>)\n",
            "tensor(13451.5820, grad_fn=<AddBackward0>)\n",
            "tensor(13284.1494, grad_fn=<AddBackward0>)\n",
            "tensor(13287.0371, grad_fn=<AddBackward0>)\n",
            "tensor(13079.4141, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [120/469], Reconst Loss: 77.2038, KL Div: 24.9791\n",
            "tensor(13426.6992, grad_fn=<AddBackward0>)\n",
            "tensor(13244.0049, grad_fn=<AddBackward0>)\n",
            "tensor(13176.2598, grad_fn=<AddBackward0>)\n",
            "tensor(13135.9688, grad_fn=<AddBackward0>)\n",
            "tensor(12165.3926, grad_fn=<AddBackward0>)\n",
            "tensor(13350.4424, grad_fn=<AddBackward0>)\n",
            "tensor(13263.6875, grad_fn=<AddBackward0>)\n",
            "tensor(12647.1104, grad_fn=<AddBackward0>)\n",
            "tensor(13854.5879, grad_fn=<AddBackward0>)\n",
            "tensor(13305.3213, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [130/469], Reconst Loss: 78.5335, KL Div: 25.4143\n",
            "tensor(12741.1719, grad_fn=<AddBackward0>)\n",
            "tensor(13576.6436, grad_fn=<AddBackward0>)\n",
            "tensor(12966.0117, grad_fn=<AddBackward0>)\n",
            "tensor(13498.4336, grad_fn=<AddBackward0>)\n",
            "tensor(13679.8438, grad_fn=<AddBackward0>)\n",
            "tensor(13345.1738, grad_fn=<AddBackward0>)\n",
            "tensor(13257.5605, grad_fn=<AddBackward0>)\n",
            "tensor(13117.0791, grad_fn=<AddBackward0>)\n",
            "tensor(13030.4648, grad_fn=<AddBackward0>)\n",
            "tensor(12861.2676, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [140/469], Reconst Loss: 75.0656, KL Div: 25.4131\n",
            "tensor(13570.4805, grad_fn=<AddBackward0>)\n",
            "tensor(13548.9023, grad_fn=<AddBackward0>)\n",
            "tensor(13498.7754, grad_fn=<AddBackward0>)\n",
            "tensor(13416.3916, grad_fn=<AddBackward0>)\n",
            "tensor(13098.0713, grad_fn=<AddBackward0>)\n",
            "tensor(12613.2979, grad_fn=<AddBackward0>)\n",
            "tensor(13555.2324, grad_fn=<AddBackward0>)\n",
            "tensor(13498.5488, grad_fn=<AddBackward0>)\n",
            "tensor(12772.7148, grad_fn=<AddBackward0>)\n",
            "tensor(13507.8457, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [150/469], Reconst Loss: 79.5903, KL Div: 25.9397\n",
            "tensor(12723.7227, grad_fn=<AddBackward0>)\n",
            "tensor(13425.8232, grad_fn=<AddBackward0>)\n",
            "tensor(12807.1953, grad_fn=<AddBackward0>)\n",
            "tensor(13410.2637, grad_fn=<AddBackward0>)\n",
            "tensor(13189.7725, grad_fn=<AddBackward0>)\n",
            "tensor(13971.5156, grad_fn=<AddBackward0>)\n",
            "tensor(13336.3066, grad_fn=<AddBackward0>)\n",
            "tensor(13179.9355, grad_fn=<AddBackward0>)\n",
            "tensor(13728.2139, grad_fn=<AddBackward0>)\n",
            "tensor(13346.5820, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [160/469], Reconst Loss: 78.0621, KL Div: 26.2080\n",
            "tensor(12937.9785, grad_fn=<AddBackward0>)\n",
            "tensor(12945.9512, grad_fn=<AddBackward0>)\n",
            "tensor(12838.2617, grad_fn=<AddBackward0>)\n",
            "tensor(13345.4189, grad_fn=<AddBackward0>)\n",
            "tensor(13186.1211, grad_fn=<AddBackward0>)\n",
            "tensor(12982.0342, grad_fn=<AddBackward0>)\n",
            "tensor(12973.8027, grad_fn=<AddBackward0>)\n",
            "tensor(13080.0684, grad_fn=<AddBackward0>)\n",
            "tensor(12781.8184, grad_fn=<AddBackward0>)\n",
            "tensor(13067.0410, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [170/469], Reconst Loss: 77.0607, KL Div: 25.0255\n",
            "tensor(13190.5400, grad_fn=<AddBackward0>)\n",
            "tensor(13820.0449, grad_fn=<AddBackward0>)\n",
            "tensor(13305.2002, grad_fn=<AddBackward0>)\n",
            "tensor(13281.0752, grad_fn=<AddBackward0>)\n",
            "tensor(13303.0430, grad_fn=<AddBackward0>)\n",
            "tensor(13513.9609, grad_fn=<AddBackward0>)\n",
            "tensor(13231.4453, grad_fn=<AddBackward0>)\n",
            "tensor(13232.4434, grad_fn=<AddBackward0>)\n",
            "tensor(13815.3066, grad_fn=<AddBackward0>)\n",
            "tensor(13464.5518, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [180/469], Reconst Loss: 78.9748, KL Div: 26.2170\n",
            "tensor(13753.1504, grad_fn=<AddBackward0>)\n",
            "tensor(13381.9951, grad_fn=<AddBackward0>)\n",
            "tensor(12972.9023, grad_fn=<AddBackward0>)\n",
            "tensor(13150.0840, grad_fn=<AddBackward0>)\n",
            "tensor(13541.5195, grad_fn=<AddBackward0>)\n",
            "tensor(12842.0439, grad_fn=<AddBackward0>)\n",
            "tensor(13416.1963, grad_fn=<AddBackward0>)\n",
            "tensor(13130.1191, grad_fn=<AddBackward0>)\n",
            "tensor(13383.2637, grad_fn=<AddBackward0>)\n",
            "tensor(13274.8379, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [190/469], Reconst Loss: 78.4658, KL Div: 25.2439\n",
            "tensor(13354.3145, grad_fn=<AddBackward0>)\n",
            "tensor(13079.6631, grad_fn=<AddBackward0>)\n",
            "tensor(13408.6855, grad_fn=<AddBackward0>)\n",
            "tensor(13463.3535, grad_fn=<AddBackward0>)\n",
            "tensor(12859.1211, grad_fn=<AddBackward0>)\n",
            "tensor(13009.3320, grad_fn=<AddBackward0>)\n",
            "tensor(13281.6650, grad_fn=<AddBackward0>)\n",
            "tensor(13531.5986, grad_fn=<AddBackward0>)\n",
            "tensor(13497.2529, grad_fn=<AddBackward0>)\n",
            "tensor(13244.5605, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [200/469], Reconst Loss: 78.0379, KL Div: 25.4352\n",
            "tensor(12774.0596, grad_fn=<AddBackward0>)\n",
            "tensor(13426.4199, grad_fn=<AddBackward0>)\n",
            "tensor(13335.0889, grad_fn=<AddBackward0>)\n",
            "tensor(12940.6377, grad_fn=<AddBackward0>)\n",
            "tensor(12656.0469, grad_fn=<AddBackward0>)\n",
            "tensor(12816.7012, grad_fn=<AddBackward0>)\n",
            "tensor(13139.0518, grad_fn=<AddBackward0>)\n",
            "tensor(13343.6875, grad_fn=<AddBackward0>)\n",
            "tensor(13209.2441, grad_fn=<AddBackward0>)\n",
            "tensor(13094.6162, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [210/469], Reconst Loss: 77.2662, KL Div: 25.0355\n",
            "tensor(13675.9297, grad_fn=<AddBackward0>)\n",
            "tensor(13101.5791, grad_fn=<AddBackward0>)\n",
            "tensor(13232.3066, grad_fn=<AddBackward0>)\n",
            "tensor(13056.9365, grad_fn=<AddBackward0>)\n",
            "tensor(12793.8535, grad_fn=<AddBackward0>)\n",
            "tensor(12746.1855, grad_fn=<AddBackward0>)\n",
            "tensor(13282.7178, grad_fn=<AddBackward0>)\n",
            "tensor(13043.9580, grad_fn=<AddBackward0>)\n",
            "tensor(13356.1416, grad_fn=<AddBackward0>)\n",
            "tensor(12874.6455, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [220/469], Reconst Loss: 75.9387, KL Div: 24.6445\n",
            "tensor(13138.3623, grad_fn=<AddBackward0>)\n",
            "tensor(13436.5020, grad_fn=<AddBackward0>)\n",
            "tensor(13310.1318, grad_fn=<AddBackward0>)\n",
            "tensor(13171.6699, grad_fn=<AddBackward0>)\n",
            "tensor(13259.5010, grad_fn=<AddBackward0>)\n",
            "tensor(13374.2695, grad_fn=<AddBackward0>)\n",
            "tensor(13052.8887, grad_fn=<AddBackward0>)\n",
            "tensor(12690.3789, grad_fn=<AddBackward0>)\n",
            "tensor(13079.9355, grad_fn=<AddBackward0>)\n",
            "tensor(13460.5928, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [230/469], Reconst Loss: 79.0243, KL Div: 26.1366\n",
            "tensor(12998.0908, grad_fn=<AddBackward0>)\n",
            "tensor(13335.3145, grad_fn=<AddBackward0>)\n",
            "tensor(12950.4199, grad_fn=<AddBackward0>)\n",
            "tensor(13142.0088, grad_fn=<AddBackward0>)\n",
            "tensor(13300.3877, grad_fn=<AddBackward0>)\n",
            "tensor(13431.5947, grad_fn=<AddBackward0>)\n",
            "tensor(12768.4297, grad_fn=<AddBackward0>)\n",
            "tensor(13231.6758, grad_fn=<AddBackward0>)\n",
            "tensor(13430.3281, grad_fn=<AddBackward0>)\n",
            "tensor(12612.4355, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [240/469], Reconst Loss: 74.6394, KL Div: 23.8952\n",
            "tensor(13022.6289, grad_fn=<AddBackward0>)\n",
            "tensor(13137.1250, grad_fn=<AddBackward0>)\n",
            "tensor(13145.8301, grad_fn=<AddBackward0>)\n",
            "tensor(13523.9414, grad_fn=<AddBackward0>)\n",
            "tensor(12989.0938, grad_fn=<AddBackward0>)\n",
            "tensor(12812.2344, grad_fn=<AddBackward0>)\n",
            "tensor(13398.8037, grad_fn=<AddBackward0>)\n",
            "tensor(12954.8555, grad_fn=<AddBackward0>)\n",
            "tensor(13337.2549, grad_fn=<AddBackward0>)\n",
            "tensor(13166.3125, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [250/469], Reconst Loss: 77.2695, KL Div: 25.5924\n",
            "tensor(13452.0303, grad_fn=<AddBackward0>)\n",
            "tensor(13090.6738, grad_fn=<AddBackward0>)\n",
            "tensor(13206.8984, grad_fn=<AddBackward0>)\n",
            "tensor(13306.1787, grad_fn=<AddBackward0>)\n",
            "tensor(13196.5098, grad_fn=<AddBackward0>)\n",
            "tensor(13297.2451, grad_fn=<AddBackward0>)\n",
            "tensor(13142.0957, grad_fn=<AddBackward0>)\n",
            "tensor(13267.6660, grad_fn=<AddBackward0>)\n",
            "tensor(12923.5762, grad_fn=<AddBackward0>)\n",
            "tensor(12914.6299, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [260/469], Reconst Loss: 74.7966, KL Div: 26.0989\n",
            "tensor(13209.7715, grad_fn=<AddBackward0>)\n",
            "tensor(12782.5703, grad_fn=<AddBackward0>)\n",
            "tensor(13067.2490, grad_fn=<AddBackward0>)\n",
            "tensor(12567.7002, grad_fn=<AddBackward0>)\n",
            "tensor(12800.5000, grad_fn=<AddBackward0>)\n",
            "tensor(13293.1426, grad_fn=<AddBackward0>)\n",
            "tensor(13056.1436, grad_fn=<AddBackward0>)\n",
            "tensor(13743.0127, grad_fn=<AddBackward0>)\n",
            "tensor(13387.4326, grad_fn=<AddBackward0>)\n",
            "tensor(13904.0430, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [270/469], Reconst Loss: 83.1089, KL Div: 25.5164\n",
            "tensor(13026.4336, grad_fn=<AddBackward0>)\n",
            "tensor(13237.4395, grad_fn=<AddBackward0>)\n",
            "tensor(13020.4668, grad_fn=<AddBackward0>)\n",
            "tensor(12901.3945, grad_fn=<AddBackward0>)\n",
            "tensor(13500.1104, grad_fn=<AddBackward0>)\n",
            "tensor(13097.8887, grad_fn=<AddBackward0>)\n",
            "tensor(13363.8369, grad_fn=<AddBackward0>)\n",
            "tensor(13036.2764, grad_fn=<AddBackward0>)\n",
            "tensor(13912.0732, grad_fn=<AddBackward0>)\n",
            "tensor(13552.4355, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [280/469], Reconst Loss: 79.1957, KL Div: 26.6827\n",
            "tensor(13491.0059, grad_fn=<AddBackward0>)\n",
            "tensor(13226.7051, grad_fn=<AddBackward0>)\n",
            "tensor(13603.0684, grad_fn=<AddBackward0>)\n",
            "tensor(12898.0166, grad_fn=<AddBackward0>)\n",
            "tensor(13532.3896, grad_fn=<AddBackward0>)\n",
            "tensor(13102.9600, grad_fn=<AddBackward0>)\n",
            "tensor(13415.9619, grad_fn=<AddBackward0>)\n",
            "tensor(13121.7578, grad_fn=<AddBackward0>)\n",
            "tensor(12749.8428, grad_fn=<AddBackward0>)\n",
            "tensor(13446.1094, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [290/469], Reconst Loss: 79.0788, KL Div: 25.9689\n",
            "tensor(12938.5312, grad_fn=<AddBackward0>)\n",
            "tensor(13171.5225, grad_fn=<AddBackward0>)\n",
            "tensor(13161.8008, grad_fn=<AddBackward0>)\n",
            "tensor(13638.3750, grad_fn=<AddBackward0>)\n",
            "tensor(13683.3057, grad_fn=<AddBackward0>)\n",
            "tensor(13139.3320, grad_fn=<AddBackward0>)\n",
            "tensor(12871.1670, grad_fn=<AddBackward0>)\n",
            "tensor(12772.3643, grad_fn=<AddBackward0>)\n",
            "tensor(13295.6982, grad_fn=<AddBackward0>)\n",
            "tensor(13056.3340, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [300/469], Reconst Loss: 76.8903, KL Div: 25.1123\n",
            "tensor(13695.4336, grad_fn=<AddBackward0>)\n",
            "tensor(13277.0664, grad_fn=<AddBackward0>)\n",
            "tensor(13532.0986, grad_fn=<AddBackward0>)\n",
            "tensor(13156.8477, grad_fn=<AddBackward0>)\n",
            "tensor(13166.2344, grad_fn=<AddBackward0>)\n",
            "tensor(13616.3613, grad_fn=<AddBackward0>)\n",
            "tensor(13263.5430, grad_fn=<AddBackward0>)\n",
            "tensor(13007.9082, grad_fn=<AddBackward0>)\n",
            "tensor(13384.2803, grad_fn=<AddBackward0>)\n",
            "tensor(13124.3848, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [310/469], Reconst Loss: 76.6524, KL Div: 25.8818\n",
            "tensor(13844.8242, grad_fn=<AddBackward0>)\n",
            "tensor(13492.0918, grad_fn=<AddBackward0>)\n",
            "tensor(13850.8389, grad_fn=<AddBackward0>)\n",
            "tensor(13215.1875, grad_fn=<AddBackward0>)\n",
            "tensor(13409.7109, grad_fn=<AddBackward0>)\n",
            "tensor(13468.9365, grad_fn=<AddBackward0>)\n",
            "tensor(13299.9629, grad_fn=<AddBackward0>)\n",
            "tensor(13135.2295, grad_fn=<AddBackward0>)\n",
            "tensor(12868.1318, grad_fn=<AddBackward0>)\n",
            "tensor(13349.8633, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [320/469], Reconst Loss: 78.9685, KL Div: 25.3274\n",
            "tensor(13723.9590, grad_fn=<AddBackward0>)\n",
            "tensor(13447.8047, grad_fn=<AddBackward0>)\n",
            "tensor(13278.8926, grad_fn=<AddBackward0>)\n",
            "tensor(13433.6992, grad_fn=<AddBackward0>)\n",
            "tensor(13603.3975, grad_fn=<AddBackward0>)\n",
            "tensor(13565.6387, grad_fn=<AddBackward0>)\n",
            "tensor(13186.3379, grad_fn=<AddBackward0>)\n",
            "tensor(13242.2139, grad_fn=<AddBackward0>)\n",
            "tensor(13373.5469, grad_fn=<AddBackward0>)\n",
            "tensor(13338.0908, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [330/469], Reconst Loss: 78.2471, KL Div: 25.9568\n",
            "tensor(13277.7578, grad_fn=<AddBackward0>)\n",
            "tensor(13012.8828, grad_fn=<AddBackward0>)\n",
            "tensor(13543.0840, grad_fn=<AddBackward0>)\n",
            "tensor(12706.7637, grad_fn=<AddBackward0>)\n",
            "tensor(13309.4570, grad_fn=<AddBackward0>)\n",
            "tensor(12848.6123, grad_fn=<AddBackward0>)\n",
            "tensor(12864.8613, grad_fn=<AddBackward0>)\n",
            "tensor(13410.5625, grad_fn=<AddBackward0>)\n",
            "tensor(13060.1230, grad_fn=<AddBackward0>)\n",
            "tensor(13202.6270, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [340/469], Reconst Loss: 78.1119, KL Div: 25.0336\n",
            "tensor(13503.6387, grad_fn=<AddBackward0>)\n",
            "tensor(13683.6846, grad_fn=<AddBackward0>)\n",
            "tensor(13304.3857, grad_fn=<AddBackward0>)\n",
            "tensor(13559.4512, grad_fn=<AddBackward0>)\n",
            "tensor(13109.3770, grad_fn=<AddBackward0>)\n",
            "tensor(13261.2393, grad_fn=<AddBackward0>)\n",
            "tensor(13892.2314, grad_fn=<AddBackward0>)\n",
            "tensor(13010.2871, grad_fn=<AddBackward0>)\n",
            "tensor(14160.6084, grad_fn=<AddBackward0>)\n",
            "tensor(12780.8867, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [350/469], Reconst Loss: 75.3579, KL Div: 24.4928\n",
            "tensor(13251.8828, grad_fn=<AddBackward0>)\n",
            "tensor(13160.9814, grad_fn=<AddBackward0>)\n",
            "tensor(12709.2012, grad_fn=<AddBackward0>)\n",
            "tensor(13438.1680, grad_fn=<AddBackward0>)\n",
            "tensor(13376.3867, grad_fn=<AddBackward0>)\n",
            "tensor(13032.3896, grad_fn=<AddBackward0>)\n",
            "tensor(13273.3584, grad_fn=<AddBackward0>)\n",
            "tensor(13262.1367, grad_fn=<AddBackward0>)\n",
            "tensor(12994.9590, grad_fn=<AddBackward0>)\n",
            "tensor(13480.0420, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [360/469], Reconst Loss: 79.5154, KL Div: 25.7974\n",
            "tensor(13118.7695, grad_fn=<AddBackward0>)\n",
            "tensor(13623.8008, grad_fn=<AddBackward0>)\n",
            "tensor(13148.1797, grad_fn=<AddBackward0>)\n",
            "tensor(13580.8057, grad_fn=<AddBackward0>)\n",
            "tensor(13115.2939, grad_fn=<AddBackward0>)\n",
            "tensor(13131.9238, grad_fn=<AddBackward0>)\n",
            "tensor(13208.8848, grad_fn=<AddBackward0>)\n",
            "tensor(13257.6709, grad_fn=<AddBackward0>)\n",
            "tensor(13295.9160, grad_fn=<AddBackward0>)\n",
            "tensor(13017.2168, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [370/469], Reconst Loss: 76.1249, KL Div: 25.5721\n",
            "tensor(13182.4492, grad_fn=<AddBackward0>)\n",
            "tensor(13302.6953, grad_fn=<AddBackward0>)\n",
            "tensor(12872.7930, grad_fn=<AddBackward0>)\n",
            "tensor(13422.1172, grad_fn=<AddBackward0>)\n",
            "tensor(12994.1768, grad_fn=<AddBackward0>)\n",
            "tensor(13098.0918, grad_fn=<AddBackward0>)\n",
            "tensor(13019.5420, grad_fn=<AddBackward0>)\n",
            "tensor(13408.1309, grad_fn=<AddBackward0>)\n",
            "tensor(13024.2520, grad_fn=<AddBackward0>)\n",
            "tensor(13732.9229, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [380/469], Reconst Loss: 80.9427, KL Div: 26.3457\n",
            "tensor(13424.7832, grad_fn=<AddBackward0>)\n",
            "tensor(13480.3838, grad_fn=<AddBackward0>)\n",
            "tensor(13171.8047, grad_fn=<AddBackward0>)\n",
            "tensor(13398.8125, grad_fn=<AddBackward0>)\n",
            "tensor(13454.3203, grad_fn=<AddBackward0>)\n",
            "tensor(13265.8271, grad_fn=<AddBackward0>)\n",
            "tensor(13233.9668, grad_fn=<AddBackward0>)\n",
            "tensor(12842.7881, grad_fn=<AddBackward0>)\n",
            "tensor(13762.6309, grad_fn=<AddBackward0>)\n",
            "tensor(13993.2969, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [390/469], Reconst Loss: 83.3443, KL Div: 25.9783\n",
            "tensor(13180.1777, grad_fn=<AddBackward0>)\n",
            "tensor(13156.7832, grad_fn=<AddBackward0>)\n",
            "tensor(12979.9971, grad_fn=<AddBackward0>)\n",
            "tensor(12693.9932, grad_fn=<AddBackward0>)\n",
            "tensor(13714.4043, grad_fn=<AddBackward0>)\n",
            "tensor(13215.1885, grad_fn=<AddBackward0>)\n",
            "tensor(13235.2402, grad_fn=<AddBackward0>)\n",
            "tensor(12866.8945, grad_fn=<AddBackward0>)\n",
            "tensor(12973.7129, grad_fn=<AddBackward0>)\n",
            "tensor(12376.9766, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [400/469], Reconst Loss: 72.5102, KL Div: 24.1849\n",
            "tensor(13948.2305, grad_fn=<AddBackward0>)\n",
            "tensor(13227.5996, grad_fn=<AddBackward0>)\n",
            "tensor(13267.4150, grad_fn=<AddBackward0>)\n",
            "tensor(13426.1494, grad_fn=<AddBackward0>)\n",
            "tensor(13634.0703, grad_fn=<AddBackward0>)\n",
            "tensor(13231.7119, grad_fn=<AddBackward0>)\n",
            "tensor(12980.2168, grad_fn=<AddBackward0>)\n",
            "tensor(13457.2207, grad_fn=<AddBackward0>)\n",
            "tensor(13515.9141, grad_fn=<AddBackward0>)\n",
            "tensor(13245.5518, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [410/469], Reconst Loss: 78.3541, KL Div: 25.1267\n",
            "tensor(12583.4512, grad_fn=<AddBackward0>)\n",
            "tensor(13277.8398, grad_fn=<AddBackward0>)\n",
            "tensor(13484.3789, grad_fn=<AddBackward0>)\n",
            "tensor(12951.7412, grad_fn=<AddBackward0>)\n",
            "tensor(13228.4111, grad_fn=<AddBackward0>)\n",
            "tensor(12881.1250, grad_fn=<AddBackward0>)\n",
            "tensor(13627.8281, grad_fn=<AddBackward0>)\n",
            "tensor(13361.8057, grad_fn=<AddBackward0>)\n",
            "tensor(13511.5537, grad_fn=<AddBackward0>)\n",
            "tensor(13401.7070, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [420/469], Reconst Loss: 79.4582, KL Div: 25.2426\n",
            "tensor(13050.0098, grad_fn=<AddBackward0>)\n",
            "tensor(13147.7041, grad_fn=<AddBackward0>)\n",
            "tensor(12986.4932, grad_fn=<AddBackward0>)\n",
            "tensor(13749.7559, grad_fn=<AddBackward0>)\n",
            "tensor(12868.1729, grad_fn=<AddBackward0>)\n",
            "tensor(13059.7930, grad_fn=<AddBackward0>)\n",
            "tensor(13055.5020, grad_fn=<AddBackward0>)\n",
            "tensor(13163.5244, grad_fn=<AddBackward0>)\n",
            "tensor(12709.6143, grad_fn=<AddBackward0>)\n",
            "tensor(13063.9307, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [430/469], Reconst Loss: 76.6559, KL Div: 25.4061\n",
            "tensor(12808.3975, grad_fn=<AddBackward0>)\n",
            "tensor(13271.2627, grad_fn=<AddBackward0>)\n",
            "tensor(13386.0527, grad_fn=<AddBackward0>)\n",
            "tensor(13089.3721, grad_fn=<AddBackward0>)\n",
            "tensor(13448.5381, grad_fn=<AddBackward0>)\n",
            "tensor(13362.4395, grad_fn=<AddBackward0>)\n",
            "tensor(13114.1436, grad_fn=<AddBackward0>)\n",
            "tensor(13486.2393, grad_fn=<AddBackward0>)\n",
            "tensor(13257.2969, grad_fn=<AddBackward0>)\n",
            "tensor(13398.7627, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [440/469], Reconst Loss: 79.0678, KL Div: 25.6101\n",
            "tensor(13486.5020, grad_fn=<AddBackward0>)\n",
            "tensor(12945.7539, grad_fn=<AddBackward0>)\n",
            "tensor(13413.4180, grad_fn=<AddBackward0>)\n",
            "tensor(13179.6924, grad_fn=<AddBackward0>)\n",
            "tensor(13203.2637, grad_fn=<AddBackward0>)\n",
            "tensor(13217.9912, grad_fn=<AddBackward0>)\n",
            "tensor(12865.3789, grad_fn=<AddBackward0>)\n",
            "tensor(13262.2480, grad_fn=<AddBackward0>)\n",
            "tensor(13174.7842, grad_fn=<AddBackward0>)\n",
            "tensor(13274.0547, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [450/469], Reconst Loss: 78.2088, KL Div: 25.4947\n",
            "tensor(13578.1592, grad_fn=<AddBackward0>)\n",
            "tensor(13618.7666, grad_fn=<AddBackward0>)\n",
            "tensor(13229.1309, grad_fn=<AddBackward0>)\n",
            "tensor(13416.3906, grad_fn=<AddBackward0>)\n",
            "tensor(13439.8857, grad_fn=<AddBackward0>)\n",
            "tensor(13254.9541, grad_fn=<AddBackward0>)\n",
            "tensor(13720.5576, grad_fn=<AddBackward0>)\n",
            "tensor(13502.9121, grad_fn=<AddBackward0>)\n",
            "tensor(13377.1436, grad_fn=<AddBackward0>)\n",
            "tensor(13643.2764, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [460/469], Reconst Loss: 80.9760, KL Div: 25.6121\n",
            "tensor(13356.7217, grad_fn=<AddBackward0>)\n",
            "tensor(13421.3096, grad_fn=<AddBackward0>)\n",
            "tensor(13546.6953, grad_fn=<AddBackward0>)\n",
            "tensor(12847.9453, grad_fn=<AddBackward0>)\n",
            "tensor(13127.5830, grad_fn=<AddBackward0>)\n",
            "tensor(13364.7734, grad_fn=<AddBackward0>)\n",
            "tensor(13115., grad_fn=<AddBackward0>)\n",
            "tensor(13261.8203, grad_fn=<AddBackward0>)\n",
            "tensor(9551.6055, grad_fn=<AddBackward0>)\n",
            "tensor(12945.9668, grad_fn=<AddBackward0>)\n",
            "tensor(13178.7314, grad_fn=<AddBackward0>)\n",
            "tensor(12511.0059, grad_fn=<AddBackward0>)\n",
            "tensor(13591.2637, grad_fn=<AddBackward0>)\n",
            "tensor(12861.7285, grad_fn=<AddBackward0>)\n",
            "tensor(13391.2637, grad_fn=<AddBackward0>)\n",
            "tensor(12882.6758, grad_fn=<AddBackward0>)\n",
            "tensor(13173.0137, grad_fn=<AddBackward0>)\n",
            "tensor(13533.4463, grad_fn=<AddBackward0>)\n",
            "tensor(13405.7344, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 79.2893, KL Div: 25.4430\n",
            "tensor(13354.9316, grad_fn=<AddBackward0>)\n",
            "tensor(13218.1309, grad_fn=<AddBackward0>)\n",
            "tensor(13275.6250, grad_fn=<AddBackward0>)\n",
            "tensor(13183.1494, grad_fn=<AddBackward0>)\n",
            "tensor(12574.6270, grad_fn=<AddBackward0>)\n",
            "tensor(13413.7188, grad_fn=<AddBackward0>)\n",
            "tensor(13174.7793, grad_fn=<AddBackward0>)\n",
            "tensor(12648.6836, grad_fn=<AddBackward0>)\n",
            "tensor(12860.7832, grad_fn=<AddBackward0>)\n",
            "tensor(13015.5840, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [20/469], Reconst Loss: 76.4910, KL Div: 25.1932\n",
            "tensor(13489.7334, grad_fn=<AddBackward0>)\n",
            "tensor(13161.8047, grad_fn=<AddBackward0>)\n",
            "tensor(12901.8574, grad_fn=<AddBackward0>)\n",
            "tensor(13040.7881, grad_fn=<AddBackward0>)\n",
            "tensor(13374.8975, grad_fn=<AddBackward0>)\n",
            "tensor(13137.2891, grad_fn=<AddBackward0>)\n",
            "tensor(13579.2754, grad_fn=<AddBackward0>)\n",
            "tensor(13110.0781, grad_fn=<AddBackward0>)\n",
            "tensor(13130.7393, grad_fn=<AddBackward0>)\n",
            "tensor(14000.8760, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [30/469], Reconst Loss: 82.6003, KL Div: 26.7816\n",
            "tensor(13415.1641, grad_fn=<AddBackward0>)\n",
            "tensor(12986.7324, grad_fn=<AddBackward0>)\n",
            "tensor(13357.9824, grad_fn=<AddBackward0>)\n",
            "tensor(13234.9922, grad_fn=<AddBackward0>)\n",
            "tensor(13462.7021, grad_fn=<AddBackward0>)\n",
            "tensor(13172.0107, grad_fn=<AddBackward0>)\n",
            "tensor(13566.9531, grad_fn=<AddBackward0>)\n",
            "tensor(13051.6816, grad_fn=<AddBackward0>)\n",
            "tensor(13521.3809, grad_fn=<AddBackward0>)\n",
            "tensor(13396.8564, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [40/469], Reconst Loss: 79.0894, KL Div: 25.5736\n",
            "tensor(12756.4824, grad_fn=<AddBackward0>)\n",
            "tensor(13176.3467, grad_fn=<AddBackward0>)\n",
            "tensor(12890.3916, grad_fn=<AddBackward0>)\n",
            "tensor(12771.0293, grad_fn=<AddBackward0>)\n",
            "tensor(13600.7148, grad_fn=<AddBackward0>)\n",
            "tensor(13172.2891, grad_fn=<AddBackward0>)\n",
            "tensor(13057.1846, grad_fn=<AddBackward0>)\n",
            "tensor(13152.4434, grad_fn=<AddBackward0>)\n",
            "tensor(12961.2578, grad_fn=<AddBackward0>)\n",
            "tensor(13129.5293, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [50/469], Reconst Loss: 77.5779, KL Div: 24.9966\n",
            "tensor(13620.6426, grad_fn=<AddBackward0>)\n",
            "tensor(13182.9619, grad_fn=<AddBackward0>)\n",
            "tensor(13693.2900, grad_fn=<AddBackward0>)\n",
            "tensor(13367.8135, grad_fn=<AddBackward0>)\n",
            "tensor(12902.6836, grad_fn=<AddBackward0>)\n",
            "tensor(13175.1055, grad_fn=<AddBackward0>)\n",
            "tensor(13476.0898, grad_fn=<AddBackward0>)\n",
            "tensor(13314.7422, grad_fn=<AddBackward0>)\n",
            "tensor(13155.0352, grad_fn=<AddBackward0>)\n",
            "tensor(12885.5566, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [60/469], Reconst Loss: 75.1732, KL Div: 25.4952\n",
            "tensor(13472.1758, grad_fn=<AddBackward0>)\n",
            "tensor(13458.8848, grad_fn=<AddBackward0>)\n",
            "tensor(12917.8135, grad_fn=<AddBackward0>)\n",
            "tensor(12960.0020, grad_fn=<AddBackward0>)\n",
            "tensor(13581.3652, grad_fn=<AddBackward0>)\n",
            "tensor(13781.9990, grad_fn=<AddBackward0>)\n",
            "tensor(13233.4336, grad_fn=<AddBackward0>)\n",
            "tensor(12971.6836, grad_fn=<AddBackward0>)\n",
            "tensor(13096.7471, grad_fn=<AddBackward0>)\n",
            "tensor(13580.3145, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [70/469], Reconst Loss: 80.7189, KL Div: 25.3773\n",
            "tensor(13404.1953, grad_fn=<AddBackward0>)\n",
            "tensor(13001.7607, grad_fn=<AddBackward0>)\n",
            "tensor(13598.3350, grad_fn=<AddBackward0>)\n",
            "tensor(13044.9863, grad_fn=<AddBackward0>)\n",
            "tensor(13120.8496, grad_fn=<AddBackward0>)\n",
            "tensor(12941.7900, grad_fn=<AddBackward0>)\n",
            "tensor(13121.4121, grad_fn=<AddBackward0>)\n",
            "tensor(13420.7002, grad_fn=<AddBackward0>)\n",
            "tensor(13463.4775, grad_fn=<AddBackward0>)\n",
            "tensor(13161.0850, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [80/469], Reconst Loss: 77.4138, KL Div: 25.4072\n",
            "tensor(13145.7598, grad_fn=<AddBackward0>)\n",
            "tensor(12967.4141, grad_fn=<AddBackward0>)\n",
            "tensor(12838.2676, grad_fn=<AddBackward0>)\n",
            "tensor(13529.0732, grad_fn=<AddBackward0>)\n",
            "tensor(13428.1963, grad_fn=<AddBackward0>)\n",
            "tensor(12935.2305, grad_fn=<AddBackward0>)\n",
            "tensor(13115.5693, grad_fn=<AddBackward0>)\n",
            "tensor(13230.8613, grad_fn=<AddBackward0>)\n",
            "tensor(13183.8828, grad_fn=<AddBackward0>)\n",
            "tensor(13632.5625, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [90/469], Reconst Loss: 80.1596, KL Div: 26.3448\n",
            "tensor(13318.0703, grad_fn=<AddBackward0>)\n",
            "tensor(12825.5469, grad_fn=<AddBackward0>)\n",
            "tensor(13470.3477, grad_fn=<AddBackward0>)\n",
            "tensor(13524.0059, grad_fn=<AddBackward0>)\n",
            "tensor(12727.3516, grad_fn=<AddBackward0>)\n",
            "tensor(12885.5098, grad_fn=<AddBackward0>)\n",
            "tensor(13652.8408, grad_fn=<AddBackward0>)\n",
            "tensor(13345.3281, grad_fn=<AddBackward0>)\n",
            "tensor(13167.0576, grad_fn=<AddBackward0>)\n",
            "tensor(13150.9629, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [100/469], Reconst Loss: 77.3200, KL Div: 25.4219\n",
            "tensor(13074.9336, grad_fn=<AddBackward0>)\n",
            "tensor(13509.7119, grad_fn=<AddBackward0>)\n",
            "tensor(13095.6016, grad_fn=<AddBackward0>)\n",
            "tensor(12913.1816, grad_fn=<AddBackward0>)\n",
            "tensor(13185.9980, grad_fn=<AddBackward0>)\n",
            "tensor(13022.4717, grad_fn=<AddBackward0>)\n",
            "tensor(14183.1348, grad_fn=<AddBackward0>)\n",
            "tensor(12981.1348, grad_fn=<AddBackward0>)\n",
            "tensor(13272.3994, grad_fn=<AddBackward0>)\n",
            "tensor(12844.5078, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [110/469], Reconst Loss: 75.1448, KL Div: 25.2029\n",
            "tensor(13271.7764, grad_fn=<AddBackward0>)\n",
            "tensor(13072.4678, grad_fn=<AddBackward0>)\n",
            "tensor(13149.5146, grad_fn=<AddBackward0>)\n",
            "tensor(12536.6357, grad_fn=<AddBackward0>)\n",
            "tensor(13020.1514, grad_fn=<AddBackward0>)\n",
            "tensor(13649.2832, grad_fn=<AddBackward0>)\n",
            "tensor(12957.0938, grad_fn=<AddBackward0>)\n",
            "tensor(13320.9912, grad_fn=<AddBackward0>)\n",
            "tensor(13381.2441, grad_fn=<AddBackward0>)\n",
            "tensor(13404.4648, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [120/469], Reconst Loss: 79.1052, KL Div: 25.6172\n",
            "tensor(12818.6445, grad_fn=<AddBackward0>)\n",
            "tensor(13594.1855, grad_fn=<AddBackward0>)\n",
            "tensor(12892.1514, grad_fn=<AddBackward0>)\n",
            "tensor(13169.0684, grad_fn=<AddBackward0>)\n",
            "tensor(12799.3887, grad_fn=<AddBackward0>)\n",
            "tensor(13460.8066, grad_fn=<AddBackward0>)\n",
            "tensor(13343.3525, grad_fn=<AddBackward0>)\n",
            "tensor(12878.6836, grad_fn=<AddBackward0>)\n",
            "tensor(13489.1533, grad_fn=<AddBackward0>)\n",
            "tensor(13432.3574, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [130/469], Reconst Loss: 79.0791, KL Div: 25.8612\n",
            "tensor(13129.6631, grad_fn=<AddBackward0>)\n",
            "tensor(12983.3125, grad_fn=<AddBackward0>)\n",
            "tensor(13065.0557, grad_fn=<AddBackward0>)\n",
            "tensor(13174., grad_fn=<AddBackward0>)\n",
            "tensor(13317.9434, grad_fn=<AddBackward0>)\n",
            "tensor(13187.7148, grad_fn=<AddBackward0>)\n",
            "tensor(12702.0693, grad_fn=<AddBackward0>)\n",
            "tensor(13530.2637, grad_fn=<AddBackward0>)\n",
            "tensor(12905.3047, grad_fn=<AddBackward0>)\n",
            "tensor(13676.2354, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [140/469], Reconst Loss: 81.1460, KL Div: 25.6995\n",
            "tensor(13433.9131, grad_fn=<AddBackward0>)\n",
            "tensor(13105.2891, grad_fn=<AddBackward0>)\n",
            "tensor(13305.2695, grad_fn=<AddBackward0>)\n",
            "tensor(12858.0645, grad_fn=<AddBackward0>)\n",
            "tensor(12744.7754, grad_fn=<AddBackward0>)\n",
            "tensor(13111.3740, grad_fn=<AddBackward0>)\n",
            "tensor(12987.4648, grad_fn=<AddBackward0>)\n",
            "tensor(13408.1885, grad_fn=<AddBackward0>)\n",
            "tensor(12789.2324, grad_fn=<AddBackward0>)\n",
            "tensor(13394.3984, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [150/469], Reconst Loss: 78.8091, KL Div: 25.8346\n",
            "tensor(13802.8789, grad_fn=<AddBackward0>)\n",
            "tensor(12811.9355, grad_fn=<AddBackward0>)\n",
            "tensor(13269.2480, grad_fn=<AddBackward0>)\n",
            "tensor(13302.7188, grad_fn=<AddBackward0>)\n",
            "tensor(13387.9102, grad_fn=<AddBackward0>)\n",
            "tensor(13434.9375, grad_fn=<AddBackward0>)\n",
            "tensor(13847.5645, grad_fn=<AddBackward0>)\n",
            "tensor(13138.3633, grad_fn=<AddBackward0>)\n",
            "tensor(13216.5781, grad_fn=<AddBackward0>)\n",
            "tensor(13721.2510, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [160/469], Reconst Loss: 80.9864, KL Div: 26.2109\n",
            "tensor(13010.0215, grad_fn=<AddBackward0>)\n",
            "tensor(12490.5918, grad_fn=<AddBackward0>)\n",
            "tensor(13074.2012, grad_fn=<AddBackward0>)\n",
            "tensor(13515.3555, grad_fn=<AddBackward0>)\n",
            "tensor(13567.0977, grad_fn=<AddBackward0>)\n",
            "tensor(13045.9902, grad_fn=<AddBackward0>)\n",
            "tensor(13303.1270, grad_fn=<AddBackward0>)\n",
            "tensor(13046.0537, grad_fn=<AddBackward0>)\n",
            "tensor(12859.0117, grad_fn=<AddBackward0>)\n",
            "tensor(13544.9355, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [170/469], Reconst Loss: 80.2918, KL Div: 25.5280\n",
            "tensor(13013.7051, grad_fn=<AddBackward0>)\n",
            "tensor(13266.3730, grad_fn=<AddBackward0>)\n",
            "tensor(13477.7031, grad_fn=<AddBackward0>)\n",
            "tensor(13781.8682, grad_fn=<AddBackward0>)\n",
            "tensor(13200.8721, grad_fn=<AddBackward0>)\n",
            "tensor(12989.9229, grad_fn=<AddBackward0>)\n",
            "tensor(13505.8740, grad_fn=<AddBackward0>)\n",
            "tensor(13226.0303, grad_fn=<AddBackward0>)\n",
            "tensor(13291.9443, grad_fn=<AddBackward0>)\n",
            "tensor(12997.2285, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [180/469], Reconst Loss: 76.5360, KL Div: 25.0049\n",
            "tensor(12964.5664, grad_fn=<AddBackward0>)\n",
            "tensor(13470.0410, grad_fn=<AddBackward0>)\n",
            "tensor(13433.2383, grad_fn=<AddBackward0>)\n",
            "tensor(13487.5742, grad_fn=<AddBackward0>)\n",
            "tensor(13614.7988, grad_fn=<AddBackward0>)\n",
            "tensor(12935.5605, grad_fn=<AddBackward0>)\n",
            "tensor(13659.2129, grad_fn=<AddBackward0>)\n",
            "tensor(13380.2578, grad_fn=<AddBackward0>)\n",
            "tensor(13468.4873, grad_fn=<AddBackward0>)\n",
            "tensor(13466.4580, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [190/469], Reconst Loss: 78.9975, KL Div: 26.2092\n",
            "tensor(13063.0732, grad_fn=<AddBackward0>)\n",
            "tensor(13015.3428, grad_fn=<AddBackward0>)\n",
            "tensor(13428.5186, grad_fn=<AddBackward0>)\n",
            "tensor(13501.3125, grad_fn=<AddBackward0>)\n",
            "tensor(13236.0566, grad_fn=<AddBackward0>)\n",
            "tensor(12639.7910, grad_fn=<AddBackward0>)\n",
            "tensor(13266.6328, grad_fn=<AddBackward0>)\n",
            "tensor(13203.5449, grad_fn=<AddBackward0>)\n",
            "tensor(13123.8115, grad_fn=<AddBackward0>)\n",
            "tensor(13318.9805, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [200/469], Reconst Loss: 78.8498, KL Div: 25.2047\n",
            "tensor(12919.7686, grad_fn=<AddBackward0>)\n",
            "tensor(13187.1484, grad_fn=<AddBackward0>)\n",
            "tensor(13000.9922, grad_fn=<AddBackward0>)\n",
            "tensor(13653.9883, grad_fn=<AddBackward0>)\n",
            "tensor(12937.5908, grad_fn=<AddBackward0>)\n",
            "tensor(12952.5732, grad_fn=<AddBackward0>)\n",
            "tensor(12808.8174, grad_fn=<AddBackward0>)\n",
            "tensor(13268.3877, grad_fn=<AddBackward0>)\n",
            "tensor(13380.7949, grad_fn=<AddBackward0>)\n",
            "tensor(13000.9824, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [210/469], Reconst Loss: 76.4092, KL Div: 25.1610\n",
            "tensor(13250.0156, grad_fn=<AddBackward0>)\n",
            "tensor(13631.5449, grad_fn=<AddBackward0>)\n",
            "tensor(13314.4561, grad_fn=<AddBackward0>)\n",
            "tensor(12814.5615, grad_fn=<AddBackward0>)\n",
            "tensor(13228.4678, grad_fn=<AddBackward0>)\n",
            "tensor(13338.7441, grad_fn=<AddBackward0>)\n",
            "tensor(13095.2158, grad_fn=<AddBackward0>)\n",
            "tensor(13051.4326, grad_fn=<AddBackward0>)\n",
            "tensor(13164.0898, grad_fn=<AddBackward0>)\n",
            "tensor(13054.4434, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [220/469], Reconst Loss: 77.2751, KL Div: 24.7127\n",
            "tensor(13178.1133, grad_fn=<AddBackward0>)\n",
            "tensor(13631.0420, grad_fn=<AddBackward0>)\n",
            "tensor(13468.9443, grad_fn=<AddBackward0>)\n",
            "tensor(13247.8916, grad_fn=<AddBackward0>)\n",
            "tensor(13491.7939, grad_fn=<AddBackward0>)\n",
            "tensor(12890.1807, grad_fn=<AddBackward0>)\n",
            "tensor(13289.2471, grad_fn=<AddBackward0>)\n",
            "tensor(13852.9316, grad_fn=<AddBackward0>)\n",
            "tensor(13536.3018, grad_fn=<AddBackward0>)\n",
            "tensor(13033.4834, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [230/469], Reconst Loss: 76.7588, KL Div: 25.0653\n",
            "tensor(13161.5186, grad_fn=<AddBackward0>)\n",
            "tensor(13313.5215, grad_fn=<AddBackward0>)\n",
            "tensor(13354.4121, grad_fn=<AddBackward0>)\n",
            "tensor(13400.5273, grad_fn=<AddBackward0>)\n",
            "tensor(12962.5840, grad_fn=<AddBackward0>)\n",
            "tensor(12981.7148, grad_fn=<AddBackward0>)\n",
            "tensor(13224.1289, grad_fn=<AddBackward0>)\n",
            "tensor(13141.6328, grad_fn=<AddBackward0>)\n",
            "tensor(12930.1240, grad_fn=<AddBackward0>)\n",
            "tensor(13540.3623, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [240/469], Reconst Loss: 80.0382, KL Div: 25.7459\n",
            "tensor(12469.3770, grad_fn=<AddBackward0>)\n",
            "tensor(13252.7930, grad_fn=<AddBackward0>)\n",
            "tensor(13581.3760, grad_fn=<AddBackward0>)\n",
            "tensor(13068.3633, grad_fn=<AddBackward0>)\n",
            "tensor(13169.3516, grad_fn=<AddBackward0>)\n",
            "tensor(13309.8848, grad_fn=<AddBackward0>)\n",
            "tensor(13747.3633, grad_fn=<AddBackward0>)\n",
            "tensor(13475.9834, grad_fn=<AddBackward0>)\n",
            "tensor(12738.0020, grad_fn=<AddBackward0>)\n",
            "tensor(13560.3574, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [250/469], Reconst Loss: 80.4184, KL Div: 25.5219\n",
            "tensor(13197.8994, grad_fn=<AddBackward0>)\n",
            "tensor(13434.4609, grad_fn=<AddBackward0>)\n",
            "tensor(13455.0049, grad_fn=<AddBackward0>)\n",
            "tensor(13223.0078, grad_fn=<AddBackward0>)\n",
            "tensor(13024.8926, grad_fn=<AddBackward0>)\n",
            "tensor(13552.0215, grad_fn=<AddBackward0>)\n",
            "tensor(13577.8906, grad_fn=<AddBackward0>)\n",
            "tensor(13487.3154, grad_fn=<AddBackward0>)\n",
            "tensor(13297.3232, grad_fn=<AddBackward0>)\n",
            "tensor(13524.5713, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [260/469], Reconst Loss: 79.0947, KL Div: 26.5660\n",
            "tensor(13253.2637, grad_fn=<AddBackward0>)\n",
            "tensor(13467.3906, grad_fn=<AddBackward0>)\n",
            "tensor(13209.6846, grad_fn=<AddBackward0>)\n",
            "tensor(13058.6641, grad_fn=<AddBackward0>)\n",
            "tensor(13290.6133, grad_fn=<AddBackward0>)\n",
            "tensor(12655.5195, grad_fn=<AddBackward0>)\n",
            "tensor(13995.2979, grad_fn=<AddBackward0>)\n",
            "tensor(13450.2773, grad_fn=<AddBackward0>)\n",
            "tensor(12907.3730, grad_fn=<AddBackward0>)\n",
            "tensor(12717.8271, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [270/469], Reconst Loss: 73.9925, KL Div: 25.3655\n",
            "tensor(12894.4619, grad_fn=<AddBackward0>)\n",
            "tensor(13707.2344, grad_fn=<AddBackward0>)\n",
            "tensor(13318.1094, grad_fn=<AddBackward0>)\n",
            "tensor(13235.3594, grad_fn=<AddBackward0>)\n",
            "tensor(13025.2451, grad_fn=<AddBackward0>)\n",
            "tensor(12851.3105, grad_fn=<AddBackward0>)\n",
            "tensor(13183.8408, grad_fn=<AddBackward0>)\n",
            "tensor(13488.5508, grad_fn=<AddBackward0>)\n",
            "tensor(12802.6660, grad_fn=<AddBackward0>)\n",
            "tensor(12691.3828, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [280/469], Reconst Loss: 73.7202, KL Div: 25.4313\n",
            "tensor(13402.7842, grad_fn=<AddBackward0>)\n",
            "tensor(13414.8945, grad_fn=<AddBackward0>)\n",
            "tensor(13453.4941, grad_fn=<AddBackward0>)\n",
            "tensor(13696.0703, grad_fn=<AddBackward0>)\n",
            "tensor(13235.4346, grad_fn=<AddBackward0>)\n",
            "tensor(13243.8965, grad_fn=<AddBackward0>)\n",
            "tensor(13373.9365, grad_fn=<AddBackward0>)\n",
            "tensor(13263.7256, grad_fn=<AddBackward0>)\n",
            "tensor(12875.9854, grad_fn=<AddBackward0>)\n",
            "tensor(12446.9160, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [290/469], Reconst Loss: 73.3793, KL Div: 23.8622\n",
            "tensor(13133.9199, grad_fn=<AddBackward0>)\n",
            "tensor(13181.6504, grad_fn=<AddBackward0>)\n",
            "tensor(13311.1777, grad_fn=<AddBackward0>)\n",
            "tensor(13347.9482, grad_fn=<AddBackward0>)\n",
            "tensor(13128.5752, grad_fn=<AddBackward0>)\n",
            "tensor(13054.1504, grad_fn=<AddBackward0>)\n",
            "tensor(12793.1631, grad_fn=<AddBackward0>)\n",
            "tensor(13271.8945, grad_fn=<AddBackward0>)\n",
            "tensor(13544.0879, grad_fn=<AddBackward0>)\n",
            "tensor(13262.5244, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [300/469], Reconst Loss: 78.1817, KL Div: 25.4317\n",
            "tensor(13352.7383, grad_fn=<AddBackward0>)\n",
            "tensor(13297.3848, grad_fn=<AddBackward0>)\n",
            "tensor(13944.9736, grad_fn=<AddBackward0>)\n",
            "tensor(13095.7383, grad_fn=<AddBackward0>)\n",
            "tensor(13139.2168, grad_fn=<AddBackward0>)\n",
            "tensor(13179.5078, grad_fn=<AddBackward0>)\n",
            "tensor(13842.0811, grad_fn=<AddBackward0>)\n",
            "tensor(13428.3008, grad_fn=<AddBackward0>)\n",
            "tensor(13366.6152, grad_fn=<AddBackward0>)\n",
            "tensor(13210.4727, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [310/469], Reconst Loss: 77.8151, KL Div: 25.3917\n",
            "tensor(13025.3516, grad_fn=<AddBackward0>)\n",
            "tensor(13512.2324, grad_fn=<AddBackward0>)\n",
            "tensor(13371.6748, grad_fn=<AddBackward0>)\n",
            "tensor(13803.6113, grad_fn=<AddBackward0>)\n",
            "tensor(13243.6025, grad_fn=<AddBackward0>)\n",
            "tensor(13055.5527, grad_fn=<AddBackward0>)\n",
            "tensor(13215.5645, grad_fn=<AddBackward0>)\n",
            "tensor(13565.7217, grad_fn=<AddBackward0>)\n",
            "tensor(12982.4629, grad_fn=<AddBackward0>)\n",
            "tensor(13421.9922, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [320/469], Reconst Loss: 78.8276, KL Div: 26.0317\n",
            "tensor(12784.9492, grad_fn=<AddBackward0>)\n",
            "tensor(13205.0098, grad_fn=<AddBackward0>)\n",
            "tensor(13358.7041, grad_fn=<AddBackward0>)\n",
            "tensor(13407.0986, grad_fn=<AddBackward0>)\n",
            "tensor(13138.7793, grad_fn=<AddBackward0>)\n",
            "tensor(13564.1572, grad_fn=<AddBackward0>)\n",
            "tensor(13426.2471, grad_fn=<AddBackward0>)\n",
            "tensor(13008.2656, grad_fn=<AddBackward0>)\n",
            "tensor(13120.1484, grad_fn=<AddBackward0>)\n",
            "tensor(13131.5840, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [330/469], Reconst Loss: 77.2899, KL Div: 25.3006\n",
            "tensor(13207.9121, grad_fn=<AddBackward0>)\n",
            "tensor(13157.5137, grad_fn=<AddBackward0>)\n",
            "tensor(13251.0918, grad_fn=<AddBackward0>)\n",
            "tensor(13297.4473, grad_fn=<AddBackward0>)\n",
            "tensor(13165.3623, grad_fn=<AddBackward0>)\n",
            "tensor(13215.1387, grad_fn=<AddBackward0>)\n",
            "tensor(13619.2578, grad_fn=<AddBackward0>)\n",
            "tensor(12591.6875, grad_fn=<AddBackward0>)\n",
            "tensor(13221.6699, grad_fn=<AddBackward0>)\n",
            "tensor(13806.3584, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [340/469], Reconst Loss: 81.4912, KL Div: 26.3710\n",
            "tensor(13861.8457, grad_fn=<AddBackward0>)\n",
            "tensor(13079.0020, grad_fn=<AddBackward0>)\n",
            "tensor(13076.8809, grad_fn=<AddBackward0>)\n",
            "tensor(13965.8691, grad_fn=<AddBackward0>)\n",
            "tensor(13365.6582, grad_fn=<AddBackward0>)\n",
            "tensor(12920.0059, grad_fn=<AddBackward0>)\n",
            "tensor(13235.7080, grad_fn=<AddBackward0>)\n",
            "tensor(12947.1338, grad_fn=<AddBackward0>)\n",
            "tensor(13197.7363, grad_fn=<AddBackward0>)\n",
            "tensor(12932.1787, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [350/469], Reconst Loss: 76.2191, KL Div: 24.8135\n",
            "tensor(13338.1963, grad_fn=<AddBackward0>)\n",
            "tensor(13248.3105, grad_fn=<AddBackward0>)\n",
            "tensor(13216.9531, grad_fn=<AddBackward0>)\n",
            "tensor(13405.5645, grad_fn=<AddBackward0>)\n",
            "tensor(13169.9023, grad_fn=<AddBackward0>)\n",
            "tensor(13399.7324, grad_fn=<AddBackward0>)\n",
            "tensor(13372.5928, grad_fn=<AddBackward0>)\n",
            "tensor(13814.9229, grad_fn=<AddBackward0>)\n",
            "tensor(13166.8398, grad_fn=<AddBackward0>)\n",
            "tensor(12939.9561, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [360/469], Reconst Loss: 75.2330, KL Div: 25.8604\n",
            "tensor(13329.9648, grad_fn=<AddBackward0>)\n",
            "tensor(13717.1836, grad_fn=<AddBackward0>)\n",
            "tensor(12936.8027, grad_fn=<AddBackward0>)\n",
            "tensor(13137.7861, grad_fn=<AddBackward0>)\n",
            "tensor(13628.1914, grad_fn=<AddBackward0>)\n",
            "tensor(13398.7979, grad_fn=<AddBackward0>)\n",
            "tensor(13164.7051, grad_fn=<AddBackward0>)\n",
            "tensor(13329.9375, grad_fn=<AddBackward0>)\n",
            "tensor(13005.4307, grad_fn=<AddBackward0>)\n",
            "tensor(13023.7393, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [370/469], Reconst Loss: 76.6050, KL Div: 25.1430\n",
            "tensor(13240.8496, grad_fn=<AddBackward0>)\n",
            "tensor(13577.5391, grad_fn=<AddBackward0>)\n",
            "tensor(13601.0039, grad_fn=<AddBackward0>)\n",
            "tensor(13055.7734, grad_fn=<AddBackward0>)\n",
            "tensor(13120.5605, grad_fn=<AddBackward0>)\n",
            "tensor(13724.3447, grad_fn=<AddBackward0>)\n",
            "tensor(12955.4766, grad_fn=<AddBackward0>)\n",
            "tensor(12796.8818, grad_fn=<AddBackward0>)\n",
            "tensor(13177.4648, grad_fn=<AddBackward0>)\n",
            "tensor(13494.5195, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [380/469], Reconst Loss: 80.3801, KL Div: 25.0459\n",
            "tensor(13347.7188, grad_fn=<AddBackward0>)\n",
            "tensor(13208.5078, grad_fn=<AddBackward0>)\n",
            "tensor(12921.5029, grad_fn=<AddBackward0>)\n",
            "tensor(12963.0117, grad_fn=<AddBackward0>)\n",
            "tensor(13292.8906, grad_fn=<AddBackward0>)\n",
            "tensor(13182.5684, grad_fn=<AddBackward0>)\n",
            "tensor(13375.3350, grad_fn=<AddBackward0>)\n",
            "tensor(13570.0479, grad_fn=<AddBackward0>)\n",
            "tensor(13196.3965, grad_fn=<AddBackward0>)\n",
            "tensor(13315.8193, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [390/469], Reconst Loss: 78.4393, KL Div: 25.5905\n",
            "tensor(13145.0176, grad_fn=<AddBackward0>)\n",
            "tensor(13148.1289, grad_fn=<AddBackward0>)\n",
            "tensor(13103.2646, grad_fn=<AddBackward0>)\n",
            "tensor(13450.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13322.1016, grad_fn=<AddBackward0>)\n",
            "tensor(13660.7559, grad_fn=<AddBackward0>)\n",
            "tensor(13692.7930, grad_fn=<AddBackward0>)\n",
            "tensor(13194.6084, grad_fn=<AddBackward0>)\n",
            "tensor(13317.7871, grad_fn=<AddBackward0>)\n",
            "tensor(13093.0781, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [400/469], Reconst Loss: 77.5863, KL Div: 24.7034\n",
            "tensor(13315.0430, grad_fn=<AddBackward0>)\n",
            "tensor(12837.2559, grad_fn=<AddBackward0>)\n",
            "tensor(13412.3027, grad_fn=<AddBackward0>)\n",
            "tensor(13070.6484, grad_fn=<AddBackward0>)\n",
            "tensor(12947.3242, grad_fn=<AddBackward0>)\n",
            "tensor(12682.4219, grad_fn=<AddBackward0>)\n",
            "tensor(13350.2402, grad_fn=<AddBackward0>)\n",
            "tensor(13973.3926, grad_fn=<AddBackward0>)\n",
            "tensor(13320.4004, grad_fn=<AddBackward0>)\n",
            "tensor(13053.3574, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [410/469], Reconst Loss: 76.3984, KL Div: 25.5810\n",
            "tensor(12933.7256, grad_fn=<AddBackward0>)\n",
            "tensor(13291.6143, grad_fn=<AddBackward0>)\n",
            "tensor(13852.3320, grad_fn=<AddBackward0>)\n",
            "tensor(13652.9336, grad_fn=<AddBackward0>)\n",
            "tensor(13388.6406, grad_fn=<AddBackward0>)\n",
            "tensor(13265.4980, grad_fn=<AddBackward0>)\n",
            "tensor(13072.6621, grad_fn=<AddBackward0>)\n",
            "tensor(13326.2129, grad_fn=<AddBackward0>)\n",
            "tensor(12756.3828, grad_fn=<AddBackward0>)\n",
            "tensor(12839.1719, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [420/469], Reconst Loss: 75.3458, KL Div: 24.9602\n",
            "tensor(13422.7139, grad_fn=<AddBackward0>)\n",
            "tensor(13000.4082, grad_fn=<AddBackward0>)\n",
            "tensor(13075.3242, grad_fn=<AddBackward0>)\n",
            "tensor(13458.9355, grad_fn=<AddBackward0>)\n",
            "tensor(13201.9893, grad_fn=<AddBackward0>)\n",
            "tensor(13054.0371, grad_fn=<AddBackward0>)\n",
            "tensor(13466.9492, grad_fn=<AddBackward0>)\n",
            "tensor(13730.8750, grad_fn=<AddBackward0>)\n",
            "tensor(13267.3418, grad_fn=<AddBackward0>)\n",
            "tensor(13216.2461, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [430/469], Reconst Loss: 77.2999, KL Div: 25.9520\n",
            "tensor(13299.6562, grad_fn=<AddBackward0>)\n",
            "tensor(12859.5332, grad_fn=<AddBackward0>)\n",
            "tensor(12657.2949, grad_fn=<AddBackward0>)\n",
            "tensor(12952.3662, grad_fn=<AddBackward0>)\n",
            "tensor(13305.7939, grad_fn=<AddBackward0>)\n",
            "tensor(13087.5176, grad_fn=<AddBackward0>)\n",
            "tensor(13838.1807, grad_fn=<AddBackward0>)\n",
            "tensor(12885.6289, grad_fn=<AddBackward0>)\n",
            "tensor(13558.5469, grad_fn=<AddBackward0>)\n",
            "tensor(13303.4844, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [440/469], Reconst Loss: 78.7407, KL Div: 25.1928\n",
            "tensor(13307.4365, grad_fn=<AddBackward0>)\n",
            "tensor(13096.2178, grad_fn=<AddBackward0>)\n",
            "tensor(13223.7207, grad_fn=<AddBackward0>)\n",
            "tensor(13726.1631, grad_fn=<AddBackward0>)\n",
            "tensor(13706.4219, grad_fn=<AddBackward0>)\n",
            "tensor(13133.6611, grad_fn=<AddBackward0>)\n",
            "tensor(13688.5859, grad_fn=<AddBackward0>)\n",
            "tensor(12906.5176, grad_fn=<AddBackward0>)\n",
            "tensor(12914.0439, grad_fn=<AddBackward0>)\n",
            "tensor(13110.2305, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [450/469], Reconst Loss: 77.5423, KL Div: 24.8813\n",
            "tensor(12881.3145, grad_fn=<AddBackward0>)\n",
            "tensor(12640.8877, grad_fn=<AddBackward0>)\n",
            "tensor(13260.5703, grad_fn=<AddBackward0>)\n",
            "tensor(13130.6836, grad_fn=<AddBackward0>)\n",
            "tensor(13334.6504, grad_fn=<AddBackward0>)\n",
            "tensor(13123.4336, grad_fn=<AddBackward0>)\n",
            "tensor(12667.5322, grad_fn=<AddBackward0>)\n",
            "tensor(12995.4531, grad_fn=<AddBackward0>)\n",
            "tensor(13449.0107, grad_fn=<AddBackward0>)\n",
            "tensor(13608.4004, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [460/469], Reconst Loss: 79.9796, KL Div: 26.3360\n",
            "tensor(13659.6953, grad_fn=<AddBackward0>)\n",
            "tensor(13629.6045, grad_fn=<AddBackward0>)\n",
            "tensor(13258.6152, grad_fn=<AddBackward0>)\n",
            "tensor(13450.8301, grad_fn=<AddBackward0>)\n",
            "tensor(12960.2031, grad_fn=<AddBackward0>)\n",
            "tensor(12918.3809, grad_fn=<AddBackward0>)\n",
            "tensor(12743.7969, grad_fn=<AddBackward0>)\n",
            "tensor(12870.2490, grad_fn=<AddBackward0>)\n",
            "tensor(9983.8984, grad_fn=<AddBackward0>)\n",
            "tensor(12633.7139, grad_fn=<AddBackward0>)\n",
            "tensor(13377.4238, grad_fn=<AddBackward0>)\n",
            "tensor(13311.4277, grad_fn=<AddBackward0>)\n",
            "tensor(13253.7754, grad_fn=<AddBackward0>)\n",
            "tensor(13241.5225, grad_fn=<AddBackward0>)\n",
            "tensor(13183.5898, grad_fn=<AddBackward0>)\n",
            "tensor(12975.0723, grad_fn=<AddBackward0>)\n",
            "tensor(13456.8965, grad_fn=<AddBackward0>)\n",
            "tensor(13154.7002, grad_fn=<AddBackward0>)\n",
            "tensor(13151.9814, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 77.4887, KL Div: 25.2612\n",
            "tensor(13299.8379, grad_fn=<AddBackward0>)\n",
            "tensor(12917.8945, grad_fn=<AddBackward0>)\n",
            "tensor(12874.5244, grad_fn=<AddBackward0>)\n",
            "tensor(12979.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13337.2988, grad_fn=<AddBackward0>)\n",
            "tensor(13153.4414, grad_fn=<AddBackward0>)\n",
            "tensor(13248.3633, grad_fn=<AddBackward0>)\n",
            "tensor(12867.6006, grad_fn=<AddBackward0>)\n",
            "tensor(13437.8252, grad_fn=<AddBackward0>)\n",
            "tensor(12959.6582, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [20/469], Reconst Loss: 76.2531, KL Div: 24.9942\n",
            "tensor(13309.8467, grad_fn=<AddBackward0>)\n",
            "tensor(13546.2031, grad_fn=<AddBackward0>)\n",
            "tensor(12903.3926, grad_fn=<AddBackward0>)\n",
            "tensor(13122.6436, grad_fn=<AddBackward0>)\n",
            "tensor(13401.6904, grad_fn=<AddBackward0>)\n",
            "tensor(13671.8301, grad_fn=<AddBackward0>)\n",
            "tensor(13208.3242, grad_fn=<AddBackward0>)\n",
            "tensor(13332.2168, grad_fn=<AddBackward0>)\n",
            "tensor(13106.4326, grad_fn=<AddBackward0>)\n",
            "tensor(12851.8477, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [30/469], Reconst Loss: 76.0539, KL Div: 24.3511\n",
            "tensor(13068.9385, grad_fn=<AddBackward0>)\n",
            "tensor(13099.9258, grad_fn=<AddBackward0>)\n",
            "tensor(13222.6367, grad_fn=<AddBackward0>)\n",
            "tensor(13161.8838, grad_fn=<AddBackward0>)\n",
            "tensor(12768.0244, grad_fn=<AddBackward0>)\n",
            "tensor(13045.7725, grad_fn=<AddBackward0>)\n",
            "tensor(13189.9932, grad_fn=<AddBackward0>)\n",
            "tensor(13231.1953, grad_fn=<AddBackward0>)\n",
            "tensor(13303.6992, grad_fn=<AddBackward0>)\n",
            "tensor(12859.8457, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [40/469], Reconst Loss: 75.4200, KL Div: 25.0475\n",
            "tensor(13474.3398, grad_fn=<AddBackward0>)\n",
            "tensor(13452.6133, grad_fn=<AddBackward0>)\n",
            "tensor(13406.0938, grad_fn=<AddBackward0>)\n",
            "tensor(12993.8535, grad_fn=<AddBackward0>)\n",
            "tensor(12852.9248, grad_fn=<AddBackward0>)\n",
            "tensor(13213.9287, grad_fn=<AddBackward0>)\n",
            "tensor(13303.5010, grad_fn=<AddBackward0>)\n",
            "tensor(13187.9648, grad_fn=<AddBackward0>)\n",
            "tensor(12928.6533, grad_fn=<AddBackward0>)\n",
            "tensor(13090.3496, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [50/469], Reconst Loss: 77.2107, KL Div: 25.0576\n",
            "tensor(12798.7891, grad_fn=<AddBackward0>)\n",
            "tensor(12724.4883, grad_fn=<AddBackward0>)\n",
            "tensor(13567.5332, grad_fn=<AddBackward0>)\n",
            "tensor(13267.6035, grad_fn=<AddBackward0>)\n",
            "tensor(13187.9121, grad_fn=<AddBackward0>)\n",
            "tensor(12889.8398, grad_fn=<AddBackward0>)\n",
            "tensor(13372.1094, grad_fn=<AddBackward0>)\n",
            "tensor(13150.0215, grad_fn=<AddBackward0>)\n",
            "tensor(13496.1367, grad_fn=<AddBackward0>)\n",
            "tensor(12898.9385, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [60/469], Reconst Loss: 76.1171, KL Div: 24.6559\n",
            "tensor(13067.1816, grad_fn=<AddBackward0>)\n",
            "tensor(13673.0479, grad_fn=<AddBackward0>)\n",
            "tensor(13359.7949, grad_fn=<AddBackward0>)\n",
            "tensor(13606.1455, grad_fn=<AddBackward0>)\n",
            "tensor(13380.0400, grad_fn=<AddBackward0>)\n",
            "tensor(13254.5117, grad_fn=<AddBackward0>)\n",
            "tensor(13331.7422, grad_fn=<AddBackward0>)\n",
            "tensor(12602.6465, grad_fn=<AddBackward0>)\n",
            "tensor(13429.2559, grad_fn=<AddBackward0>)\n",
            "tensor(13379.0312, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [70/469], Reconst Loss: 78.6960, KL Div: 25.8276\n",
            "tensor(12825.6816, grad_fn=<AddBackward0>)\n",
            "tensor(13181.7480, grad_fn=<AddBackward0>)\n",
            "tensor(13361.5742, grad_fn=<AddBackward0>)\n",
            "tensor(13059.2500, grad_fn=<AddBackward0>)\n",
            "tensor(13175.2109, grad_fn=<AddBackward0>)\n",
            "tensor(12660.7725, grad_fn=<AddBackward0>)\n",
            "tensor(12951.1719, grad_fn=<AddBackward0>)\n",
            "tensor(13299.2822, grad_fn=<AddBackward0>)\n",
            "tensor(12996.2393, grad_fn=<AddBackward0>)\n",
            "tensor(13510.7129, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [80/469], Reconst Loss: 80.2953, KL Div: 25.2571\n",
            "tensor(12755.1816, grad_fn=<AddBackward0>)\n",
            "tensor(13284.5137, grad_fn=<AddBackward0>)\n",
            "tensor(13164.6592, grad_fn=<AddBackward0>)\n",
            "tensor(12923.8262, grad_fn=<AddBackward0>)\n",
            "tensor(13325.4316, grad_fn=<AddBackward0>)\n",
            "tensor(13017.4355, grad_fn=<AddBackward0>)\n",
            "tensor(13054.2373, grad_fn=<AddBackward0>)\n",
            "tensor(13170.0225, grad_fn=<AddBackward0>)\n",
            "tensor(12638.9043, grad_fn=<AddBackward0>)\n",
            "tensor(13360.3516, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [90/469], Reconst Loss: 79.1020, KL Div: 25.2758\n",
            "tensor(13921.7979, grad_fn=<AddBackward0>)\n",
            "tensor(13187.1426, grad_fn=<AddBackward0>)\n",
            "tensor(12855.5859, grad_fn=<AddBackward0>)\n",
            "tensor(13517.8574, grad_fn=<AddBackward0>)\n",
            "tensor(13578.9385, grad_fn=<AddBackward0>)\n",
            "tensor(13453.4424, grad_fn=<AddBackward0>)\n",
            "tensor(13223.3906, grad_fn=<AddBackward0>)\n",
            "tensor(12979.7441, grad_fn=<AddBackward0>)\n",
            "tensor(13238.2559, grad_fn=<AddBackward0>)\n",
            "tensor(13079.5195, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [100/469], Reconst Loss: 76.7624, KL Div: 25.4213\n",
            "tensor(13039.3516, grad_fn=<AddBackward0>)\n",
            "tensor(13287.8711, grad_fn=<AddBackward0>)\n",
            "tensor(13905.4922, grad_fn=<AddBackward0>)\n",
            "tensor(12608.4512, grad_fn=<AddBackward0>)\n",
            "tensor(13633.6016, grad_fn=<AddBackward0>)\n",
            "tensor(13415.2568, grad_fn=<AddBackward0>)\n",
            "tensor(13330.9619, grad_fn=<AddBackward0>)\n",
            "tensor(12924.5684, grad_fn=<AddBackward0>)\n",
            "tensor(13000.4023, grad_fn=<AddBackward0>)\n",
            "tensor(13283.7246, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [110/469], Reconst Loss: 77.6234, KL Div: 26.1557\n",
            "tensor(13242.6748, grad_fn=<AddBackward0>)\n",
            "tensor(13074.4180, grad_fn=<AddBackward0>)\n",
            "tensor(12916.1953, grad_fn=<AddBackward0>)\n",
            "tensor(12796.3740, grad_fn=<AddBackward0>)\n",
            "tensor(12919.7266, grad_fn=<AddBackward0>)\n",
            "tensor(12716.5098, grad_fn=<AddBackward0>)\n",
            "tensor(12874.7998, grad_fn=<AddBackward0>)\n",
            "tensor(12961.8252, grad_fn=<AddBackward0>)\n",
            "tensor(13238.1104, grad_fn=<AddBackward0>)\n",
            "tensor(13395.6299, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [120/469], Reconst Loss: 79.1416, KL Div: 25.5117\n",
            "tensor(12749.1357, grad_fn=<AddBackward0>)\n",
            "tensor(13468.9375, grad_fn=<AddBackward0>)\n",
            "tensor(12968.6006, grad_fn=<AddBackward0>)\n",
            "tensor(13384.7148, grad_fn=<AddBackward0>)\n",
            "tensor(12961.4609, grad_fn=<AddBackward0>)\n",
            "tensor(13808.6719, grad_fn=<AddBackward0>)\n",
            "tensor(12854.3564, grad_fn=<AddBackward0>)\n",
            "tensor(13459.3438, grad_fn=<AddBackward0>)\n",
            "tensor(13165.6855, grad_fn=<AddBackward0>)\n",
            "tensor(12971.0352, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [130/469], Reconst Loss: 75.8126, KL Div: 25.5236\n",
            "tensor(12926.8486, grad_fn=<AddBackward0>)\n",
            "tensor(13243.9287, grad_fn=<AddBackward0>)\n",
            "tensor(13118.0898, grad_fn=<AddBackward0>)\n",
            "tensor(13147.3750, grad_fn=<AddBackward0>)\n",
            "tensor(13364.2178, grad_fn=<AddBackward0>)\n",
            "tensor(13147.4512, grad_fn=<AddBackward0>)\n",
            "tensor(13161.2646, grad_fn=<AddBackward0>)\n",
            "tensor(13210.3574, grad_fn=<AddBackward0>)\n",
            "tensor(13067.8496, grad_fn=<AddBackward0>)\n",
            "tensor(13324.8379, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [140/469], Reconst Loss: 78.2699, KL Div: 25.8304\n",
            "tensor(13403.6299, grad_fn=<AddBackward0>)\n",
            "tensor(13000.8740, grad_fn=<AddBackward0>)\n",
            "tensor(13613.5518, grad_fn=<AddBackward0>)\n",
            "tensor(13373.9033, grad_fn=<AddBackward0>)\n",
            "tensor(13715.3174, grad_fn=<AddBackward0>)\n",
            "tensor(13366.0273, grad_fn=<AddBackward0>)\n",
            "tensor(13074.4844, grad_fn=<AddBackward0>)\n",
            "tensor(12950.5195, grad_fn=<AddBackward0>)\n",
            "tensor(12872.5273, grad_fn=<AddBackward0>)\n",
            "tensor(13131.7441, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [150/469], Reconst Loss: 77.5453, KL Div: 25.0464\n",
            "tensor(13610.6367, grad_fn=<AddBackward0>)\n",
            "tensor(13421.5879, grad_fn=<AddBackward0>)\n",
            "tensor(13597.1855, grad_fn=<AddBackward0>)\n",
            "tensor(13171.4941, grad_fn=<AddBackward0>)\n",
            "tensor(13462.8711, grad_fn=<AddBackward0>)\n",
            "tensor(13087.8887, grad_fn=<AddBackward0>)\n",
            "tensor(12909.9668, grad_fn=<AddBackward0>)\n",
            "tensor(12820.4502, grad_fn=<AddBackward0>)\n",
            "tensor(13323.9961, grad_fn=<AddBackward0>)\n",
            "tensor(13437.6211, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [160/469], Reconst Loss: 79.2049, KL Div: 25.7765\n",
            "tensor(12777.3701, grad_fn=<AddBackward0>)\n",
            "tensor(13142.8418, grad_fn=<AddBackward0>)\n",
            "tensor(13172.4395, grad_fn=<AddBackward0>)\n",
            "tensor(12886.6533, grad_fn=<AddBackward0>)\n",
            "tensor(13544.9746, grad_fn=<AddBackward0>)\n",
            "tensor(12988.1660, grad_fn=<AddBackward0>)\n",
            "tensor(13323.9238, grad_fn=<AddBackward0>)\n",
            "tensor(13536.7490, grad_fn=<AddBackward0>)\n",
            "tensor(13179.3584, grad_fn=<AddBackward0>)\n",
            "tensor(13850.2988, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [170/469], Reconst Loss: 82.3537, KL Div: 25.8517\n",
            "tensor(13177.0645, grad_fn=<AddBackward0>)\n",
            "tensor(13204.2783, grad_fn=<AddBackward0>)\n",
            "tensor(13005.7129, grad_fn=<AddBackward0>)\n",
            "tensor(13179.7129, grad_fn=<AddBackward0>)\n",
            "tensor(13069.7344, grad_fn=<AddBackward0>)\n",
            "tensor(12860.6768, grad_fn=<AddBackward0>)\n",
            "tensor(13318.1484, grad_fn=<AddBackward0>)\n",
            "tensor(13099.5098, grad_fn=<AddBackward0>)\n",
            "tensor(13492.8887, grad_fn=<AddBackward0>)\n",
            "tensor(13311.4883, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [180/469], Reconst Loss: 78.4643, KL Div: 25.5317\n",
            "tensor(13221.6152, grad_fn=<AddBackward0>)\n",
            "tensor(13034.1719, grad_fn=<AddBackward0>)\n",
            "tensor(13153.2764, grad_fn=<AddBackward0>)\n",
            "tensor(13141.6240, grad_fn=<AddBackward0>)\n",
            "tensor(13104.8320, grad_fn=<AddBackward0>)\n",
            "tensor(13959.7979, grad_fn=<AddBackward0>)\n",
            "tensor(13224.1689, grad_fn=<AddBackward0>)\n",
            "tensor(13599.5908, grad_fn=<AddBackward0>)\n",
            "tensor(13036.6191, grad_fn=<AddBackward0>)\n",
            "tensor(13371.6689, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [190/469], Reconst Loss: 78.7514, KL Div: 25.7147\n",
            "tensor(13339.5156, grad_fn=<AddBackward0>)\n",
            "tensor(13103.3623, grad_fn=<AddBackward0>)\n",
            "tensor(13416.7471, grad_fn=<AddBackward0>)\n",
            "tensor(13357.2148, grad_fn=<AddBackward0>)\n",
            "tensor(13274.1455, grad_fn=<AddBackward0>)\n",
            "tensor(12979.7607, grad_fn=<AddBackward0>)\n",
            "tensor(13616.1602, grad_fn=<AddBackward0>)\n",
            "tensor(13201.0039, grad_fn=<AddBackward0>)\n",
            "tensor(13044.8936, grad_fn=<AddBackward0>)\n",
            "tensor(12723.3164, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [200/469], Reconst Loss: 74.5955, KL Div: 24.8054\n",
            "tensor(13264.3584, grad_fn=<AddBackward0>)\n",
            "tensor(13254.2139, grad_fn=<AddBackward0>)\n",
            "tensor(13377.7559, grad_fn=<AddBackward0>)\n",
            "tensor(13056.0859, grad_fn=<AddBackward0>)\n",
            "tensor(13131.6445, grad_fn=<AddBackward0>)\n",
            "tensor(12999.5752, grad_fn=<AddBackward0>)\n",
            "tensor(12697.8486, grad_fn=<AddBackward0>)\n",
            "tensor(12822.8350, grad_fn=<AddBackward0>)\n",
            "tensor(13003.9512, grad_fn=<AddBackward0>)\n",
            "tensor(13291.6826, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [210/469], Reconst Loss: 78.1760, KL Div: 25.6653\n",
            "tensor(12880.8164, grad_fn=<AddBackward0>)\n",
            "tensor(12994.9453, grad_fn=<AddBackward0>)\n",
            "tensor(13071.8867, grad_fn=<AddBackward0>)\n",
            "tensor(12570.0059, grad_fn=<AddBackward0>)\n",
            "tensor(13013.5742, grad_fn=<AddBackward0>)\n",
            "tensor(13223.5742, grad_fn=<AddBackward0>)\n",
            "tensor(13003.0029, grad_fn=<AddBackward0>)\n",
            "tensor(13174.9746, grad_fn=<AddBackward0>)\n",
            "tensor(13160.2969, grad_fn=<AddBackward0>)\n",
            "tensor(13405.3555, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [220/469], Reconst Loss: 78.7970, KL Div: 25.9323\n",
            "tensor(12667.8105, grad_fn=<AddBackward0>)\n",
            "tensor(12976.9199, grad_fn=<AddBackward0>)\n",
            "tensor(12766.2090, grad_fn=<AddBackward0>)\n",
            "tensor(12634.1426, grad_fn=<AddBackward0>)\n",
            "tensor(13108.7295, grad_fn=<AddBackward0>)\n",
            "tensor(13233.1670, grad_fn=<AddBackward0>)\n",
            "tensor(13140.0371, grad_fn=<AddBackward0>)\n",
            "tensor(13156.5547, grad_fn=<AddBackward0>)\n",
            "tensor(12680.6299, grad_fn=<AddBackward0>)\n",
            "tensor(12259.0859, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [230/469], Reconst Loss: 71.7510, KL Div: 24.0231\n",
            "tensor(13890.8936, grad_fn=<AddBackward0>)\n",
            "tensor(13229.9180, grad_fn=<AddBackward0>)\n",
            "tensor(13337.7725, grad_fn=<AddBackward0>)\n",
            "tensor(13354.2832, grad_fn=<AddBackward0>)\n",
            "tensor(13641.2598, grad_fn=<AddBackward0>)\n",
            "tensor(12947.2715, grad_fn=<AddBackward0>)\n",
            "tensor(12889.3770, grad_fn=<AddBackward0>)\n",
            "tensor(13222.8467, grad_fn=<AddBackward0>)\n",
            "tensor(13247.4111, grad_fn=<AddBackward0>)\n",
            "tensor(13487.7207, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [240/469], Reconst Loss: 79.0788, KL Div: 26.2941\n",
            "tensor(13070.1045, grad_fn=<AddBackward0>)\n",
            "tensor(13103.8789, grad_fn=<AddBackward0>)\n",
            "tensor(12685.6162, grad_fn=<AddBackward0>)\n",
            "tensor(12925.4893, grad_fn=<AddBackward0>)\n",
            "tensor(12495.1953, grad_fn=<AddBackward0>)\n",
            "tensor(13084.5801, grad_fn=<AddBackward0>)\n",
            "tensor(13369.7109, grad_fn=<AddBackward0>)\n",
            "tensor(13174.4941, grad_fn=<AddBackward0>)\n",
            "tensor(13240.6670, grad_fn=<AddBackward0>)\n",
            "tensor(13632.2559, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [250/469], Reconst Loss: 80.9507, KL Div: 25.5513\n",
            "tensor(13194.7100, grad_fn=<AddBackward0>)\n",
            "tensor(13109.7188, grad_fn=<AddBackward0>)\n",
            "tensor(13203.9521, grad_fn=<AddBackward0>)\n",
            "tensor(13746.9805, grad_fn=<AddBackward0>)\n",
            "tensor(13404.3877, grad_fn=<AddBackward0>)\n",
            "tensor(13051.6572, grad_fn=<AddBackward0>)\n",
            "tensor(13372.1230, grad_fn=<AddBackward0>)\n",
            "tensor(13373.6660, grad_fn=<AddBackward0>)\n",
            "tensor(13102.4150, grad_fn=<AddBackward0>)\n",
            "tensor(13522.1289, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [260/469], Reconst Loss: 80.3095, KL Div: 25.3321\n",
            "tensor(13795.2891, grad_fn=<AddBackward0>)\n",
            "tensor(13300.2256, grad_fn=<AddBackward0>)\n",
            "tensor(13479.8047, grad_fn=<AddBackward0>)\n",
            "tensor(13576.5303, grad_fn=<AddBackward0>)\n",
            "tensor(13051.7939, grad_fn=<AddBackward0>)\n",
            "tensor(13117.1680, grad_fn=<AddBackward0>)\n",
            "tensor(13196.0459, grad_fn=<AddBackward0>)\n",
            "tensor(13585.7900, grad_fn=<AddBackward0>)\n",
            "tensor(13492.8936, grad_fn=<AddBackward0>)\n",
            "tensor(12950.6191, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [270/469], Reconst Loss: 75.8426, KL Div: 25.3341\n",
            "tensor(13065.5684, grad_fn=<AddBackward0>)\n",
            "tensor(13284.5596, grad_fn=<AddBackward0>)\n",
            "tensor(13188.5566, grad_fn=<AddBackward0>)\n",
            "tensor(13051.1719, grad_fn=<AddBackward0>)\n",
            "tensor(13617.0400, grad_fn=<AddBackward0>)\n",
            "tensor(13610.6123, grad_fn=<AddBackward0>)\n",
            "tensor(12982.6152, grad_fn=<AddBackward0>)\n",
            "tensor(13377.8203, grad_fn=<AddBackward0>)\n",
            "tensor(13289.8340, grad_fn=<AddBackward0>)\n",
            "tensor(12707.3213, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [280/469], Reconst Loss: 74.2492, KL Div: 25.0268\n",
            "tensor(12869.5127, grad_fn=<AddBackward0>)\n",
            "tensor(13282.2031, grad_fn=<AddBackward0>)\n",
            "tensor(13547.4932, grad_fn=<AddBackward0>)\n",
            "tensor(13149.0186, grad_fn=<AddBackward0>)\n",
            "tensor(13479.2344, grad_fn=<AddBackward0>)\n",
            "tensor(13430.4414, grad_fn=<AddBackward0>)\n",
            "tensor(13035.6260, grad_fn=<AddBackward0>)\n",
            "tensor(13158.3574, grad_fn=<AddBackward0>)\n",
            "tensor(13067.2383, grad_fn=<AddBackward0>)\n",
            "tensor(12835.6572, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [290/469], Reconst Loss: 75.2892, KL Div: 24.9894\n",
            "tensor(13351.1113, grad_fn=<AddBackward0>)\n",
            "tensor(13583.9092, grad_fn=<AddBackward0>)\n",
            "tensor(13337.5293, grad_fn=<AddBackward0>)\n",
            "tensor(13338.0820, grad_fn=<AddBackward0>)\n",
            "tensor(13528.5889, grad_fn=<AddBackward0>)\n",
            "tensor(13532.4180, grad_fn=<AddBackward0>)\n",
            "tensor(13051.6230, grad_fn=<AddBackward0>)\n",
            "tensor(13556.9521, grad_fn=<AddBackward0>)\n",
            "tensor(13220.0586, grad_fn=<AddBackward0>)\n",
            "tensor(13138.7402, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [300/469], Reconst Loss: 77.2623, KL Div: 25.3841\n",
            "tensor(13438.3750, grad_fn=<AddBackward0>)\n",
            "tensor(13512.1348, grad_fn=<AddBackward0>)\n",
            "tensor(13318.7441, grad_fn=<AddBackward0>)\n",
            "tensor(12599.2227, grad_fn=<AddBackward0>)\n",
            "tensor(13105.7910, grad_fn=<AddBackward0>)\n",
            "tensor(13658.6221, grad_fn=<AddBackward0>)\n",
            "tensor(13034.7012, grad_fn=<AddBackward0>)\n",
            "tensor(13120.5029, grad_fn=<AddBackward0>)\n",
            "tensor(12941.1729, grad_fn=<AddBackward0>)\n",
            "tensor(12991.2441, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [310/469], Reconst Loss: 76.3987, KL Div: 25.0954\n",
            "tensor(13042.3994, grad_fn=<AddBackward0>)\n",
            "tensor(13487.3701, grad_fn=<AddBackward0>)\n",
            "tensor(13374.8701, grad_fn=<AddBackward0>)\n",
            "tensor(13359.1299, grad_fn=<AddBackward0>)\n",
            "tensor(13522.2129, grad_fn=<AddBackward0>)\n",
            "tensor(13161.3008, grad_fn=<AddBackward0>)\n",
            "tensor(13406.6836, grad_fn=<AddBackward0>)\n",
            "tensor(13722.7363, grad_fn=<AddBackward0>)\n",
            "tensor(13129.1016, grad_fn=<AddBackward0>)\n",
            "tensor(13148.0293, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [320/469], Reconst Loss: 77.2271, KL Div: 25.4919\n",
            "tensor(13122.0889, grad_fn=<AddBackward0>)\n",
            "tensor(13181.9600, grad_fn=<AddBackward0>)\n",
            "tensor(13544.1758, grad_fn=<AddBackward0>)\n",
            "tensor(13123.8838, grad_fn=<AddBackward0>)\n",
            "tensor(13217.2227, grad_fn=<AddBackward0>)\n",
            "tensor(13325.7969, grad_fn=<AddBackward0>)\n",
            "tensor(13187.0703, grad_fn=<AddBackward0>)\n",
            "tensor(13230.9375, grad_fn=<AddBackward0>)\n",
            "tensor(13341.9023, grad_fn=<AddBackward0>)\n",
            "tensor(13281.7744, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [330/469], Reconst Loss: 78.3240, KL Div: 25.4398\n",
            "tensor(13294.2510, grad_fn=<AddBackward0>)\n",
            "tensor(13270.0020, grad_fn=<AddBackward0>)\n",
            "tensor(13307.1865, grad_fn=<AddBackward0>)\n",
            "tensor(13896.4277, grad_fn=<AddBackward0>)\n",
            "tensor(13205.0254, grad_fn=<AddBackward0>)\n",
            "tensor(12987.4775, grad_fn=<AddBackward0>)\n",
            "tensor(13011.7393, grad_fn=<AddBackward0>)\n",
            "tensor(13675.2070, grad_fn=<AddBackward0>)\n",
            "tensor(13375.0234, grad_fn=<AddBackward0>)\n",
            "tensor(12967.8740, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [340/469], Reconst Loss: 76.3637, KL Div: 24.9479\n",
            "tensor(13243.4453, grad_fn=<AddBackward0>)\n",
            "tensor(12747.7256, grad_fn=<AddBackward0>)\n",
            "tensor(13152.4775, grad_fn=<AddBackward0>)\n",
            "tensor(13007.5732, grad_fn=<AddBackward0>)\n",
            "tensor(13710.4404, grad_fn=<AddBackward0>)\n",
            "tensor(13185.8496, grad_fn=<AddBackward0>)\n",
            "tensor(12891.8711, grad_fn=<AddBackward0>)\n",
            "tensor(13540.8066, grad_fn=<AddBackward0>)\n",
            "tensor(13330.1162, grad_fn=<AddBackward0>)\n",
            "tensor(12918.6553, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [350/469], Reconst Loss: 75.9676, KL Div: 24.9594\n",
            "tensor(13174.8369, grad_fn=<AddBackward0>)\n",
            "tensor(13217.9297, grad_fn=<AddBackward0>)\n",
            "tensor(13084.1348, grad_fn=<AddBackward0>)\n",
            "tensor(13632.7891, grad_fn=<AddBackward0>)\n",
            "tensor(13764.4590, grad_fn=<AddBackward0>)\n",
            "tensor(13192.5830, grad_fn=<AddBackward0>)\n",
            "tensor(13511.4209, grad_fn=<AddBackward0>)\n",
            "tensor(13820.9297, grad_fn=<AddBackward0>)\n",
            "tensor(12613.8555, grad_fn=<AddBackward0>)\n",
            "tensor(12703.4062, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [360/469], Reconst Loss: 74.2311, KL Div: 25.0143\n",
            "tensor(13207.7188, grad_fn=<AddBackward0>)\n",
            "tensor(13166.7471, grad_fn=<AddBackward0>)\n",
            "tensor(12962.7383, grad_fn=<AddBackward0>)\n",
            "tensor(13594.0605, grad_fn=<AddBackward0>)\n",
            "tensor(13213.4932, grad_fn=<AddBackward0>)\n",
            "tensor(13440.6650, grad_fn=<AddBackward0>)\n",
            "tensor(13324.4385, grad_fn=<AddBackward0>)\n",
            "tensor(13639.1152, grad_fn=<AddBackward0>)\n",
            "tensor(13853.0557, grad_fn=<AddBackward0>)\n",
            "tensor(13891.8311, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [370/469], Reconst Loss: 82.0099, KL Div: 26.5200\n",
            "tensor(13020.3213, grad_fn=<AddBackward0>)\n",
            "tensor(13381.3145, grad_fn=<AddBackward0>)\n",
            "tensor(12908.5166, grad_fn=<AddBackward0>)\n",
            "tensor(13272.4746, grad_fn=<AddBackward0>)\n",
            "tensor(13134.7285, grad_fn=<AddBackward0>)\n",
            "tensor(13021.8535, grad_fn=<AddBackward0>)\n",
            "tensor(13043.0488, grad_fn=<AddBackward0>)\n",
            "tensor(13332.9902, grad_fn=<AddBackward0>)\n",
            "tensor(12784.8418, grad_fn=<AddBackward0>)\n",
            "tensor(13483.6777, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [380/469], Reconst Loss: 79.4233, KL Div: 25.9179\n",
            "tensor(12960.6377, grad_fn=<AddBackward0>)\n",
            "tensor(13457.2227, grad_fn=<AddBackward0>)\n",
            "tensor(13369.2676, grad_fn=<AddBackward0>)\n",
            "tensor(13844.3398, grad_fn=<AddBackward0>)\n",
            "tensor(13334.0176, grad_fn=<AddBackward0>)\n",
            "tensor(13112.9160, grad_fn=<AddBackward0>)\n",
            "tensor(13764.1650, grad_fn=<AddBackward0>)\n",
            "tensor(13203.4219, grad_fn=<AddBackward0>)\n",
            "tensor(12703.4355, grad_fn=<AddBackward0>)\n",
            "tensor(13105.4707, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [390/469], Reconst Loss: 77.2453, KL Div: 25.1412\n",
            "tensor(13225.6396, grad_fn=<AddBackward0>)\n",
            "tensor(12921.4707, grad_fn=<AddBackward0>)\n",
            "tensor(13544.1699, grad_fn=<AddBackward0>)\n",
            "tensor(13802.3223, grad_fn=<AddBackward0>)\n",
            "tensor(13746.9111, grad_fn=<AddBackward0>)\n",
            "tensor(13357.1689, grad_fn=<AddBackward0>)\n",
            "tensor(13607.6338, grad_fn=<AddBackward0>)\n",
            "tensor(13460.3027, grad_fn=<AddBackward0>)\n",
            "tensor(13232.3379, grad_fn=<AddBackward0>)\n",
            "tensor(13195.3652, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [400/469], Reconst Loss: 77.6245, KL Div: 25.4643\n",
            "tensor(13250.4404, grad_fn=<AddBackward0>)\n",
            "tensor(12935.2930, grad_fn=<AddBackward0>)\n",
            "tensor(12879.0801, grad_fn=<AddBackward0>)\n",
            "tensor(13288.7422, grad_fn=<AddBackward0>)\n",
            "tensor(13190.2334, grad_fn=<AddBackward0>)\n",
            "tensor(13272.0049, grad_fn=<AddBackward0>)\n",
            "tensor(13122.8857, grad_fn=<AddBackward0>)\n",
            "tensor(13104.1973, grad_fn=<AddBackward0>)\n",
            "tensor(13523.8896, grad_fn=<AddBackward0>)\n",
            "tensor(13209.6289, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [410/469], Reconst Loss: 77.2612, KL Div: 25.9390\n",
            "tensor(13059.8389, grad_fn=<AddBackward0>)\n",
            "tensor(13489.7354, grad_fn=<AddBackward0>)\n",
            "tensor(13232.3691, grad_fn=<AddBackward0>)\n",
            "tensor(13450.5488, grad_fn=<AddBackward0>)\n",
            "tensor(13946.4863, grad_fn=<AddBackward0>)\n",
            "tensor(13725.9785, grad_fn=<AddBackward0>)\n",
            "tensor(13379.3018, grad_fn=<AddBackward0>)\n",
            "tensor(13516.6787, grad_fn=<AddBackward0>)\n",
            "tensor(12969.7012, grad_fn=<AddBackward0>)\n",
            "tensor(13572.7324, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [420/469], Reconst Loss: 79.8296, KL Div: 26.2073\n",
            "tensor(13238.6680, grad_fn=<AddBackward0>)\n",
            "tensor(13417.2900, grad_fn=<AddBackward0>)\n",
            "tensor(13104.9766, grad_fn=<AddBackward0>)\n",
            "tensor(13228.6230, grad_fn=<AddBackward0>)\n",
            "tensor(12945.9951, grad_fn=<AddBackward0>)\n",
            "tensor(14150.2285, grad_fn=<AddBackward0>)\n",
            "tensor(13230.1055, grad_fn=<AddBackward0>)\n",
            "tensor(13500.8242, grad_fn=<AddBackward0>)\n",
            "tensor(12901.6172, grad_fn=<AddBackward0>)\n",
            "tensor(13099.6084, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [430/469], Reconst Loss: 77.3531, KL Div: 24.9876\n",
            "tensor(13092.6338, grad_fn=<AddBackward0>)\n",
            "tensor(12990.2461, grad_fn=<AddBackward0>)\n",
            "tensor(13312.9961, grad_fn=<AddBackward0>)\n",
            "tensor(13350.3379, grad_fn=<AddBackward0>)\n",
            "tensor(13308.4307, grad_fn=<AddBackward0>)\n",
            "tensor(13284.6035, grad_fn=<AddBackward0>)\n",
            "tensor(13447.1836, grad_fn=<AddBackward0>)\n",
            "tensor(12833.9980, grad_fn=<AddBackward0>)\n",
            "tensor(13192.9678, grad_fn=<AddBackward0>)\n",
            "tensor(13350.6855, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [440/469], Reconst Loss: 78.6545, KL Div: 25.6477\n",
            "tensor(13294.8984, grad_fn=<AddBackward0>)\n",
            "tensor(13009.8672, grad_fn=<AddBackward0>)\n",
            "tensor(13338.7188, grad_fn=<AddBackward0>)\n",
            "tensor(12517.2637, grad_fn=<AddBackward0>)\n",
            "tensor(12721.7939, grad_fn=<AddBackward0>)\n",
            "tensor(13507.2168, grad_fn=<AddBackward0>)\n",
            "tensor(13117.8242, grad_fn=<AddBackward0>)\n",
            "tensor(12827.1055, grad_fn=<AddBackward0>)\n",
            "tensor(13447.0547, grad_fn=<AddBackward0>)\n",
            "tensor(12991.9365, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [450/469], Reconst Loss: 76.5146, KL Div: 24.9849\n",
            "tensor(13627.7285, grad_fn=<AddBackward0>)\n",
            "tensor(13348.1230, grad_fn=<AddBackward0>)\n",
            "tensor(13428.8418, grad_fn=<AddBackward0>)\n",
            "tensor(13385.1631, grad_fn=<AddBackward0>)\n",
            "tensor(13230.3027, grad_fn=<AddBackward0>)\n",
            "tensor(12711.6768, grad_fn=<AddBackward0>)\n",
            "tensor(13204.4951, grad_fn=<AddBackward0>)\n",
            "tensor(12816.8955, grad_fn=<AddBackward0>)\n",
            "tensor(13239.8066, grad_fn=<AddBackward0>)\n",
            "tensor(13189.5479, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [460/469], Reconst Loss: 77.7091, KL Div: 25.3342\n",
            "tensor(13445.5488, grad_fn=<AddBackward0>)\n",
            "tensor(13079.3223, grad_fn=<AddBackward0>)\n",
            "tensor(13518.3379, grad_fn=<AddBackward0>)\n",
            "tensor(13552.4639, grad_fn=<AddBackward0>)\n",
            "tensor(13240.5400, grad_fn=<AddBackward0>)\n",
            "tensor(13071.5498, grad_fn=<AddBackward0>)\n",
            "tensor(13330.2910, grad_fn=<AddBackward0>)\n",
            "tensor(12988.9844, grad_fn=<AddBackward0>)\n",
            "tensor(10473.6689, grad_fn=<AddBackward0>)\n",
            "tensor(12990.7900, grad_fn=<AddBackward0>)\n",
            "tensor(13094.8516, grad_fn=<AddBackward0>)\n",
            "tensor(13644.2812, grad_fn=<AddBackward0>)\n",
            "tensor(12611.5527, grad_fn=<AddBackward0>)\n",
            "tensor(13660.4385, grad_fn=<AddBackward0>)\n",
            "tensor(13070.5791, grad_fn=<AddBackward0>)\n",
            "tensor(12903.8047, grad_fn=<AddBackward0>)\n",
            "tensor(13300.9062, grad_fn=<AddBackward0>)\n",
            "tensor(12862.3623, grad_fn=<AddBackward0>)\n",
            "tensor(13627.8105, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 80.8278, KL Div: 25.6395\n",
            "tensor(13203.6328, grad_fn=<AddBackward0>)\n",
            "tensor(13026.4902, grad_fn=<AddBackward0>)\n",
            "tensor(13293.8877, grad_fn=<AddBackward0>)\n",
            "tensor(13878.1484, grad_fn=<AddBackward0>)\n",
            "tensor(13011.9160, grad_fn=<AddBackward0>)\n",
            "tensor(13121.2480, grad_fn=<AddBackward0>)\n",
            "tensor(13004.3184, grad_fn=<AddBackward0>)\n",
            "tensor(13099.6377, grad_fn=<AddBackward0>)\n",
            "tensor(12791.1514, grad_fn=<AddBackward0>)\n",
            "tensor(12927.9648, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [20/469], Reconst Loss: 75.7909, KL Div: 25.2088\n",
            "tensor(13430.9482, grad_fn=<AddBackward0>)\n",
            "tensor(13237.2324, grad_fn=<AddBackward0>)\n",
            "tensor(13127.1885, grad_fn=<AddBackward0>)\n",
            "tensor(13823.4316, grad_fn=<AddBackward0>)\n",
            "tensor(13319.9834, grad_fn=<AddBackward0>)\n",
            "tensor(12941.7393, grad_fn=<AddBackward0>)\n",
            "tensor(12878.8711, grad_fn=<AddBackward0>)\n",
            "tensor(13122.3418, grad_fn=<AddBackward0>)\n",
            "tensor(13105.2246, grad_fn=<AddBackward0>)\n",
            "tensor(13589.0195, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [30/469], Reconst Loss: 79.9771, KL Div: 26.1871\n",
            "tensor(12993.7363, grad_fn=<AddBackward0>)\n",
            "tensor(12771.7891, grad_fn=<AddBackward0>)\n",
            "tensor(13411., grad_fn=<AddBackward0>)\n",
            "tensor(13024.4180, grad_fn=<AddBackward0>)\n",
            "tensor(13177.7129, grad_fn=<AddBackward0>)\n",
            "tensor(13182.6973, grad_fn=<AddBackward0>)\n",
            "tensor(13363.7598, grad_fn=<AddBackward0>)\n",
            "tensor(13488.4170, grad_fn=<AddBackward0>)\n",
            "tensor(12306.7246, grad_fn=<AddBackward0>)\n",
            "tensor(13423.7832, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [40/469], Reconst Loss: 79.4224, KL Div: 25.4509\n",
            "tensor(13304.8223, grad_fn=<AddBackward0>)\n",
            "tensor(13250.5186, grad_fn=<AddBackward0>)\n",
            "tensor(12967.5664, grad_fn=<AddBackward0>)\n",
            "tensor(12929.4570, grad_fn=<AddBackward0>)\n",
            "tensor(13200.6699, grad_fn=<AddBackward0>)\n",
            "tensor(13326.1953, grad_fn=<AddBackward0>)\n",
            "tensor(13371.7158, grad_fn=<AddBackward0>)\n",
            "tensor(12964.3760, grad_fn=<AddBackward0>)\n",
            "tensor(13101.5488, grad_fn=<AddBackward0>)\n",
            "tensor(13128.5820, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [50/469], Reconst Loss: 77.3707, KL Div: 25.1964\n",
            "tensor(12903.9971, grad_fn=<AddBackward0>)\n",
            "tensor(13397.4160, grad_fn=<AddBackward0>)\n",
            "tensor(13128.8555, grad_fn=<AddBackward0>)\n",
            "tensor(13180.6670, grad_fn=<AddBackward0>)\n",
            "tensor(12821.3730, grad_fn=<AddBackward0>)\n",
            "tensor(13123.6895, grad_fn=<AddBackward0>)\n",
            "tensor(13351.3564, grad_fn=<AddBackward0>)\n",
            "tensor(13067.9678, grad_fn=<AddBackward0>)\n",
            "tensor(13561.0537, grad_fn=<AddBackward0>)\n",
            "tensor(13033.4893, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [60/469], Reconst Loss: 76.7155, KL Div: 25.1087\n",
            "tensor(13248.8613, grad_fn=<AddBackward0>)\n",
            "tensor(13420.9209, grad_fn=<AddBackward0>)\n",
            "tensor(12691.6758, grad_fn=<AddBackward0>)\n",
            "tensor(13496.3652, grad_fn=<AddBackward0>)\n",
            "tensor(13177.2363, grad_fn=<AddBackward0>)\n",
            "tensor(12900.2715, grad_fn=<AddBackward0>)\n",
            "tensor(13112.1875, grad_fn=<AddBackward0>)\n",
            "tensor(13062.3828, grad_fn=<AddBackward0>)\n",
            "tensor(13172.1436, grad_fn=<AddBackward0>)\n",
            "tensor(13134.3770, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [70/469], Reconst Loss: 77.8327, KL Div: 24.7796\n",
            "tensor(12649.3789, grad_fn=<AddBackward0>)\n",
            "tensor(13100.3467, grad_fn=<AddBackward0>)\n",
            "tensor(12904.0078, grad_fn=<AddBackward0>)\n",
            "tensor(13425.6162, grad_fn=<AddBackward0>)\n",
            "tensor(13086.4951, grad_fn=<AddBackward0>)\n",
            "tensor(12991.4033, grad_fn=<AddBackward0>)\n",
            "tensor(13318.8887, grad_fn=<AddBackward0>)\n",
            "tensor(13701.8057, grad_fn=<AddBackward0>)\n",
            "tensor(12783.0752, grad_fn=<AddBackward0>)\n",
            "tensor(13227.9326, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [80/469], Reconst Loss: 77.7785, KL Div: 25.5647\n",
            "tensor(12806.3799, grad_fn=<AddBackward0>)\n",
            "tensor(13468.9551, grad_fn=<AddBackward0>)\n",
            "tensor(13295.5371, grad_fn=<AddBackward0>)\n",
            "tensor(13598.3047, grad_fn=<AddBackward0>)\n",
            "tensor(13509.5332, grad_fn=<AddBackward0>)\n",
            "tensor(13299.4727, grad_fn=<AddBackward0>)\n",
            "tensor(13508.8945, grad_fn=<AddBackward0>)\n",
            "tensor(13193.3271, grad_fn=<AddBackward0>)\n",
            "tensor(12422.9404, grad_fn=<AddBackward0>)\n",
            "tensor(13503.1465, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [90/469], Reconst Loss: 79.7193, KL Div: 25.7740\n",
            "tensor(12998.4590, grad_fn=<AddBackward0>)\n",
            "tensor(13049.5654, grad_fn=<AddBackward0>)\n",
            "tensor(12954.6572, grad_fn=<AddBackward0>)\n",
            "tensor(13860.5703, grad_fn=<AddBackward0>)\n",
            "tensor(13320.2148, grad_fn=<AddBackward0>)\n",
            "tensor(13063.5312, grad_fn=<AddBackward0>)\n",
            "tensor(12485.4688, grad_fn=<AddBackward0>)\n",
            "tensor(13043.9062, grad_fn=<AddBackward0>)\n",
            "tensor(13107.2012, grad_fn=<AddBackward0>)\n",
            "tensor(12802.9082, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [100/469], Reconst Loss: 74.8316, KL Div: 25.1911\n",
            "tensor(13169.0596, grad_fn=<AddBackward0>)\n",
            "tensor(13002.5967, grad_fn=<AddBackward0>)\n",
            "tensor(13361.8086, grad_fn=<AddBackward0>)\n",
            "tensor(12962.9961, grad_fn=<AddBackward0>)\n",
            "tensor(13307.1260, grad_fn=<AddBackward0>)\n",
            "tensor(13646.0996, grad_fn=<AddBackward0>)\n",
            "tensor(13323.5781, grad_fn=<AddBackward0>)\n",
            "tensor(13263.9639, grad_fn=<AddBackward0>)\n",
            "tensor(13185.9521, grad_fn=<AddBackward0>)\n",
            "tensor(13700.0391, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [110/469], Reconst Loss: 81.4039, KL Div: 25.6277\n",
            "tensor(13214.6055, grad_fn=<AddBackward0>)\n",
            "tensor(13106.9932, grad_fn=<AddBackward0>)\n",
            "tensor(13477.4121, grad_fn=<AddBackward0>)\n",
            "tensor(13084.9756, grad_fn=<AddBackward0>)\n",
            "tensor(12984.8438, grad_fn=<AddBackward0>)\n",
            "tensor(12956.7041, grad_fn=<AddBackward0>)\n",
            "tensor(13224.7314, grad_fn=<AddBackward0>)\n",
            "tensor(13603.2275, grad_fn=<AddBackward0>)\n",
            "tensor(13136.4570, grad_fn=<AddBackward0>)\n",
            "tensor(13057.6738, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [120/469], Reconst Loss: 76.0654, KL Div: 25.9477\n",
            "tensor(13278.9600, grad_fn=<AddBackward0>)\n",
            "tensor(12900.2842, grad_fn=<AddBackward0>)\n",
            "tensor(12963.3613, grad_fn=<AddBackward0>)\n",
            "tensor(13301.4365, grad_fn=<AddBackward0>)\n",
            "tensor(12664.8809, grad_fn=<AddBackward0>)\n",
            "tensor(13196.1543, grad_fn=<AddBackward0>)\n",
            "tensor(13116.9854, grad_fn=<AddBackward0>)\n",
            "tensor(13262.0723, grad_fn=<AddBackward0>)\n",
            "tensor(13376.3086, grad_fn=<AddBackward0>)\n",
            "tensor(13302.7041, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [130/469], Reconst Loss: 77.9687, KL Div: 25.9587\n",
            "tensor(13022.7002, grad_fn=<AddBackward0>)\n",
            "tensor(13416.3262, grad_fn=<AddBackward0>)\n",
            "tensor(13322.1133, grad_fn=<AddBackward0>)\n",
            "tensor(12907.2305, grad_fn=<AddBackward0>)\n",
            "tensor(13084.5801, grad_fn=<AddBackward0>)\n",
            "tensor(13261.8809, grad_fn=<AddBackward0>)\n",
            "tensor(13235.0479, grad_fn=<AddBackward0>)\n",
            "tensor(13181.4980, grad_fn=<AddBackward0>)\n",
            "tensor(12888.0039, grad_fn=<AddBackward0>)\n",
            "tensor(12656.5381, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [140/469], Reconst Loss: 73.8889, KL Div: 24.9903\n",
            "tensor(13235.8701, grad_fn=<AddBackward0>)\n",
            "tensor(12829.8730, grad_fn=<AddBackward0>)\n",
            "tensor(13730.8271, grad_fn=<AddBackward0>)\n",
            "tensor(13267.2617, grad_fn=<AddBackward0>)\n",
            "tensor(12928.5137, grad_fn=<AddBackward0>)\n",
            "tensor(12785.0859, grad_fn=<AddBackward0>)\n",
            "tensor(12561.4072, grad_fn=<AddBackward0>)\n",
            "tensor(13161.0127, grad_fn=<AddBackward0>)\n",
            "tensor(13556.8076, grad_fn=<AddBackward0>)\n",
            "tensor(12769.9551, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [150/469], Reconst Loss: 74.8199, KL Div: 24.9454\n",
            "tensor(13474.6094, grad_fn=<AddBackward0>)\n",
            "tensor(13430.7568, grad_fn=<AddBackward0>)\n",
            "tensor(13351.7764, grad_fn=<AddBackward0>)\n",
            "tensor(13387.4619, grad_fn=<AddBackward0>)\n",
            "tensor(13288.2041, grad_fn=<AddBackward0>)\n",
            "tensor(13220.9648, grad_fn=<AddBackward0>)\n",
            "tensor(13252.0547, grad_fn=<AddBackward0>)\n",
            "tensor(13096.5195, grad_fn=<AddBackward0>)\n",
            "tensor(13182.5352, grad_fn=<AddBackward0>)\n",
            "tensor(12831.8154, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [160/469], Reconst Loss: 74.5601, KL Div: 25.6885\n",
            "tensor(13397.6250, grad_fn=<AddBackward0>)\n",
            "tensor(12978.0566, grad_fn=<AddBackward0>)\n",
            "tensor(13139.1973, grad_fn=<AddBackward0>)\n",
            "tensor(13352.0762, grad_fn=<AddBackward0>)\n",
            "tensor(13218.6406, grad_fn=<AddBackward0>)\n",
            "tensor(13168.4014, grad_fn=<AddBackward0>)\n",
            "tensor(13532.2607, grad_fn=<AddBackward0>)\n",
            "tensor(13006.3555, grad_fn=<AddBackward0>)\n",
            "tensor(13679.6934, grad_fn=<AddBackward0>)\n",
            "tensor(13217.0059, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [170/469], Reconst Loss: 78.2668, KL Div: 24.9911\n",
            "tensor(13340.3525, grad_fn=<AddBackward0>)\n",
            "tensor(13205.9922, grad_fn=<AddBackward0>)\n",
            "tensor(13171.0039, grad_fn=<AddBackward0>)\n",
            "tensor(13510.9141, grad_fn=<AddBackward0>)\n",
            "tensor(13086.5107, grad_fn=<AddBackward0>)\n",
            "tensor(13096.4316, grad_fn=<AddBackward0>)\n",
            "tensor(12886.4199, grad_fn=<AddBackward0>)\n",
            "tensor(13374.1748, grad_fn=<AddBackward0>)\n",
            "tensor(13624.1729, grad_fn=<AddBackward0>)\n",
            "tensor(13581.7695, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [180/469], Reconst Loss: 79.5073, KL Div: 26.6003\n",
            "tensor(13322.4004, grad_fn=<AddBackward0>)\n",
            "tensor(13450.4287, grad_fn=<AddBackward0>)\n",
            "tensor(13230.2305, grad_fn=<AddBackward0>)\n",
            "tensor(12929.3848, grad_fn=<AddBackward0>)\n",
            "tensor(13394.8203, grad_fn=<AddBackward0>)\n",
            "tensor(13024.0820, grad_fn=<AddBackward0>)\n",
            "tensor(13198.8320, grad_fn=<AddBackward0>)\n",
            "tensor(13702.8096, grad_fn=<AddBackward0>)\n",
            "tensor(12752.8486, grad_fn=<AddBackward0>)\n",
            "tensor(13391.6211, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [190/469], Reconst Loss: 78.9941, KL Div: 25.6280\n",
            "tensor(13300.4863, grad_fn=<AddBackward0>)\n",
            "tensor(12933.9121, grad_fn=<AddBackward0>)\n",
            "tensor(13077.9131, grad_fn=<AddBackward0>)\n",
            "tensor(13354.3984, grad_fn=<AddBackward0>)\n",
            "tensor(13477.2422, grad_fn=<AddBackward0>)\n",
            "tensor(13342.3574, grad_fn=<AddBackward0>)\n",
            "tensor(12982.4922, grad_fn=<AddBackward0>)\n",
            "tensor(13358.3301, grad_fn=<AddBackward0>)\n",
            "tensor(13128.7461, grad_fn=<AddBackward0>)\n",
            "tensor(13345.0469, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [200/469], Reconst Loss: 78.9930, KL Div: 25.2652\n",
            "tensor(12744.0801, grad_fn=<AddBackward0>)\n",
            "tensor(13046.3145, grad_fn=<AddBackward0>)\n",
            "tensor(12887.7246, grad_fn=<AddBackward0>)\n",
            "tensor(13202.7656, grad_fn=<AddBackward0>)\n",
            "tensor(13006.3252, grad_fn=<AddBackward0>)\n",
            "tensor(13220.1465, grad_fn=<AddBackward0>)\n",
            "tensor(13365.7090, grad_fn=<AddBackward0>)\n",
            "tensor(13795.8379, grad_fn=<AddBackward0>)\n",
            "tensor(13222.0479, grad_fn=<AddBackward0>)\n",
            "tensor(12275.5117, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [210/469], Reconst Loss: 71.6763, KL Div: 24.2261\n",
            "tensor(12982.9414, grad_fn=<AddBackward0>)\n",
            "tensor(13180.6973, grad_fn=<AddBackward0>)\n",
            "tensor(13351.5410, grad_fn=<AddBackward0>)\n",
            "tensor(12940.8594, grad_fn=<AddBackward0>)\n",
            "tensor(13180.5303, grad_fn=<AddBackward0>)\n",
            "tensor(13187.0342, grad_fn=<AddBackward0>)\n",
            "tensor(12773.1777, grad_fn=<AddBackward0>)\n",
            "tensor(13399.5371, grad_fn=<AddBackward0>)\n",
            "tensor(13239.6885, grad_fn=<AddBackward0>)\n",
            "tensor(13851.7344, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [220/469], Reconst Loss: 81.7989, KL Div: 26.4177\n",
            "tensor(13131.3203, grad_fn=<AddBackward0>)\n",
            "tensor(13352.1260, grad_fn=<AddBackward0>)\n",
            "tensor(13898.5059, grad_fn=<AddBackward0>)\n",
            "tensor(13155.5762, grad_fn=<AddBackward0>)\n",
            "tensor(13111.0898, grad_fn=<AddBackward0>)\n",
            "tensor(13264.1494, grad_fn=<AddBackward0>)\n",
            "tensor(13244.1875, grad_fn=<AddBackward0>)\n",
            "tensor(13385.7129, grad_fn=<AddBackward0>)\n",
            "tensor(13231.0898, grad_fn=<AddBackward0>)\n",
            "tensor(13013.7383, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [230/469], Reconst Loss: 76.1107, KL Div: 25.5592\n",
            "tensor(12921.3066, grad_fn=<AddBackward0>)\n",
            "tensor(13340.8389, grad_fn=<AddBackward0>)\n",
            "tensor(13380.4229, grad_fn=<AddBackward0>)\n",
            "tensor(12684.1172, grad_fn=<AddBackward0>)\n",
            "tensor(13399.8984, grad_fn=<AddBackward0>)\n",
            "tensor(13182.9443, grad_fn=<AddBackward0>)\n",
            "tensor(13138.2881, grad_fn=<AddBackward0>)\n",
            "tensor(13009.5000, grad_fn=<AddBackward0>)\n",
            "tensor(13226.7354, grad_fn=<AddBackward0>)\n",
            "tensor(13276.4971, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [240/469], Reconst Loss: 78.3369, KL Div: 25.3857\n",
            "tensor(12776.3750, grad_fn=<AddBackward0>)\n",
            "tensor(13356.8887, grad_fn=<AddBackward0>)\n",
            "tensor(13366.4248, grad_fn=<AddBackward0>)\n",
            "tensor(13357.4258, grad_fn=<AddBackward0>)\n",
            "tensor(13659.2773, grad_fn=<AddBackward0>)\n",
            "tensor(13457.0332, grad_fn=<AddBackward0>)\n",
            "tensor(13626.7881, grad_fn=<AddBackward0>)\n",
            "tensor(12326.2754, grad_fn=<AddBackward0>)\n",
            "tensor(13000.8145, grad_fn=<AddBackward0>)\n",
            "tensor(13076.2910, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [250/469], Reconst Loss: 76.5317, KL Div: 25.6268\n",
            "tensor(12937.0430, grad_fn=<AddBackward0>)\n",
            "tensor(12880.1504, grad_fn=<AddBackward0>)\n",
            "tensor(13450.7998, grad_fn=<AddBackward0>)\n",
            "tensor(12933.7510, grad_fn=<AddBackward0>)\n",
            "tensor(13630.9365, grad_fn=<AddBackward0>)\n",
            "tensor(13382.6699, grad_fn=<AddBackward0>)\n",
            "tensor(13114.0049, grad_fn=<AddBackward0>)\n",
            "tensor(13088.8848, grad_fn=<AddBackward0>)\n",
            "tensor(13272.0615, grad_fn=<AddBackward0>)\n",
            "tensor(13418.9688, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [260/469], Reconst Loss: 79.5874, KL Div: 25.2483\n",
            "tensor(13395.2520, grad_fn=<AddBackward0>)\n",
            "tensor(12976.6416, grad_fn=<AddBackward0>)\n",
            "tensor(13976.5371, grad_fn=<AddBackward0>)\n",
            "tensor(13103.2041, grad_fn=<AddBackward0>)\n",
            "tensor(12786.1895, grad_fn=<AddBackward0>)\n",
            "tensor(13759.9707, grad_fn=<AddBackward0>)\n",
            "tensor(13120.5967, grad_fn=<AddBackward0>)\n",
            "tensor(12555.4570, grad_fn=<AddBackward0>)\n",
            "tensor(13202.2197, grad_fn=<AddBackward0>)\n",
            "tensor(12752.9434, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [270/469], Reconst Loss: 74.2567, KL Div: 25.3757\n",
            "tensor(13248.8887, grad_fn=<AddBackward0>)\n",
            "tensor(13608.1465, grad_fn=<AddBackward0>)\n",
            "tensor(13239.8164, grad_fn=<AddBackward0>)\n",
            "tensor(13420.5840, grad_fn=<AddBackward0>)\n",
            "tensor(13030.3135, grad_fn=<AddBackward0>)\n",
            "tensor(13067.4814, grad_fn=<AddBackward0>)\n",
            "tensor(13279.9512, grad_fn=<AddBackward0>)\n",
            "tensor(13197.2266, grad_fn=<AddBackward0>)\n",
            "tensor(13405.8242, grad_fn=<AddBackward0>)\n",
            "tensor(13462.0547, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [280/469], Reconst Loss: 79.5549, KL Div: 25.6174\n",
            "tensor(12736.5020, grad_fn=<AddBackward0>)\n",
            "tensor(13506.9395, grad_fn=<AddBackward0>)\n",
            "tensor(13294.0039, grad_fn=<AddBackward0>)\n",
            "tensor(13156.4219, grad_fn=<AddBackward0>)\n",
            "tensor(13414.6348, grad_fn=<AddBackward0>)\n",
            "tensor(13663.1582, grad_fn=<AddBackward0>)\n",
            "tensor(13698.1328, grad_fn=<AddBackward0>)\n",
            "tensor(13335.0557, grad_fn=<AddBackward0>)\n",
            "tensor(13594.4336, grad_fn=<AddBackward0>)\n",
            "tensor(13579.4932, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [290/469], Reconst Loss: 80.5184, KL Div: 25.5714\n",
            "tensor(13468.3076, grad_fn=<AddBackward0>)\n",
            "tensor(13413.7578, grad_fn=<AddBackward0>)\n",
            "tensor(13092.5449, grad_fn=<AddBackward0>)\n",
            "tensor(13288.4336, grad_fn=<AddBackward0>)\n",
            "tensor(12894.8721, grad_fn=<AddBackward0>)\n",
            "tensor(13175.0859, grad_fn=<AddBackward0>)\n",
            "tensor(13528.7549, grad_fn=<AddBackward0>)\n",
            "tensor(12997.6885, grad_fn=<AddBackward0>)\n",
            "tensor(13301.6045, grad_fn=<AddBackward0>)\n",
            "tensor(12729.0459, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [300/469], Reconst Loss: 74.9861, KL Div: 24.4595\n",
            "tensor(13221.6074, grad_fn=<AddBackward0>)\n",
            "tensor(13170.6221, grad_fn=<AddBackward0>)\n",
            "tensor(13305.3164, grad_fn=<AddBackward0>)\n",
            "tensor(13696.2949, grad_fn=<AddBackward0>)\n",
            "tensor(12861.6953, grad_fn=<AddBackward0>)\n",
            "tensor(13449.8760, grad_fn=<AddBackward0>)\n",
            "tensor(13172.6426, grad_fn=<AddBackward0>)\n",
            "tensor(13077.2490, grad_fn=<AddBackward0>)\n",
            "tensor(13557.3047, grad_fn=<AddBackward0>)\n",
            "tensor(13290.5996, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [310/469], Reconst Loss: 77.9388, KL Div: 25.8941\n",
            "tensor(13646.0205, grad_fn=<AddBackward0>)\n",
            "tensor(13283.9922, grad_fn=<AddBackward0>)\n",
            "tensor(13467.0430, grad_fn=<AddBackward0>)\n",
            "tensor(13385.8672, grad_fn=<AddBackward0>)\n",
            "tensor(13226.5479, grad_fn=<AddBackward0>)\n",
            "tensor(13195.0781, grad_fn=<AddBackward0>)\n",
            "tensor(13314.1777, grad_fn=<AddBackward0>)\n",
            "tensor(13239.6807, grad_fn=<AddBackward0>)\n",
            "tensor(12830.9258, grad_fn=<AddBackward0>)\n",
            "tensor(13755.9590, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [320/469], Reconst Loss: 81.5307, KL Div: 25.9377\n",
            "tensor(13363.2734, grad_fn=<AddBackward0>)\n",
            "tensor(13072.8711, grad_fn=<AddBackward0>)\n",
            "tensor(13224.0332, grad_fn=<AddBackward0>)\n",
            "tensor(13415.5352, grad_fn=<AddBackward0>)\n",
            "tensor(13326.4443, grad_fn=<AddBackward0>)\n",
            "tensor(13530.2627, grad_fn=<AddBackward0>)\n",
            "tensor(12725.4814, grad_fn=<AddBackward0>)\n",
            "tensor(13398.2002, grad_fn=<AddBackward0>)\n",
            "tensor(13260.7578, grad_fn=<AddBackward0>)\n",
            "tensor(13183.6797, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [330/469], Reconst Loss: 78.1145, KL Div: 24.8830\n",
            "tensor(13241.9697, grad_fn=<AddBackward0>)\n",
            "tensor(13505.8203, grad_fn=<AddBackward0>)\n",
            "tensor(13330.1289, grad_fn=<AddBackward0>)\n",
            "tensor(13409.2520, grad_fn=<AddBackward0>)\n",
            "tensor(13173.9365, grad_fn=<AddBackward0>)\n",
            "tensor(13103.7617, grad_fn=<AddBackward0>)\n",
            "tensor(12835.0449, grad_fn=<AddBackward0>)\n",
            "tensor(12935.1797, grad_fn=<AddBackward0>)\n",
            "tensor(13002.4756, grad_fn=<AddBackward0>)\n",
            "tensor(13361.8730, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [340/469], Reconst Loss: 78.9260, KL Div: 25.4636\n",
            "tensor(12802.7676, grad_fn=<AddBackward0>)\n",
            "tensor(13078.5225, grad_fn=<AddBackward0>)\n",
            "tensor(13183.3320, grad_fn=<AddBackward0>)\n",
            "tensor(13186.9512, grad_fn=<AddBackward0>)\n",
            "tensor(12730.3340, grad_fn=<AddBackward0>)\n",
            "tensor(13632.8486, grad_fn=<AddBackward0>)\n",
            "tensor(13522.5244, grad_fn=<AddBackward0>)\n",
            "tensor(13018.4297, grad_fn=<AddBackward0>)\n",
            "tensor(13392.1426, grad_fn=<AddBackward0>)\n",
            "tensor(13621.4053, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [350/469], Reconst Loss: 80.7119, KL Div: 25.7053\n",
            "tensor(13002.0479, grad_fn=<AddBackward0>)\n",
            "tensor(13209.6416, grad_fn=<AddBackward0>)\n",
            "tensor(13088.7383, grad_fn=<AddBackward0>)\n",
            "tensor(13146.3301, grad_fn=<AddBackward0>)\n",
            "tensor(12988.8086, grad_fn=<AddBackward0>)\n",
            "tensor(13246.5742, grad_fn=<AddBackward0>)\n",
            "tensor(12924.4453, grad_fn=<AddBackward0>)\n",
            "tensor(13827.6992, grad_fn=<AddBackward0>)\n",
            "tensor(13304.6582, grad_fn=<AddBackward0>)\n",
            "tensor(13594.7734, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [360/469], Reconst Loss: 80.5831, KL Div: 25.6260\n",
            "tensor(13669.5400, grad_fn=<AddBackward0>)\n",
            "tensor(13097.9766, grad_fn=<AddBackward0>)\n",
            "tensor(12871.8428, grad_fn=<AddBackward0>)\n",
            "tensor(13476.4844, grad_fn=<AddBackward0>)\n",
            "tensor(12880.6934, grad_fn=<AddBackward0>)\n",
            "tensor(13309.4336, grad_fn=<AddBackward0>)\n",
            "tensor(13299.3203, grad_fn=<AddBackward0>)\n",
            "tensor(12980.5322, grad_fn=<AddBackward0>)\n",
            "tensor(13159.6543, grad_fn=<AddBackward0>)\n",
            "tensor(13130.8906, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [370/469], Reconst Loss: 76.0809, KL Div: 26.5042\n",
            "tensor(12958.5889, grad_fn=<AddBackward0>)\n",
            "tensor(13256.8340, grad_fn=<AddBackward0>)\n",
            "tensor(13308.1191, grad_fn=<AddBackward0>)\n",
            "tensor(13001.6406, grad_fn=<AddBackward0>)\n",
            "tensor(13142.3633, grad_fn=<AddBackward0>)\n",
            "tensor(13318.4375, grad_fn=<AddBackward0>)\n",
            "tensor(13656.1699, grad_fn=<AddBackward0>)\n",
            "tensor(12919.4229, grad_fn=<AddBackward0>)\n",
            "tensor(13243.9678, grad_fn=<AddBackward0>)\n",
            "tensor(13080.4004, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [380/469], Reconst Loss: 77.3522, KL Div: 24.8384\n",
            "tensor(12956.4219, grad_fn=<AddBackward0>)\n",
            "tensor(13384.0283, grad_fn=<AddBackward0>)\n",
            "tensor(13355.6777, grad_fn=<AddBackward0>)\n",
            "tensor(13333.7832, grad_fn=<AddBackward0>)\n",
            "tensor(13486.0137, grad_fn=<AddBackward0>)\n",
            "tensor(13473.1152, grad_fn=<AddBackward0>)\n",
            "tensor(13005.4570, grad_fn=<AddBackward0>)\n",
            "tensor(12938.1699, grad_fn=<AddBackward0>)\n",
            "tensor(13270.0576, grad_fn=<AddBackward0>)\n",
            "tensor(13657.4521, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [390/469], Reconst Loss: 80.9006, KL Div: 25.7982\n",
            "tensor(13050.1299, grad_fn=<AddBackward0>)\n",
            "tensor(13184.9473, grad_fn=<AddBackward0>)\n",
            "tensor(13172.2412, grad_fn=<AddBackward0>)\n",
            "tensor(13342.1221, grad_fn=<AddBackward0>)\n",
            "tensor(13126.1943, grad_fn=<AddBackward0>)\n",
            "tensor(13543.3252, grad_fn=<AddBackward0>)\n",
            "tensor(13298.7139, grad_fn=<AddBackward0>)\n",
            "tensor(13069.9443, grad_fn=<AddBackward0>)\n",
            "tensor(13518.5078, grad_fn=<AddBackward0>)\n",
            "tensor(13192.2227, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [400/469], Reconst Loss: 77.7085, KL Div: 25.3558\n",
            "tensor(13060.4219, grad_fn=<AddBackward0>)\n",
            "tensor(13206.4990, grad_fn=<AddBackward0>)\n",
            "tensor(12629.8633, grad_fn=<AddBackward0>)\n",
            "tensor(13345.8223, grad_fn=<AddBackward0>)\n",
            "tensor(13061.1689, grad_fn=<AddBackward0>)\n",
            "tensor(12543.2559, grad_fn=<AddBackward0>)\n",
            "tensor(13418.9961, grad_fn=<AddBackward0>)\n",
            "tensor(13199.0078, grad_fn=<AddBackward0>)\n",
            "tensor(13043.4541, grad_fn=<AddBackward0>)\n",
            "tensor(13594.9971, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [410/469], Reconst Loss: 80.4593, KL Div: 25.7516\n",
            "tensor(13313.0957, grad_fn=<AddBackward0>)\n",
            "tensor(13324.2256, grad_fn=<AddBackward0>)\n",
            "tensor(13000.1660, grad_fn=<AddBackward0>)\n",
            "tensor(12976.4160, grad_fn=<AddBackward0>)\n",
            "tensor(12622.5225, grad_fn=<AddBackward0>)\n",
            "tensor(13145.9346, grad_fn=<AddBackward0>)\n",
            "tensor(12924.0615, grad_fn=<AddBackward0>)\n",
            "tensor(13396.0449, grad_fn=<AddBackward0>)\n",
            "tensor(13040.1289, grad_fn=<AddBackward0>)\n",
            "tensor(13401.9971, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [420/469], Reconst Loss: 78.6682, KL Div: 26.0349\n",
            "tensor(13188.0098, grad_fn=<AddBackward0>)\n",
            "tensor(13506.7822, grad_fn=<AddBackward0>)\n",
            "tensor(13279.6602, grad_fn=<AddBackward0>)\n",
            "tensor(13395.1953, grad_fn=<AddBackward0>)\n",
            "tensor(12685.7471, grad_fn=<AddBackward0>)\n",
            "tensor(13001.8789, grad_fn=<AddBackward0>)\n",
            "tensor(12676.7598, grad_fn=<AddBackward0>)\n",
            "tensor(12947.7793, grad_fn=<AddBackward0>)\n",
            "tensor(13634.9375, grad_fn=<AddBackward0>)\n",
            "tensor(13445.2373, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [430/469], Reconst Loss: 79.9121, KL Div: 25.1288\n",
            "tensor(13115.2539, grad_fn=<AddBackward0>)\n",
            "tensor(13304.0557, grad_fn=<AddBackward0>)\n",
            "tensor(12624.2334, grad_fn=<AddBackward0>)\n",
            "tensor(13865.4502, grad_fn=<AddBackward0>)\n",
            "tensor(13316.1680, grad_fn=<AddBackward0>)\n",
            "tensor(13040.9355, grad_fn=<AddBackward0>)\n",
            "tensor(13831.3496, grad_fn=<AddBackward0>)\n",
            "tensor(13770.2480, grad_fn=<AddBackward0>)\n",
            "tensor(13246.1074, grad_fn=<AddBackward0>)\n",
            "tensor(13287.4414, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [440/469], Reconst Loss: 78.8447, KL Div: 24.9635\n",
            "tensor(13311.9678, grad_fn=<AddBackward0>)\n",
            "tensor(13091.2832, grad_fn=<AddBackward0>)\n",
            "tensor(12652.2383, grad_fn=<AddBackward0>)\n",
            "tensor(12985.6357, grad_fn=<AddBackward0>)\n",
            "tensor(13238.6211, grad_fn=<AddBackward0>)\n",
            "tensor(13429.9199, grad_fn=<AddBackward0>)\n",
            "tensor(13610.7500, grad_fn=<AddBackward0>)\n",
            "tensor(13044.7793, grad_fn=<AddBackward0>)\n",
            "tensor(13109.6973, grad_fn=<AddBackward0>)\n",
            "tensor(13141.1816, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [450/469], Reconst Loss: 77.1616, KL Div: 25.5039\n",
            "tensor(13022.6006, grad_fn=<AddBackward0>)\n",
            "tensor(12980.3301, grad_fn=<AddBackward0>)\n",
            "tensor(13592.8867, grad_fn=<AddBackward0>)\n",
            "tensor(13304.6406, grad_fn=<AddBackward0>)\n",
            "tensor(13166.7324, grad_fn=<AddBackward0>)\n",
            "tensor(13204.3867, grad_fn=<AddBackward0>)\n",
            "tensor(13620.8340, grad_fn=<AddBackward0>)\n",
            "tensor(13258.9551, grad_fn=<AddBackward0>)\n",
            "tensor(13032.4229, grad_fn=<AddBackward0>)\n",
            "tensor(13439.4893, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [460/469], Reconst Loss: 79.6942, KL Div: 25.3018\n",
            "tensor(12989.5879, grad_fn=<AddBackward0>)\n",
            "tensor(13018.3242, grad_fn=<AddBackward0>)\n",
            "tensor(12876.6426, grad_fn=<AddBackward0>)\n",
            "tensor(13442.8135, grad_fn=<AddBackward0>)\n",
            "tensor(13377.4180, grad_fn=<AddBackward0>)\n",
            "tensor(12816.3086, grad_fn=<AddBackward0>)\n",
            "tensor(12957.4600, grad_fn=<AddBackward0>)\n",
            "tensor(12742.8867, grad_fn=<AddBackward0>)\n",
            "tensor(9972.2676, grad_fn=<AddBackward0>)\n",
            "tensor(12976.7988, grad_fn=<AddBackward0>)\n",
            "tensor(12645.1025, grad_fn=<AddBackward0>)\n",
            "tensor(13016.0771, grad_fn=<AddBackward0>)\n",
            "tensor(12864.3477, grad_fn=<AddBackward0>)\n",
            "tensor(12949.6201, grad_fn=<AddBackward0>)\n",
            "tensor(13754.8164, grad_fn=<AddBackward0>)\n",
            "tensor(13206.1699, grad_fn=<AddBackward0>)\n",
            "tensor(13036.2480, grad_fn=<AddBackward0>)\n",
            "tensor(13166.4629, grad_fn=<AddBackward0>)\n",
            "tensor(12746.0762, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 74.5059, KL Div: 25.0728\n",
            "tensor(13428.7148, grad_fn=<AddBackward0>)\n",
            "tensor(12961.2939, grad_fn=<AddBackward0>)\n",
            "tensor(13191.3125, grad_fn=<AddBackward0>)\n",
            "tensor(12891.9551, grad_fn=<AddBackward0>)\n",
            "tensor(12885.8027, grad_fn=<AddBackward0>)\n",
            "tensor(13119.8955, grad_fn=<AddBackward0>)\n",
            "tensor(13406.4805, grad_fn=<AddBackward0>)\n",
            "tensor(13556.9395, grad_fn=<AddBackward0>)\n",
            "tensor(12947.5342, grad_fn=<AddBackward0>)\n",
            "tensor(13256.1670, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [20/469], Reconst Loss: 77.4485, KL Div: 26.1153\n",
            "tensor(12986.0654, grad_fn=<AddBackward0>)\n",
            "tensor(12803.8457, grad_fn=<AddBackward0>)\n",
            "tensor(12673.5137, grad_fn=<AddBackward0>)\n",
            "tensor(13156.9248, grad_fn=<AddBackward0>)\n",
            "tensor(13057.3730, grad_fn=<AddBackward0>)\n",
            "tensor(12963.6699, grad_fn=<AddBackward0>)\n",
            "tensor(13226.5156, grad_fn=<AddBackward0>)\n",
            "tensor(13119.3086, grad_fn=<AddBackward0>)\n",
            "tensor(13491.5547, grad_fn=<AddBackward0>)\n",
            "tensor(13357.1709, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [30/469], Reconst Loss: 78.9371, KL Div: 25.4158\n",
            "tensor(12894.4238, grad_fn=<AddBackward0>)\n",
            "tensor(13903.1064, grad_fn=<AddBackward0>)\n",
            "tensor(13136.5596, grad_fn=<AddBackward0>)\n",
            "tensor(12944.6768, grad_fn=<AddBackward0>)\n",
            "tensor(13217.6826, grad_fn=<AddBackward0>)\n",
            "tensor(13430.7979, grad_fn=<AddBackward0>)\n",
            "tensor(13154.7598, grad_fn=<AddBackward0>)\n",
            "tensor(12879.7422, grad_fn=<AddBackward0>)\n",
            "tensor(13327.3711, grad_fn=<AddBackward0>)\n",
            "tensor(13237.0156, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [40/469], Reconst Loss: 77.6059, KL Div: 25.8083\n",
            "tensor(13020.5010, grad_fn=<AddBackward0>)\n",
            "tensor(12833.6260, grad_fn=<AddBackward0>)\n",
            "tensor(13226.2822, grad_fn=<AddBackward0>)\n",
            "tensor(13276.0703, grad_fn=<AddBackward0>)\n",
            "tensor(12929.6816, grad_fn=<AddBackward0>)\n",
            "tensor(13468.1846, grad_fn=<AddBackward0>)\n",
            "tensor(13619.7637, grad_fn=<AddBackward0>)\n",
            "tensor(13150.6396, grad_fn=<AddBackward0>)\n",
            "tensor(13459.8369, grad_fn=<AddBackward0>)\n",
            "tensor(12903.5811, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [50/469], Reconst Loss: 75.7189, KL Div: 25.0903\n",
            "tensor(12646.8496, grad_fn=<AddBackward0>)\n",
            "tensor(12941.6582, grad_fn=<AddBackward0>)\n",
            "tensor(12718.3301, grad_fn=<AddBackward0>)\n",
            "tensor(12580.3682, grad_fn=<AddBackward0>)\n",
            "tensor(13164.1094, grad_fn=<AddBackward0>)\n",
            "tensor(13215.0088, grad_fn=<AddBackward0>)\n",
            "tensor(13144.6543, grad_fn=<AddBackward0>)\n",
            "tensor(13463.8633, grad_fn=<AddBackward0>)\n",
            "tensor(12690.3311, grad_fn=<AddBackward0>)\n",
            "tensor(12773.1484, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [60/469], Reconst Loss: 74.8402, KL Div: 24.9500\n",
            "tensor(13206.9629, grad_fn=<AddBackward0>)\n",
            "tensor(13696.8584, grad_fn=<AddBackward0>)\n",
            "tensor(12827.9209, grad_fn=<AddBackward0>)\n",
            "tensor(13346.9375, grad_fn=<AddBackward0>)\n",
            "tensor(13084.4746, grad_fn=<AddBackward0>)\n",
            "tensor(13212.4541, grad_fn=<AddBackward0>)\n",
            "tensor(13035.3418, grad_fn=<AddBackward0>)\n",
            "tensor(13086.3359, grad_fn=<AddBackward0>)\n",
            "tensor(13606.2783, grad_fn=<AddBackward0>)\n",
            "tensor(13244.7422, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [70/469], Reconst Loss: 78.3275, KL Div: 25.1471\n",
            "tensor(13313.3672, grad_fn=<AddBackward0>)\n",
            "tensor(13406.7100, grad_fn=<AddBackward0>)\n",
            "tensor(13692.6914, grad_fn=<AddBackward0>)\n",
            "tensor(12747.0479, grad_fn=<AddBackward0>)\n",
            "tensor(13256.1562, grad_fn=<AddBackward0>)\n",
            "tensor(12875.7305, grad_fn=<AddBackward0>)\n",
            "tensor(13454.6455, grad_fn=<AddBackward0>)\n",
            "tensor(13218.5801, grad_fn=<AddBackward0>)\n",
            "tensor(13233.1318, grad_fn=<AddBackward0>)\n",
            "tensor(13072.6777, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [80/469], Reconst Loss: 76.8132, KL Div: 25.3171\n",
            "tensor(12755.3008, grad_fn=<AddBackward0>)\n",
            "tensor(13043.4102, grad_fn=<AddBackward0>)\n",
            "tensor(13332.3408, grad_fn=<AddBackward0>)\n",
            "tensor(13808.4072, grad_fn=<AddBackward0>)\n",
            "tensor(13191.9551, grad_fn=<AddBackward0>)\n",
            "tensor(13433.3066, grad_fn=<AddBackward0>)\n",
            "tensor(13300.0576, grad_fn=<AddBackward0>)\n",
            "tensor(13062.8213, grad_fn=<AddBackward0>)\n",
            "tensor(12898.6113, grad_fn=<AddBackward0>)\n",
            "tensor(13390.2305, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [90/469], Reconst Loss: 79.4186, KL Div: 25.1925\n",
            "tensor(13526.1465, grad_fn=<AddBackward0>)\n",
            "tensor(13602.3457, grad_fn=<AddBackward0>)\n",
            "tensor(12864.6562, grad_fn=<AddBackward0>)\n",
            "tensor(13365.5938, grad_fn=<AddBackward0>)\n",
            "tensor(13341.1875, grad_fn=<AddBackward0>)\n",
            "tensor(13471., grad_fn=<AddBackward0>)\n",
            "tensor(12859.4355, grad_fn=<AddBackward0>)\n",
            "tensor(13512.1523, grad_fn=<AddBackward0>)\n",
            "tensor(13017.3320, grad_fn=<AddBackward0>)\n",
            "tensor(13090.6045, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [100/469], Reconst Loss: 76.3083, KL Div: 25.9621\n",
            "tensor(13324.5625, grad_fn=<AddBackward0>)\n",
            "tensor(13183.3330, grad_fn=<AddBackward0>)\n",
            "tensor(13018.7207, grad_fn=<AddBackward0>)\n",
            "tensor(12913.9482, grad_fn=<AddBackward0>)\n",
            "tensor(13340.6367, grad_fn=<AddBackward0>)\n",
            "tensor(13529.6484, grad_fn=<AddBackward0>)\n",
            "tensor(13288.6729, grad_fn=<AddBackward0>)\n",
            "tensor(13933.9971, grad_fn=<AddBackward0>)\n",
            "tensor(13657.8887, grad_fn=<AddBackward0>)\n",
            "tensor(13348.2314, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [110/469], Reconst Loss: 78.7211, KL Div: 25.5619\n",
            "tensor(13624.8125, grad_fn=<AddBackward0>)\n",
            "tensor(13360.7510, grad_fn=<AddBackward0>)\n",
            "tensor(13518.2051, grad_fn=<AddBackward0>)\n",
            "tensor(13646.4756, grad_fn=<AddBackward0>)\n",
            "tensor(12795.8740, grad_fn=<AddBackward0>)\n",
            "tensor(13676.1709, grad_fn=<AddBackward0>)\n",
            "tensor(13320.3135, grad_fn=<AddBackward0>)\n",
            "tensor(12849.2002, grad_fn=<AddBackward0>)\n",
            "tensor(13275.7490, grad_fn=<AddBackward0>)\n",
            "tensor(13161.7109, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [120/469], Reconst Loss: 77.4045, KL Div: 25.4214\n",
            "tensor(12879.1055, grad_fn=<AddBackward0>)\n",
            "tensor(12873.1104, grad_fn=<AddBackward0>)\n",
            "tensor(13014.0996, grad_fn=<AddBackward0>)\n",
            "tensor(13337.3125, grad_fn=<AddBackward0>)\n",
            "tensor(12914.6787, grad_fn=<AddBackward0>)\n",
            "tensor(13045.1641, grad_fn=<AddBackward0>)\n",
            "tensor(13577.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13283.7939, grad_fn=<AddBackward0>)\n",
            "tensor(13218.3789, grad_fn=<AddBackward0>)\n",
            "tensor(13114.2480, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [130/469], Reconst Loss: 76.9040, KL Div: 25.5511\n",
            "tensor(13382.8086, grad_fn=<AddBackward0>)\n",
            "tensor(13501.0703, grad_fn=<AddBackward0>)\n",
            "tensor(13472.4092, grad_fn=<AddBackward0>)\n",
            "tensor(12925.4854, grad_fn=<AddBackward0>)\n",
            "tensor(13332.7275, grad_fn=<AddBackward0>)\n",
            "tensor(13061.2949, grad_fn=<AddBackward0>)\n",
            "tensor(13288.6777, grad_fn=<AddBackward0>)\n",
            "tensor(13746.8164, grad_fn=<AddBackward0>)\n",
            "tensor(12535.7383, grad_fn=<AddBackward0>)\n",
            "tensor(13392.0088, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [140/469], Reconst Loss: 78.8934, KL Div: 25.7317\n",
            "tensor(13284.1680, grad_fn=<AddBackward0>)\n",
            "tensor(13051.9873, grad_fn=<AddBackward0>)\n",
            "tensor(13306.7803, grad_fn=<AddBackward0>)\n",
            "tensor(13193.2822, grad_fn=<AddBackward0>)\n",
            "tensor(13617.3232, grad_fn=<AddBackward0>)\n",
            "tensor(13039.0049, grad_fn=<AddBackward0>)\n",
            "tensor(12834.1230, grad_fn=<AddBackward0>)\n",
            "tensor(13314.4668, grad_fn=<AddBackward0>)\n",
            "tensor(13528.4844, grad_fn=<AddBackward0>)\n",
            "tensor(13299.4570, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [150/469], Reconst Loss: 78.2736, KL Div: 25.6284\n",
            "tensor(13495.4141, grad_fn=<AddBackward0>)\n",
            "tensor(12851.7373, grad_fn=<AddBackward0>)\n",
            "tensor(13429.7285, grad_fn=<AddBackward0>)\n",
            "tensor(12902.7646, grad_fn=<AddBackward0>)\n",
            "tensor(13313.4023, grad_fn=<AddBackward0>)\n",
            "tensor(13078.8711, grad_fn=<AddBackward0>)\n",
            "tensor(13393.1484, grad_fn=<AddBackward0>)\n",
            "tensor(12870.2715, grad_fn=<AddBackward0>)\n",
            "tensor(12656.8291, grad_fn=<AddBackward0>)\n",
            "tensor(13661.8027, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [160/469], Reconst Loss: 80.6488, KL Div: 26.0841\n",
            "tensor(12798.2969, grad_fn=<AddBackward0>)\n",
            "tensor(13191.1377, grad_fn=<AddBackward0>)\n",
            "tensor(13658.9053, grad_fn=<AddBackward0>)\n",
            "tensor(13420.0732, grad_fn=<AddBackward0>)\n",
            "tensor(13377.0742, grad_fn=<AddBackward0>)\n",
            "tensor(13217.7500, grad_fn=<AddBackward0>)\n",
            "tensor(13233.8555, grad_fn=<AddBackward0>)\n",
            "tensor(13403.2812, grad_fn=<AddBackward0>)\n",
            "tensor(13344.3867, grad_fn=<AddBackward0>)\n",
            "tensor(13165.9766, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [170/469], Reconst Loss: 77.3705, KL Div: 25.4887\n",
            "tensor(13130.9131, grad_fn=<AddBackward0>)\n",
            "tensor(13318.8398, grad_fn=<AddBackward0>)\n",
            "tensor(13203.5107, grad_fn=<AddBackward0>)\n",
            "tensor(13559.5312, grad_fn=<AddBackward0>)\n",
            "tensor(13021.2432, grad_fn=<AddBackward0>)\n",
            "tensor(12869.7725, grad_fn=<AddBackward0>)\n",
            "tensor(12968.5957, grad_fn=<AddBackward0>)\n",
            "tensor(13275.7051, grad_fn=<AddBackward0>)\n",
            "tensor(12934.7988, grad_fn=<AddBackward0>)\n",
            "tensor(13368.5986, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [180/469], Reconst Loss: 79.0527, KL Div: 25.3895\n",
            "tensor(13811.1084, grad_fn=<AddBackward0>)\n",
            "tensor(13007.4951, grad_fn=<AddBackward0>)\n",
            "tensor(13630.8076, grad_fn=<AddBackward0>)\n",
            "tensor(12920.5918, grad_fn=<AddBackward0>)\n",
            "tensor(13477.9229, grad_fn=<AddBackward0>)\n",
            "tensor(13476.4375, grad_fn=<AddBackward0>)\n",
            "tensor(13032.2100, grad_fn=<AddBackward0>)\n",
            "tensor(13271.7783, grad_fn=<AddBackward0>)\n",
            "tensor(12825.6504, grad_fn=<AddBackward0>)\n",
            "tensor(12716.1289, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [190/469], Reconst Loss: 74.4977, KL Div: 24.8471\n",
            "tensor(13257.8643, grad_fn=<AddBackward0>)\n",
            "tensor(13364.5234, grad_fn=<AddBackward0>)\n",
            "tensor(12920.0967, grad_fn=<AddBackward0>)\n",
            "tensor(13234.8086, grad_fn=<AddBackward0>)\n",
            "tensor(13398.0068, grad_fn=<AddBackward0>)\n",
            "tensor(13120.5566, grad_fn=<AddBackward0>)\n",
            "tensor(12389.0781, grad_fn=<AddBackward0>)\n",
            "tensor(13175.9209, grad_fn=<AddBackward0>)\n",
            "tensor(12907.5723, grad_fn=<AddBackward0>)\n",
            "tensor(13389.1143, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [200/469], Reconst Loss: 78.4644, KL Div: 26.1381\n",
            "tensor(12894.3477, grad_fn=<AddBackward0>)\n",
            "tensor(13617.3398, grad_fn=<AddBackward0>)\n",
            "tensor(13266.2539, grad_fn=<AddBackward0>)\n",
            "tensor(13010.2881, grad_fn=<AddBackward0>)\n",
            "tensor(13417.2412, grad_fn=<AddBackward0>)\n",
            "tensor(13458.7363, grad_fn=<AddBackward0>)\n",
            "tensor(12829.5996, grad_fn=<AddBackward0>)\n",
            "tensor(13535.2227, grad_fn=<AddBackward0>)\n",
            "tensor(13350.7920, grad_fn=<AddBackward0>)\n",
            "tensor(12839.1758, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [210/469], Reconst Loss: 75.0491, KL Div: 25.2569\n",
            "tensor(13129.0781, grad_fn=<AddBackward0>)\n",
            "tensor(12848.6982, grad_fn=<AddBackward0>)\n",
            "tensor(13007.4170, grad_fn=<AddBackward0>)\n",
            "tensor(13468.8350, grad_fn=<AddBackward0>)\n",
            "tensor(12793.2275, grad_fn=<AddBackward0>)\n",
            "tensor(13484.1396, grad_fn=<AddBackward0>)\n",
            "tensor(13071.9414, grad_fn=<AddBackward0>)\n",
            "tensor(13184.0615, grad_fn=<AddBackward0>)\n",
            "tensor(13025.8145, grad_fn=<AddBackward0>)\n",
            "tensor(13329.9707, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [220/469], Reconst Loss: 78.7346, KL Div: 25.4057\n",
            "tensor(13260.1094, grad_fn=<AddBackward0>)\n",
            "tensor(13234.1787, grad_fn=<AddBackward0>)\n",
            "tensor(13068.2646, grad_fn=<AddBackward0>)\n",
            "tensor(13542.9824, grad_fn=<AddBackward0>)\n",
            "tensor(13591.2402, grad_fn=<AddBackward0>)\n",
            "tensor(13043.3105, grad_fn=<AddBackward0>)\n",
            "tensor(12868.6953, grad_fn=<AddBackward0>)\n",
            "tensor(13037.1982, grad_fn=<AddBackward0>)\n",
            "tensor(13648.1475, grad_fn=<AddBackward0>)\n",
            "tensor(12943.2324, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [230/469], Reconst Loss: 75.8240, KL Div: 25.2950\n",
            "tensor(13177.8877, grad_fn=<AddBackward0>)\n",
            "tensor(13369.2139, grad_fn=<AddBackward0>)\n",
            "tensor(13410.5742, grad_fn=<AddBackward0>)\n",
            "tensor(13446.6885, grad_fn=<AddBackward0>)\n",
            "tensor(12948.2314, grad_fn=<AddBackward0>)\n",
            "tensor(12974.1230, grad_fn=<AddBackward0>)\n",
            "tensor(13035.6191, grad_fn=<AddBackward0>)\n",
            "tensor(13701.6113, grad_fn=<AddBackward0>)\n",
            "tensor(13051.5557, grad_fn=<AddBackward0>)\n",
            "tensor(13024.2705, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [240/469], Reconst Loss: 76.9667, KL Div: 24.7854\n",
            "tensor(13017.0215, grad_fn=<AddBackward0>)\n",
            "tensor(13552.8242, grad_fn=<AddBackward0>)\n",
            "tensor(12672.9443, grad_fn=<AddBackward0>)\n",
            "tensor(12731.2207, grad_fn=<AddBackward0>)\n",
            "tensor(13079.5957, grad_fn=<AddBackward0>)\n",
            "tensor(13465.5488, grad_fn=<AddBackward0>)\n",
            "tensor(12949.5283, grad_fn=<AddBackward0>)\n",
            "tensor(12885.5879, grad_fn=<AddBackward0>)\n",
            "tensor(13076.2480, grad_fn=<AddBackward0>)\n",
            "tensor(13191.9346, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [250/469], Reconst Loss: 77.4726, KL Div: 25.5894\n",
            "tensor(13100.5762, grad_fn=<AddBackward0>)\n",
            "tensor(12727.4209, grad_fn=<AddBackward0>)\n",
            "tensor(13119.3379, grad_fn=<AddBackward0>)\n",
            "tensor(13571.0020, grad_fn=<AddBackward0>)\n",
            "tensor(13084.2559, grad_fn=<AddBackward0>)\n",
            "tensor(12837.2188, grad_fn=<AddBackward0>)\n",
            "tensor(12803.2656, grad_fn=<AddBackward0>)\n",
            "tensor(13111.3867, grad_fn=<AddBackward0>)\n",
            "tensor(13448.6172, grad_fn=<AddBackward0>)\n",
            "tensor(13364.7314, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [260/469], Reconst Loss: 79.5974, KL Div: 24.8146\n",
            "tensor(13036.0498, grad_fn=<AddBackward0>)\n",
            "tensor(13494.8760, grad_fn=<AddBackward0>)\n",
            "tensor(13350.0371, grad_fn=<AddBackward0>)\n",
            "tensor(12964.0342, grad_fn=<AddBackward0>)\n",
            "tensor(13090.4023, grad_fn=<AddBackward0>)\n",
            "tensor(13476.9551, grad_fn=<AddBackward0>)\n",
            "tensor(13615.0020, grad_fn=<AddBackward0>)\n",
            "tensor(12481.8555, grad_fn=<AddBackward0>)\n",
            "tensor(13560.8193, grad_fn=<AddBackward0>)\n",
            "tensor(13559.9814, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [270/469], Reconst Loss: 80.2740, KL Div: 25.6633\n",
            "tensor(13439.7764, grad_fn=<AddBackward0>)\n",
            "tensor(13645.2705, grad_fn=<AddBackward0>)\n",
            "tensor(13302.9248, grad_fn=<AddBackward0>)\n",
            "tensor(13864.5859, grad_fn=<AddBackward0>)\n",
            "tensor(13603.7246, grad_fn=<AddBackward0>)\n",
            "tensor(13249.1318, grad_fn=<AddBackward0>)\n",
            "tensor(13305.5938, grad_fn=<AddBackward0>)\n",
            "tensor(13183.0059, grad_fn=<AddBackward0>)\n",
            "tensor(13183.2988, grad_fn=<AddBackward0>)\n",
            "tensor(12836.3770, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [280/469], Reconst Loss: 75.1991, KL Div: 25.0850\n",
            "tensor(12745.0957, grad_fn=<AddBackward0>)\n",
            "tensor(13505.7793, grad_fn=<AddBackward0>)\n",
            "tensor(12921.3145, grad_fn=<AddBackward0>)\n",
            "tensor(13106.2334, grad_fn=<AddBackward0>)\n",
            "tensor(12851.5332, grad_fn=<AddBackward0>)\n",
            "tensor(13067.0391, grad_fn=<AddBackward0>)\n",
            "tensor(13356.6338, grad_fn=<AddBackward0>)\n",
            "tensor(12959.7832, grad_fn=<AddBackward0>)\n",
            "tensor(13559.8125, grad_fn=<AddBackward0>)\n",
            "tensor(13127.4873, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [290/469], Reconst Loss: 77.0096, KL Div: 25.5489\n",
            "tensor(13234.8418, grad_fn=<AddBackward0>)\n",
            "tensor(13459.1729, grad_fn=<AddBackward0>)\n",
            "tensor(13246.5293, grad_fn=<AddBackward0>)\n",
            "tensor(13665.1758, grad_fn=<AddBackward0>)\n",
            "tensor(13281.5703, grad_fn=<AddBackward0>)\n",
            "tensor(12665.3516, grad_fn=<AddBackward0>)\n",
            "tensor(12853.1533, grad_fn=<AddBackward0>)\n",
            "tensor(13080.6992, grad_fn=<AddBackward0>)\n",
            "tensor(13584.5342, grad_fn=<AddBackward0>)\n",
            "tensor(13366.6299, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [300/469], Reconst Loss: 78.8739, KL Div: 25.5528\n",
            "tensor(13241.3984, grad_fn=<AddBackward0>)\n",
            "tensor(13240.1465, grad_fn=<AddBackward0>)\n",
            "tensor(12863.7637, grad_fn=<AddBackward0>)\n",
            "tensor(13009.2217, grad_fn=<AddBackward0>)\n",
            "tensor(13313.3711, grad_fn=<AddBackward0>)\n",
            "tensor(13105.7119, grad_fn=<AddBackward0>)\n",
            "tensor(13210.7832, grad_fn=<AddBackward0>)\n",
            "tensor(13110.1064, grad_fn=<AddBackward0>)\n",
            "tensor(13685.0742, grad_fn=<AddBackward0>)\n",
            "tensor(13160.0918, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [310/469], Reconst Loss: 77.6431, KL Div: 25.1701\n",
            "tensor(13878.4746, grad_fn=<AddBackward0>)\n",
            "tensor(12662.7549, grad_fn=<AddBackward0>)\n",
            "tensor(12883.3691, grad_fn=<AddBackward0>)\n",
            "tensor(13071.4277, grad_fn=<AddBackward0>)\n",
            "tensor(13443.0215, grad_fn=<AddBackward0>)\n",
            "tensor(12913.2168, grad_fn=<AddBackward0>)\n",
            "tensor(13333.1836, grad_fn=<AddBackward0>)\n",
            "tensor(13295.2305, grad_fn=<AddBackward0>)\n",
            "tensor(13546.8008, grad_fn=<AddBackward0>)\n",
            "tensor(12796.7588, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [320/469], Reconst Loss: 74.8354, KL Div: 25.1392\n",
            "tensor(13552.4521, grad_fn=<AddBackward0>)\n",
            "tensor(13522.8262, grad_fn=<AddBackward0>)\n",
            "tensor(12908.5508, grad_fn=<AddBackward0>)\n",
            "tensor(13119.8037, grad_fn=<AddBackward0>)\n",
            "tensor(13314.5410, grad_fn=<AddBackward0>)\n",
            "tensor(13471.2246, grad_fn=<AddBackward0>)\n",
            "tensor(13011.5996, grad_fn=<AddBackward0>)\n",
            "tensor(13127.7373, grad_fn=<AddBackward0>)\n",
            "tensor(13205.3975, grad_fn=<AddBackward0>)\n",
            "tensor(13195.1289, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [330/469], Reconst Loss: 77.6398, KL Div: 25.4471\n",
            "tensor(13309.7295, grad_fn=<AddBackward0>)\n",
            "tensor(13410.5820, grad_fn=<AddBackward0>)\n",
            "tensor(12990.2051, grad_fn=<AddBackward0>)\n",
            "tensor(13509.2334, grad_fn=<AddBackward0>)\n",
            "tensor(13023.6211, grad_fn=<AddBackward0>)\n",
            "tensor(13480.9795, grad_fn=<AddBackward0>)\n",
            "tensor(13467.8467, grad_fn=<AddBackward0>)\n",
            "tensor(13352.8975, grad_fn=<AddBackward0>)\n",
            "tensor(12893.7695, grad_fn=<AddBackward0>)\n",
            "tensor(12980.8145, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [340/469], Reconst Loss: 76.3276, KL Div: 25.0850\n",
            "tensor(13361.6514, grad_fn=<AddBackward0>)\n",
            "tensor(13253.0439, grad_fn=<AddBackward0>)\n",
            "tensor(13582.6172, grad_fn=<AddBackward0>)\n",
            "tensor(13600.3379, grad_fn=<AddBackward0>)\n",
            "tensor(13097.3525, grad_fn=<AddBackward0>)\n",
            "tensor(13171.0908, grad_fn=<AddBackward0>)\n",
            "tensor(13163.3711, grad_fn=<AddBackward0>)\n",
            "tensor(12784.4873, grad_fn=<AddBackward0>)\n",
            "tensor(13287.4756, grad_fn=<AddBackward0>)\n",
            "tensor(13180.8799, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [350/469], Reconst Loss: 77.3763, KL Div: 25.5993\n",
            "tensor(12703.6973, grad_fn=<AddBackward0>)\n",
            "tensor(13370.4170, grad_fn=<AddBackward0>)\n",
            "tensor(13318.8750, grad_fn=<AddBackward0>)\n",
            "tensor(12732.3818, grad_fn=<AddBackward0>)\n",
            "tensor(13565.5654, grad_fn=<AddBackward0>)\n",
            "tensor(13443.6973, grad_fn=<AddBackward0>)\n",
            "tensor(12949.0518, grad_fn=<AddBackward0>)\n",
            "tensor(13225.1162, grad_fn=<AddBackward0>)\n",
            "tensor(13295.0605, grad_fn=<AddBackward0>)\n",
            "tensor(13271.1279, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [360/469], Reconst Loss: 78.0693, KL Div: 25.6114\n",
            "tensor(13067.4355, grad_fn=<AddBackward0>)\n",
            "tensor(12963.0674, grad_fn=<AddBackward0>)\n",
            "tensor(13490.7529, grad_fn=<AddBackward0>)\n",
            "tensor(12944.4297, grad_fn=<AddBackward0>)\n",
            "tensor(13222.6465, grad_fn=<AddBackward0>)\n",
            "tensor(13416.4277, grad_fn=<AddBackward0>)\n",
            "tensor(13116.8936, grad_fn=<AddBackward0>)\n",
            "tensor(13195.8496, grad_fn=<AddBackward0>)\n",
            "tensor(12905.7285, grad_fn=<AddBackward0>)\n",
            "tensor(13631.5547, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [370/469], Reconst Loss: 81.0742, KL Div: 25.4223\n",
            "tensor(13046.2861, grad_fn=<AddBackward0>)\n",
            "tensor(12675.5059, grad_fn=<AddBackward0>)\n",
            "tensor(13442.3818, grad_fn=<AddBackward0>)\n",
            "tensor(13519.7812, grad_fn=<AddBackward0>)\n",
            "tensor(13235.7422, grad_fn=<AddBackward0>)\n",
            "tensor(13050.0518, grad_fn=<AddBackward0>)\n",
            "tensor(13419.9268, grad_fn=<AddBackward0>)\n",
            "tensor(12941.1074, grad_fn=<AddBackward0>)\n",
            "tensor(12921.5674, grad_fn=<AddBackward0>)\n",
            "tensor(12798.6973, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [380/469], Reconst Loss: 74.9784, KL Div: 25.0114\n",
            "tensor(13601.0137, grad_fn=<AddBackward0>)\n",
            "tensor(13312.9512, grad_fn=<AddBackward0>)\n",
            "tensor(13408.8242, grad_fn=<AddBackward0>)\n",
            "tensor(13069.7236, grad_fn=<AddBackward0>)\n",
            "tensor(13000.6582, grad_fn=<AddBackward0>)\n",
            "tensor(13736.1992, grad_fn=<AddBackward0>)\n",
            "tensor(13018.9697, grad_fn=<AddBackward0>)\n",
            "tensor(13085.4727, grad_fn=<AddBackward0>)\n",
            "tensor(13450.8027, grad_fn=<AddBackward0>)\n",
            "tensor(13302.4141, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [390/469], Reconst Loss: 79.0557, KL Div: 24.8694\n",
            "tensor(13562.8496, grad_fn=<AddBackward0>)\n",
            "tensor(13446.8457, grad_fn=<AddBackward0>)\n",
            "tensor(13599.5352, grad_fn=<AddBackward0>)\n",
            "tensor(13285.4648, grad_fn=<AddBackward0>)\n",
            "tensor(12631.1299, grad_fn=<AddBackward0>)\n",
            "tensor(13130.0234, grad_fn=<AddBackward0>)\n",
            "tensor(13294.6250, grad_fn=<AddBackward0>)\n",
            "tensor(13372.8047, grad_fn=<AddBackward0>)\n",
            "tensor(12905.8555, grad_fn=<AddBackward0>)\n",
            "tensor(13249.0293, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [400/469], Reconst Loss: 78.7821, KL Div: 24.7260\n",
            "tensor(13410.2285, grad_fn=<AddBackward0>)\n",
            "tensor(12989.3076, grad_fn=<AddBackward0>)\n",
            "tensor(13415.7207, grad_fn=<AddBackward0>)\n",
            "tensor(13525.7441, grad_fn=<AddBackward0>)\n",
            "tensor(13410.8105, grad_fn=<AddBackward0>)\n",
            "tensor(13192.2461, grad_fn=<AddBackward0>)\n",
            "tensor(13209.3477, grad_fn=<AddBackward0>)\n",
            "tensor(13571.8203, grad_fn=<AddBackward0>)\n",
            "tensor(13142.9668, grad_fn=<AddBackward0>)\n",
            "tensor(13638.4561, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [410/469], Reconst Loss: 80.3223, KL Div: 26.2282\n",
            "tensor(12608.4580, grad_fn=<AddBackward0>)\n",
            "tensor(12585.5176, grad_fn=<AddBackward0>)\n",
            "tensor(13279.8145, grad_fn=<AddBackward0>)\n",
            "tensor(13248.1602, grad_fn=<AddBackward0>)\n",
            "tensor(13352.5830, grad_fn=<AddBackward0>)\n",
            "tensor(13600.9824, grad_fn=<AddBackward0>)\n",
            "tensor(13392.7500, grad_fn=<AddBackward0>)\n",
            "tensor(12815.0430, grad_fn=<AddBackward0>)\n",
            "tensor(13493.3359, grad_fn=<AddBackward0>)\n",
            "tensor(12868.3525, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [420/469], Reconst Loss: 75.5376, KL Div: 24.9964\n",
            "tensor(13709.3418, grad_fn=<AddBackward0>)\n",
            "tensor(13079.8740, grad_fn=<AddBackward0>)\n",
            "tensor(13311.0547, grad_fn=<AddBackward0>)\n",
            "tensor(13340.1973, grad_fn=<AddBackward0>)\n",
            "tensor(13038.0625, grad_fn=<AddBackward0>)\n",
            "tensor(12762.9268, grad_fn=<AddBackward0>)\n",
            "tensor(13651.3525, grad_fn=<AddBackward0>)\n",
            "tensor(13200.0986, grad_fn=<AddBackward0>)\n",
            "tensor(13238.0986, grad_fn=<AddBackward0>)\n",
            "tensor(12980.5977, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [430/469], Reconst Loss: 75.4258, KL Div: 25.9851\n",
            "tensor(13252.5293, grad_fn=<AddBackward0>)\n",
            "tensor(13189.6621, grad_fn=<AddBackward0>)\n",
            "tensor(13053.1328, grad_fn=<AddBackward0>)\n",
            "tensor(12838.4238, grad_fn=<AddBackward0>)\n",
            "tensor(12942.0264, grad_fn=<AddBackward0>)\n",
            "tensor(13025.4482, grad_fn=<AddBackward0>)\n",
            "tensor(12981.4551, grad_fn=<AddBackward0>)\n",
            "tensor(13193.3916, grad_fn=<AddBackward0>)\n",
            "tensor(13507.1816, grad_fn=<AddBackward0>)\n",
            "tensor(13348.9893, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [440/469], Reconst Loss: 79.0893, KL Div: 25.1996\n",
            "tensor(13619.9355, grad_fn=<AddBackward0>)\n",
            "tensor(13003.5693, grad_fn=<AddBackward0>)\n",
            "tensor(13309.7734, grad_fn=<AddBackward0>)\n",
            "tensor(13173.1797, grad_fn=<AddBackward0>)\n",
            "tensor(12849.5479, grad_fn=<AddBackward0>)\n",
            "tensor(13124.2246, grad_fn=<AddBackward0>)\n",
            "tensor(13372.2988, grad_fn=<AddBackward0>)\n",
            "tensor(13080.3154, grad_fn=<AddBackward0>)\n",
            "tensor(13545.0391, grad_fn=<AddBackward0>)\n",
            "tensor(13563.6914, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [450/469], Reconst Loss: 80.0283, KL Div: 25.9380\n",
            "tensor(13397.6807, grad_fn=<AddBackward0>)\n",
            "tensor(13548.4551, grad_fn=<AddBackward0>)\n",
            "tensor(12860.2305, grad_fn=<AddBackward0>)\n",
            "tensor(13398.7432, grad_fn=<AddBackward0>)\n",
            "tensor(13160.3066, grad_fn=<AddBackward0>)\n",
            "tensor(12378.8340, grad_fn=<AddBackward0>)\n",
            "tensor(12692.8027, grad_fn=<AddBackward0>)\n",
            "tensor(13481.9561, grad_fn=<AddBackward0>)\n",
            "tensor(13251.2773, grad_fn=<AddBackward0>)\n",
            "tensor(13504.1182, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [460/469], Reconst Loss: 79.5420, KL Div: 25.9589\n",
            "tensor(12836.8145, grad_fn=<AddBackward0>)\n",
            "tensor(13301.6387, grad_fn=<AddBackward0>)\n",
            "tensor(13313.7129, grad_fn=<AddBackward0>)\n",
            "tensor(12868.7578, grad_fn=<AddBackward0>)\n",
            "tensor(13353.1221, grad_fn=<AddBackward0>)\n",
            "tensor(13098.3691, grad_fn=<AddBackward0>)\n",
            "tensor(12810.5947, grad_fn=<AddBackward0>)\n",
            "tensor(13354.6143, grad_fn=<AddBackward0>)\n",
            "tensor(9718.3203, grad_fn=<AddBackward0>)\n",
            "tensor(13169.1465, grad_fn=<AddBackward0>)\n",
            "tensor(13029.0049, grad_fn=<AddBackward0>)\n",
            "tensor(12677.4766, grad_fn=<AddBackward0>)\n",
            "tensor(13504.6914, grad_fn=<AddBackward0>)\n",
            "tensor(13704.6025, grad_fn=<AddBackward0>)\n",
            "tensor(13058.6758, grad_fn=<AddBackward0>)\n",
            "tensor(13202.7012, grad_fn=<AddBackward0>)\n",
            "tensor(13120.7969, grad_fn=<AddBackward0>)\n",
            "tensor(13322.8906, grad_fn=<AddBackward0>)\n",
            "tensor(13601.9365, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 80.6075, KL Div: 25.6576\n",
            "tensor(12910.1426, grad_fn=<AddBackward0>)\n",
            "tensor(13400.2422, grad_fn=<AddBackward0>)\n",
            "tensor(13362.0977, grad_fn=<AddBackward0>)\n",
            "tensor(13066.0244, grad_fn=<AddBackward0>)\n",
            "tensor(12722.1777, grad_fn=<AddBackward0>)\n",
            "tensor(13124.5273, grad_fn=<AddBackward0>)\n",
            "tensor(13303.8984, grad_fn=<AddBackward0>)\n",
            "tensor(13103.1602, grad_fn=<AddBackward0>)\n",
            "tensor(12844.4688, grad_fn=<AddBackward0>)\n",
            "tensor(13272.9746, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [20/469], Reconst Loss: 78.4162, KL Div: 25.2789\n",
            "tensor(13342.0732, grad_fn=<AddBackward0>)\n",
            "tensor(13385.7090, grad_fn=<AddBackward0>)\n",
            "tensor(12773.3086, grad_fn=<AddBackward0>)\n",
            "tensor(12732.4365, grad_fn=<AddBackward0>)\n",
            "tensor(13715.6162, grad_fn=<AddBackward0>)\n",
            "tensor(13173.5898, grad_fn=<AddBackward0>)\n",
            "tensor(12494.3389, grad_fn=<AddBackward0>)\n",
            "tensor(13005.3672, grad_fn=<AddBackward0>)\n",
            "tensor(12928.8252, grad_fn=<AddBackward0>)\n",
            "tensor(13377.1055, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [30/469], Reconst Loss: 78.1699, KL Div: 26.3387\n",
            "tensor(13744.9863, grad_fn=<AddBackward0>)\n",
            "tensor(13035.6602, grad_fn=<AddBackward0>)\n",
            "tensor(13201.9385, grad_fn=<AddBackward0>)\n",
            "tensor(13609.9736, grad_fn=<AddBackward0>)\n",
            "tensor(13215.3203, grad_fn=<AddBackward0>)\n",
            "tensor(12812.7578, grad_fn=<AddBackward0>)\n",
            "tensor(13428.3711, grad_fn=<AddBackward0>)\n",
            "tensor(13158.6865, grad_fn=<AddBackward0>)\n",
            "tensor(13326.5645, grad_fn=<AddBackward0>)\n",
            "tensor(13349.8760, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [40/469], Reconst Loss: 79.4959, KL Div: 24.8000\n",
            "tensor(13303.4492, grad_fn=<AddBackward0>)\n",
            "tensor(13418.4297, grad_fn=<AddBackward0>)\n",
            "tensor(13182.4443, grad_fn=<AddBackward0>)\n",
            "tensor(13423.8711, grad_fn=<AddBackward0>)\n",
            "tensor(13285.8281, grad_fn=<AddBackward0>)\n",
            "tensor(13008.5771, grad_fn=<AddBackward0>)\n",
            "tensor(13025.7324, grad_fn=<AddBackward0>)\n",
            "tensor(13493.8770, grad_fn=<AddBackward0>)\n",
            "tensor(12803.5996, grad_fn=<AddBackward0>)\n",
            "tensor(13011.0967, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [50/469], Reconst Loss: 76.1891, KL Div: 25.4601\n",
            "tensor(13399.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13488.6426, grad_fn=<AddBackward0>)\n",
            "tensor(13330.8486, grad_fn=<AddBackward0>)\n",
            "tensor(13451.5889, grad_fn=<AddBackward0>)\n",
            "tensor(13461.2969, grad_fn=<AddBackward0>)\n",
            "tensor(13037.0400, grad_fn=<AddBackward0>)\n",
            "tensor(13212.0488, grad_fn=<AddBackward0>)\n",
            "tensor(13247.2939, grad_fn=<AddBackward0>)\n",
            "tensor(13525.6445, grad_fn=<AddBackward0>)\n",
            "tensor(12389.8730, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [60/469], Reconst Loss: 71.3276, KL Div: 25.4683\n",
            "tensor(12567.6133, grad_fn=<AddBackward0>)\n",
            "tensor(13311.5586, grad_fn=<AddBackward0>)\n",
            "tensor(12964.2969, grad_fn=<AddBackward0>)\n",
            "tensor(12941.9805, grad_fn=<AddBackward0>)\n",
            "tensor(12796.7500, grad_fn=<AddBackward0>)\n",
            "tensor(13110.6875, grad_fn=<AddBackward0>)\n",
            "tensor(13108.9551, grad_fn=<AddBackward0>)\n",
            "tensor(13215.7949, grad_fn=<AddBackward0>)\n",
            "tensor(13225.2422, grad_fn=<AddBackward0>)\n",
            "tensor(13351.4727, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [70/469], Reconst Loss: 79.0232, KL Div: 25.2852\n",
            "tensor(13395.1064, grad_fn=<AddBackward0>)\n",
            "tensor(13672.8184, grad_fn=<AddBackward0>)\n",
            "tensor(12674.6230, grad_fn=<AddBackward0>)\n",
            "tensor(12867.9150, grad_fn=<AddBackward0>)\n",
            "tensor(13141.1855, grad_fn=<AddBackward0>)\n",
            "tensor(13126.5312, grad_fn=<AddBackward0>)\n",
            "tensor(12821.9004, grad_fn=<AddBackward0>)\n",
            "tensor(13211.3242, grad_fn=<AddBackward0>)\n",
            "tensor(13010.7705, grad_fn=<AddBackward0>)\n",
            "tensor(13400.6797, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [80/469], Reconst Loss: 80.1007, KL Div: 24.5921\n",
            "tensor(13215.3125, grad_fn=<AddBackward0>)\n",
            "tensor(13427.4873, grad_fn=<AddBackward0>)\n",
            "tensor(13765.4863, grad_fn=<AddBackward0>)\n",
            "tensor(13045.8281, grad_fn=<AddBackward0>)\n",
            "tensor(12689.7461, grad_fn=<AddBackward0>)\n",
            "tensor(13425.3818, grad_fn=<AddBackward0>)\n",
            "tensor(13043.2061, grad_fn=<AddBackward0>)\n",
            "tensor(13394.1523, grad_fn=<AddBackward0>)\n",
            "tensor(12878.6348, grad_fn=<AddBackward0>)\n",
            "tensor(13248.0811, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [90/469], Reconst Loss: 77.5744, KL Div: 25.9263\n",
            "tensor(13471.5664, grad_fn=<AddBackward0>)\n",
            "tensor(12839.9375, grad_fn=<AddBackward0>)\n",
            "tensor(13244.1357, grad_fn=<AddBackward0>)\n",
            "tensor(13021.4629, grad_fn=<AddBackward0>)\n",
            "tensor(12964.0117, grad_fn=<AddBackward0>)\n",
            "tensor(13432.1084, grad_fn=<AddBackward0>)\n",
            "tensor(13084.9951, grad_fn=<AddBackward0>)\n",
            "tensor(13200.5615, grad_fn=<AddBackward0>)\n",
            "tensor(13178.9121, grad_fn=<AddBackward0>)\n",
            "tensor(13498.3984, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [100/469], Reconst Loss: 79.4330, KL Div: 26.0232\n",
            "tensor(13754.6367, grad_fn=<AddBackward0>)\n",
            "tensor(13181.7705, grad_fn=<AddBackward0>)\n",
            "tensor(12707., grad_fn=<AddBackward0>)\n",
            "tensor(12997.3389, grad_fn=<AddBackward0>)\n",
            "tensor(13239.4141, grad_fn=<AddBackward0>)\n",
            "tensor(12920.7861, grad_fn=<AddBackward0>)\n",
            "tensor(13061.9727, grad_fn=<AddBackward0>)\n",
            "tensor(13406.8555, grad_fn=<AddBackward0>)\n",
            "tensor(13252.6436, grad_fn=<AddBackward0>)\n",
            "tensor(13056.8408, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [110/469], Reconst Loss: 76.7144, KL Div: 25.2922\n",
            "tensor(13224.0068, grad_fn=<AddBackward0>)\n",
            "tensor(13666.8066, grad_fn=<AddBackward0>)\n",
            "tensor(13633.1064, grad_fn=<AddBackward0>)\n",
            "tensor(13335.6738, grad_fn=<AddBackward0>)\n",
            "tensor(12870.7852, grad_fn=<AddBackward0>)\n",
            "tensor(12883.8760, grad_fn=<AddBackward0>)\n",
            "tensor(13380.6826, grad_fn=<AddBackward0>)\n",
            "tensor(13411.3750, grad_fn=<AddBackward0>)\n",
            "tensor(13216.3857, grad_fn=<AddBackward0>)\n",
            "tensor(13333.9551, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [120/469], Reconst Loss: 78.0809, KL Div: 26.0906\n",
            "tensor(12928.6172, grad_fn=<AddBackward0>)\n",
            "tensor(13139.5898, grad_fn=<AddBackward0>)\n",
            "tensor(13241.7490, grad_fn=<AddBackward0>)\n",
            "tensor(13442.2715, grad_fn=<AddBackward0>)\n",
            "tensor(12776.9111, grad_fn=<AddBackward0>)\n",
            "tensor(13314.9014, grad_fn=<AddBackward0>)\n",
            "tensor(13164.0674, grad_fn=<AddBackward0>)\n",
            "tensor(13100.7305, grad_fn=<AddBackward0>)\n",
            "tensor(13167.4355, grad_fn=<AddBackward0>)\n",
            "tensor(12846.5176, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [130/469], Reconst Loss: 75.4984, KL Div: 24.8650\n",
            "tensor(13357.6904, grad_fn=<AddBackward0>)\n",
            "tensor(13460.0020, grad_fn=<AddBackward0>)\n",
            "tensor(13283.8936, grad_fn=<AddBackward0>)\n",
            "tensor(13750.3535, grad_fn=<AddBackward0>)\n",
            "tensor(13276.3770, grad_fn=<AddBackward0>)\n",
            "tensor(13300.3594, grad_fn=<AddBackward0>)\n",
            "tensor(13013.1807, grad_fn=<AddBackward0>)\n",
            "tensor(13145.5801, grad_fn=<AddBackward0>)\n",
            "tensor(13453.7363, grad_fn=<AddBackward0>)\n",
            "tensor(12772.6426, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [140/469], Reconst Loss: 75.0653, KL Div: 24.7209\n",
            "tensor(12750.5371, grad_fn=<AddBackward0>)\n",
            "tensor(13253.4814, grad_fn=<AddBackward0>)\n",
            "tensor(13374.8164, grad_fn=<AddBackward0>)\n",
            "tensor(13144.9531, grad_fn=<AddBackward0>)\n",
            "tensor(13120.9131, grad_fn=<AddBackward0>)\n",
            "tensor(13668.8135, grad_fn=<AddBackward0>)\n",
            "tensor(13094.5352, grad_fn=<AddBackward0>)\n",
            "tensor(13503.7549, grad_fn=<AddBackward0>)\n",
            "tensor(13454.5400, grad_fn=<AddBackward0>)\n",
            "tensor(13254.1855, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [150/469], Reconst Loss: 77.5507, KL Div: 25.9977\n",
            "tensor(13136.0781, grad_fn=<AddBackward0>)\n",
            "tensor(13123.2314, grad_fn=<AddBackward0>)\n",
            "tensor(13410.0410, grad_fn=<AddBackward0>)\n",
            "tensor(13423.2500, grad_fn=<AddBackward0>)\n",
            "tensor(13132.8955, grad_fn=<AddBackward0>)\n",
            "tensor(12554.7295, grad_fn=<AddBackward0>)\n",
            "tensor(13443.4873, grad_fn=<AddBackward0>)\n",
            "tensor(13297.1357, grad_fn=<AddBackward0>)\n",
            "tensor(13211.8799, grad_fn=<AddBackward0>)\n",
            "tensor(13058.4355, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [160/469], Reconst Loss: 76.2618, KL Div: 25.7572\n",
            "tensor(13653.5146, grad_fn=<AddBackward0>)\n",
            "tensor(12620.9824, grad_fn=<AddBackward0>)\n",
            "tensor(13302.6514, grad_fn=<AddBackward0>)\n",
            "tensor(13200.9619, grad_fn=<AddBackward0>)\n",
            "tensor(12728.5732, grad_fn=<AddBackward0>)\n",
            "tensor(13058.4414, grad_fn=<AddBackward0>)\n",
            "tensor(13704.3291, grad_fn=<AddBackward0>)\n",
            "tensor(13512.1973, grad_fn=<AddBackward0>)\n",
            "tensor(13098.9375, grad_fn=<AddBackward0>)\n",
            "tensor(13229.8379, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [170/469], Reconst Loss: 77.7022, KL Div: 25.6559\n",
            "tensor(13338.6172, grad_fn=<AddBackward0>)\n",
            "tensor(13878.8516, grad_fn=<AddBackward0>)\n",
            "tensor(13158.9648, grad_fn=<AddBackward0>)\n",
            "tensor(13242.6592, grad_fn=<AddBackward0>)\n",
            "tensor(13016.8154, grad_fn=<AddBackward0>)\n",
            "tensor(13390.9492, grad_fn=<AddBackward0>)\n",
            "tensor(13162.8340, grad_fn=<AddBackward0>)\n",
            "tensor(13380.4463, grad_fn=<AddBackward0>)\n",
            "tensor(12975.3994, grad_fn=<AddBackward0>)\n",
            "tensor(13374.5879, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [180/469], Reconst Loss: 79.0222, KL Div: 25.4668\n",
            "tensor(13065.3398, grad_fn=<AddBackward0>)\n",
            "tensor(12702.9258, grad_fn=<AddBackward0>)\n",
            "tensor(13303.1943, grad_fn=<AddBackward0>)\n",
            "tensor(13310.6299, grad_fn=<AddBackward0>)\n",
            "tensor(13425.8740, grad_fn=<AddBackward0>)\n",
            "tensor(13109.1895, grad_fn=<AddBackward0>)\n",
            "tensor(12412.8428, grad_fn=<AddBackward0>)\n",
            "tensor(13545.7568, grad_fn=<AddBackward0>)\n",
            "tensor(13358.2969, grad_fn=<AddBackward0>)\n",
            "tensor(13292.2539, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [190/469], Reconst Loss: 78.2463, KL Div: 25.5994\n",
            "tensor(13201.5137, grad_fn=<AddBackward0>)\n",
            "tensor(12818.6504, grad_fn=<AddBackward0>)\n",
            "tensor(13035.3496, grad_fn=<AddBackward0>)\n",
            "tensor(13255.2969, grad_fn=<AddBackward0>)\n",
            "tensor(12915.1777, grad_fn=<AddBackward0>)\n",
            "tensor(13437.0928, grad_fn=<AddBackward0>)\n",
            "tensor(13064.3408, grad_fn=<AddBackward0>)\n",
            "tensor(13070.3789, grad_fn=<AddBackward0>)\n",
            "tensor(13139.4531, grad_fn=<AddBackward0>)\n",
            "tensor(13100.0381, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [200/469], Reconst Loss: 77.2135, KL Div: 25.1306\n",
            "tensor(12699.7148, grad_fn=<AddBackward0>)\n",
            "tensor(13350.2520, grad_fn=<AddBackward0>)\n",
            "tensor(12907.6660, grad_fn=<AddBackward0>)\n",
            "tensor(13061.3154, grad_fn=<AddBackward0>)\n",
            "tensor(13329.3262, grad_fn=<AddBackward0>)\n",
            "tensor(12934.4160, grad_fn=<AddBackward0>)\n",
            "tensor(13257.6543, grad_fn=<AddBackward0>)\n",
            "tensor(13139.8691, grad_fn=<AddBackward0>)\n",
            "tensor(12961.8555, grad_fn=<AddBackward0>)\n",
            "tensor(13353.3184, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [210/469], Reconst Loss: 78.1508, KL Div: 26.1720\n",
            "tensor(13230.7471, grad_fn=<AddBackward0>)\n",
            "tensor(13268.3496, grad_fn=<AddBackward0>)\n",
            "tensor(13516.4297, grad_fn=<AddBackward0>)\n",
            "tensor(13785.8555, grad_fn=<AddBackward0>)\n",
            "tensor(13269.7383, grad_fn=<AddBackward0>)\n",
            "tensor(13151.2334, grad_fn=<AddBackward0>)\n",
            "tensor(13070.6738, grad_fn=<AddBackward0>)\n",
            "tensor(12826.2012, grad_fn=<AddBackward0>)\n",
            "tensor(12981.3945, grad_fn=<AddBackward0>)\n",
            "tensor(13095.7812, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [220/469], Reconst Loss: 77.5112, KL Div: 24.7996\n",
            "tensor(12837.6152, grad_fn=<AddBackward0>)\n",
            "tensor(12860.0557, grad_fn=<AddBackward0>)\n",
            "tensor(12956.9512, grad_fn=<AddBackward0>)\n",
            "tensor(13386.5625, grad_fn=<AddBackward0>)\n",
            "tensor(13247.3496, grad_fn=<AddBackward0>)\n",
            "tensor(13216.6094, grad_fn=<AddBackward0>)\n",
            "tensor(12998.2500, grad_fn=<AddBackward0>)\n",
            "tensor(13580.4336, grad_fn=<AddBackward0>)\n",
            "tensor(13576.8066, grad_fn=<AddBackward0>)\n",
            "tensor(13313.1016, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [230/469], Reconst Loss: 78.4906, KL Div: 25.5180\n",
            "tensor(12897.1621, grad_fn=<AddBackward0>)\n",
            "tensor(13517.8770, grad_fn=<AddBackward0>)\n",
            "tensor(13213.9141, grad_fn=<AddBackward0>)\n",
            "tensor(13280.8838, grad_fn=<AddBackward0>)\n",
            "tensor(12860.8379, grad_fn=<AddBackward0>)\n",
            "tensor(12777.6484, grad_fn=<AddBackward0>)\n",
            "tensor(12693.6270, grad_fn=<AddBackward0>)\n",
            "tensor(12873.9014, grad_fn=<AddBackward0>)\n",
            "tensor(13532.6504, grad_fn=<AddBackward0>)\n",
            "tensor(12805.6211, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [240/469], Reconst Loss: 75.2756, KL Div: 24.7683\n",
            "tensor(13149.8057, grad_fn=<AddBackward0>)\n",
            "tensor(13035.2559, grad_fn=<AddBackward0>)\n",
            "tensor(13467.4795, grad_fn=<AddBackward0>)\n",
            "tensor(12978.1396, grad_fn=<AddBackward0>)\n",
            "tensor(12861.9590, grad_fn=<AddBackward0>)\n",
            "tensor(12755.6953, grad_fn=<AddBackward0>)\n",
            "tensor(13320.3789, grad_fn=<AddBackward0>)\n",
            "tensor(13366.7656, grad_fn=<AddBackward0>)\n",
            "tensor(13204.5488, grad_fn=<AddBackward0>)\n",
            "tensor(13145.9590, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [250/469], Reconst Loss: 76.3275, KL Div: 26.3753\n",
            "tensor(12516.8145, grad_fn=<AddBackward0>)\n",
            "tensor(12993.7119, grad_fn=<AddBackward0>)\n",
            "tensor(13178.7197, grad_fn=<AddBackward0>)\n",
            "tensor(13431.3477, grad_fn=<AddBackward0>)\n",
            "tensor(13462.2949, grad_fn=<AddBackward0>)\n",
            "tensor(12692.3545, grad_fn=<AddBackward0>)\n",
            "tensor(13440.5938, grad_fn=<AddBackward0>)\n",
            "tensor(13721.1602, grad_fn=<AddBackward0>)\n",
            "tensor(13456.9668, grad_fn=<AddBackward0>)\n",
            "tensor(13457., grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [260/469], Reconst Loss: 79.1195, KL Div: 26.0133\n",
            "tensor(13215.9805, grad_fn=<AddBackward0>)\n",
            "tensor(12995.3525, grad_fn=<AddBackward0>)\n",
            "tensor(12838.1465, grad_fn=<AddBackward0>)\n",
            "tensor(13445.8438, grad_fn=<AddBackward0>)\n",
            "tensor(13248.2012, grad_fn=<AddBackward0>)\n",
            "tensor(13530.4355, grad_fn=<AddBackward0>)\n",
            "tensor(13102.3867, grad_fn=<AddBackward0>)\n",
            "tensor(13141.0254, grad_fn=<AddBackward0>)\n",
            "tensor(13082.1387, grad_fn=<AddBackward0>)\n",
            "tensor(13204.4883, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [270/469], Reconst Loss: 77.3446, KL Div: 25.8155\n",
            "tensor(13921.5732, grad_fn=<AddBackward0>)\n",
            "tensor(13134.3086, grad_fn=<AddBackward0>)\n",
            "tensor(13470.3242, grad_fn=<AddBackward0>)\n",
            "tensor(13242.7363, grad_fn=<AddBackward0>)\n",
            "tensor(13494.0449, grad_fn=<AddBackward0>)\n",
            "tensor(12994.2051, grad_fn=<AddBackward0>)\n",
            "tensor(13130.7158, grad_fn=<AddBackward0>)\n",
            "tensor(12990.5283, grad_fn=<AddBackward0>)\n",
            "tensor(13698.9131, grad_fn=<AddBackward0>)\n",
            "tensor(13416.0938, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [280/469], Reconst Loss: 78.9899, KL Div: 25.8234\n",
            "tensor(13398.0752, grad_fn=<AddBackward0>)\n",
            "tensor(12954.2559, grad_fn=<AddBackward0>)\n",
            "tensor(12547.0146, grad_fn=<AddBackward0>)\n",
            "tensor(13121.1982, grad_fn=<AddBackward0>)\n",
            "tensor(12682.6719, grad_fn=<AddBackward0>)\n",
            "tensor(13214.1514, grad_fn=<AddBackward0>)\n",
            "tensor(13253.0889, grad_fn=<AddBackward0>)\n",
            "tensor(13130.4414, grad_fn=<AddBackward0>)\n",
            "tensor(12864.5107, grad_fn=<AddBackward0>)\n",
            "tensor(13242.5615, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [290/469], Reconst Loss: 77.3471, KL Div: 26.1104\n",
            "tensor(13114.5566, grad_fn=<AddBackward0>)\n",
            "tensor(13583.1826, grad_fn=<AddBackward0>)\n",
            "tensor(13607.7559, grad_fn=<AddBackward0>)\n",
            "tensor(13224.6006, grad_fn=<AddBackward0>)\n",
            "tensor(13592.4023, grad_fn=<AddBackward0>)\n",
            "tensor(13292.1191, grad_fn=<AddBackward0>)\n",
            "tensor(13122.6572, grad_fn=<AddBackward0>)\n",
            "tensor(13221.7070, grad_fn=<AddBackward0>)\n",
            "tensor(13681.8252, grad_fn=<AddBackward0>)\n",
            "tensor(13421.4873, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [300/469], Reconst Loss: 78.6495, KL Div: 26.2058\n",
            "tensor(13772.3242, grad_fn=<AddBackward0>)\n",
            "tensor(13190.6201, grad_fn=<AddBackward0>)\n",
            "tensor(13309.5498, grad_fn=<AddBackward0>)\n",
            "tensor(13194.3320, grad_fn=<AddBackward0>)\n",
            "tensor(13409.4170, grad_fn=<AddBackward0>)\n",
            "tensor(13263.9141, grad_fn=<AddBackward0>)\n",
            "tensor(12974.9697, grad_fn=<AddBackward0>)\n",
            "tensor(13487.2617, grad_fn=<AddBackward0>)\n",
            "tensor(13423.0430, grad_fn=<AddBackward0>)\n",
            "tensor(13072.7422, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [310/469], Reconst Loss: 76.7845, KL Div: 25.3463\n",
            "tensor(13156.1279, grad_fn=<AddBackward0>)\n",
            "tensor(13198.6523, grad_fn=<AddBackward0>)\n",
            "tensor(13289.5010, grad_fn=<AddBackward0>)\n",
            "tensor(13015.7441, grad_fn=<AddBackward0>)\n",
            "tensor(12757.3457, grad_fn=<AddBackward0>)\n",
            "tensor(13316.4463, grad_fn=<AddBackward0>)\n",
            "tensor(12870.5918, grad_fn=<AddBackward0>)\n",
            "tensor(13440.7705, grad_fn=<AddBackward0>)\n",
            "tensor(13032.7344, grad_fn=<AddBackward0>)\n",
            "tensor(13403.5869, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [320/469], Reconst Loss: 79.2954, KL Div: 25.4201\n",
            "tensor(13353.8711, grad_fn=<AddBackward0>)\n",
            "tensor(13137.1484, grad_fn=<AddBackward0>)\n",
            "tensor(12907.1016, grad_fn=<AddBackward0>)\n",
            "tensor(12955.7490, grad_fn=<AddBackward0>)\n",
            "tensor(13261.0107, grad_fn=<AddBackward0>)\n",
            "tensor(13009.3838, grad_fn=<AddBackward0>)\n",
            "tensor(12768.4766, grad_fn=<AddBackward0>)\n",
            "tensor(13327.0488, grad_fn=<AddBackward0>)\n",
            "tensor(13087.9844, grad_fn=<AddBackward0>)\n",
            "tensor(13018.9375, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [330/469], Reconst Loss: 77.0260, KL Div: 24.6844\n",
            "tensor(12677.3584, grad_fn=<AddBackward0>)\n",
            "tensor(13334.0420, grad_fn=<AddBackward0>)\n",
            "tensor(13279.2842, grad_fn=<AddBackward0>)\n",
            "tensor(12850.1289, grad_fn=<AddBackward0>)\n",
            "tensor(13748.3701, grad_fn=<AddBackward0>)\n",
            "tensor(12792.3105, grad_fn=<AddBackward0>)\n",
            "tensor(13027.9258, grad_fn=<AddBackward0>)\n",
            "tensor(13300.5889, grad_fn=<AddBackward0>)\n",
            "tensor(13245.2949, grad_fn=<AddBackward0>)\n",
            "tensor(12983.3516, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [340/469], Reconst Loss: 76.1553, KL Div: 25.2771\n",
            "tensor(13243.4795, grad_fn=<AddBackward0>)\n",
            "tensor(12984.2852, grad_fn=<AddBackward0>)\n",
            "tensor(13865.3672, grad_fn=<AddBackward0>)\n",
            "tensor(12459.5898, grad_fn=<AddBackward0>)\n",
            "tensor(13179.8652, grad_fn=<AddBackward0>)\n",
            "tensor(13027.3203, grad_fn=<AddBackward0>)\n",
            "tensor(13077.5342, grad_fn=<AddBackward0>)\n",
            "tensor(13337.7334, grad_fn=<AddBackward0>)\n",
            "tensor(12842.5410, grad_fn=<AddBackward0>)\n",
            "tensor(13363.7930, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [350/469], Reconst Loss: 78.3074, KL Div: 26.0973\n",
            "tensor(13251.8887, grad_fn=<AddBackward0>)\n",
            "tensor(13152.0547, grad_fn=<AddBackward0>)\n",
            "tensor(13250.6924, grad_fn=<AddBackward0>)\n",
            "tensor(12723.2910, grad_fn=<AddBackward0>)\n",
            "tensor(13102.9209, grad_fn=<AddBackward0>)\n",
            "tensor(13120.6328, grad_fn=<AddBackward0>)\n",
            "tensor(12938.1807, grad_fn=<AddBackward0>)\n",
            "tensor(12986.2168, grad_fn=<AddBackward0>)\n",
            "tensor(13259.5635, grad_fn=<AddBackward0>)\n",
            "tensor(13287.1094, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [360/469], Reconst Loss: 78.8878, KL Div: 24.9177\n",
            "tensor(13551.9727, grad_fn=<AddBackward0>)\n",
            "tensor(13286.1221, grad_fn=<AddBackward0>)\n",
            "tensor(13180.3896, grad_fn=<AddBackward0>)\n",
            "tensor(13068.3057, grad_fn=<AddBackward0>)\n",
            "tensor(13494.8398, grad_fn=<AddBackward0>)\n",
            "tensor(13687.9170, grad_fn=<AddBackward0>)\n",
            "tensor(12840.2188, grad_fn=<AddBackward0>)\n",
            "tensor(12903.0244, grad_fn=<AddBackward0>)\n",
            "tensor(13138.8691, grad_fn=<AddBackward0>)\n",
            "tensor(12976.7344, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [370/469], Reconst Loss: 76.0708, KL Div: 25.3100\n",
            "tensor(13130.9678, grad_fn=<AddBackward0>)\n",
            "tensor(13124.2783, grad_fn=<AddBackward0>)\n",
            "tensor(13353.5098, grad_fn=<AddBackward0>)\n",
            "tensor(13037.7354, grad_fn=<AddBackward0>)\n",
            "tensor(13280.1699, grad_fn=<AddBackward0>)\n",
            "tensor(13566.8301, grad_fn=<AddBackward0>)\n",
            "tensor(13393.7832, grad_fn=<AddBackward0>)\n",
            "tensor(13530.9648, grad_fn=<AddBackward0>)\n",
            "tensor(12605.9395, grad_fn=<AddBackward0>)\n",
            "tensor(13318.3477, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [380/469], Reconst Loss: 78.2601, KL Div: 25.7895\n",
            "tensor(12887.1416, grad_fn=<AddBackward0>)\n",
            "tensor(12534.9609, grad_fn=<AddBackward0>)\n",
            "tensor(13137.4932, grad_fn=<AddBackward0>)\n",
            "tensor(13395.3848, grad_fn=<AddBackward0>)\n",
            "tensor(13630.2715, grad_fn=<AddBackward0>)\n",
            "tensor(12828.4639, grad_fn=<AddBackward0>)\n",
            "tensor(13248.7480, grad_fn=<AddBackward0>)\n",
            "tensor(13271.1465, grad_fn=<AddBackward0>)\n",
            "tensor(13316.4072, grad_fn=<AddBackward0>)\n",
            "tensor(13043.4385, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [390/469], Reconst Loss: 77.2722, KL Div: 24.6297\n",
            "tensor(12921.8242, grad_fn=<AddBackward0>)\n",
            "tensor(13258.0996, grad_fn=<AddBackward0>)\n",
            "tensor(13368.6338, grad_fn=<AddBackward0>)\n",
            "tensor(13217.1641, grad_fn=<AddBackward0>)\n",
            "tensor(13763.8086, grad_fn=<AddBackward0>)\n",
            "tensor(13011.7607, grad_fn=<AddBackward0>)\n",
            "tensor(13380.1562, grad_fn=<AddBackward0>)\n",
            "tensor(13453.2725, grad_fn=<AddBackward0>)\n",
            "tensor(13258.1377, grad_fn=<AddBackward0>)\n",
            "tensor(13054.0986, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [400/469], Reconst Loss: 76.0426, KL Div: 25.9425\n",
            "tensor(13365.4883, grad_fn=<AddBackward0>)\n",
            "tensor(13300.7559, grad_fn=<AddBackward0>)\n",
            "tensor(13125.6748, grad_fn=<AddBackward0>)\n",
            "tensor(13525.4287, grad_fn=<AddBackward0>)\n",
            "tensor(13383.0742, grad_fn=<AddBackward0>)\n",
            "tensor(12755.3574, grad_fn=<AddBackward0>)\n",
            "tensor(13364.2266, grad_fn=<AddBackward0>)\n",
            "tensor(13114.7666, grad_fn=<AddBackward0>)\n",
            "tensor(13606.0557, grad_fn=<AddBackward0>)\n",
            "tensor(13687.1006, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [410/469], Reconst Loss: 81.1141, KL Div: 25.8164\n",
            "tensor(13086.5225, grad_fn=<AddBackward0>)\n",
            "tensor(13073.1787, grad_fn=<AddBackward0>)\n",
            "tensor(12861.9238, grad_fn=<AddBackward0>)\n",
            "tensor(13384.5635, grad_fn=<AddBackward0>)\n",
            "tensor(12971.3672, grad_fn=<AddBackward0>)\n",
            "tensor(13024.6562, grad_fn=<AddBackward0>)\n",
            "tensor(13527.0215, grad_fn=<AddBackward0>)\n",
            "tensor(13358.2520, grad_fn=<AddBackward0>)\n",
            "tensor(12905.4238, grad_fn=<AddBackward0>)\n",
            "tensor(12684.0273, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [420/469], Reconst Loss: 73.7769, KL Div: 25.3170\n",
            "tensor(13015.1709, grad_fn=<AddBackward0>)\n",
            "tensor(12672.1865, grad_fn=<AddBackward0>)\n",
            "tensor(13259.5938, grad_fn=<AddBackward0>)\n",
            "tensor(13161.3604, grad_fn=<AddBackward0>)\n",
            "tensor(13024.4609, grad_fn=<AddBackward0>)\n",
            "tensor(13089.2627, grad_fn=<AddBackward0>)\n",
            "tensor(13426.0156, grad_fn=<AddBackward0>)\n",
            "tensor(13781.6738, grad_fn=<AddBackward0>)\n",
            "tensor(13513.4668, grad_fn=<AddBackward0>)\n",
            "tensor(13633.6064, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [430/469], Reconst Loss: 80.5836, KL Div: 25.9289\n",
            "tensor(13106.7812, grad_fn=<AddBackward0>)\n",
            "tensor(13330.4121, grad_fn=<AddBackward0>)\n",
            "tensor(13766.8486, grad_fn=<AddBackward0>)\n",
            "tensor(13137.3105, grad_fn=<AddBackward0>)\n",
            "tensor(13123.4561, grad_fn=<AddBackward0>)\n",
            "tensor(13212.1445, grad_fn=<AddBackward0>)\n",
            "tensor(13290.4131, grad_fn=<AddBackward0>)\n",
            "tensor(13368.6367, grad_fn=<AddBackward0>)\n",
            "tensor(13406.6348, grad_fn=<AddBackward0>)\n",
            "tensor(13286.1973, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [440/469], Reconst Loss: 78.4471, KL Div: 25.3513\n",
            "tensor(13534.4424, grad_fn=<AddBackward0>)\n",
            "tensor(13042.1768, grad_fn=<AddBackward0>)\n",
            "tensor(12673.3789, grad_fn=<AddBackward0>)\n",
            "tensor(13493.0840, grad_fn=<AddBackward0>)\n",
            "tensor(13212.9980, grad_fn=<AddBackward0>)\n",
            "tensor(13421.2988, grad_fn=<AddBackward0>)\n",
            "tensor(13015.1602, grad_fn=<AddBackward0>)\n",
            "tensor(13451.2139, grad_fn=<AddBackward0>)\n",
            "tensor(12579.7373, grad_fn=<AddBackward0>)\n",
            "tensor(13662.7959, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [450/469], Reconst Loss: 81.0364, KL Div: 25.7042\n",
            "tensor(13493.0293, grad_fn=<AddBackward0>)\n",
            "tensor(13729.2559, grad_fn=<AddBackward0>)\n",
            "tensor(12856.6084, grad_fn=<AddBackward0>)\n",
            "tensor(13026.2109, grad_fn=<AddBackward0>)\n",
            "tensor(12963.0166, grad_fn=<AddBackward0>)\n",
            "tensor(13156.8623, grad_fn=<AddBackward0>)\n",
            "tensor(13341.3213, grad_fn=<AddBackward0>)\n",
            "tensor(13233.3252, grad_fn=<AddBackward0>)\n",
            "tensor(13104.0205, grad_fn=<AddBackward0>)\n",
            "tensor(13019.4004, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [460/469], Reconst Loss: 76.2837, KL Div: 25.4303\n",
            "tensor(13273.7949, grad_fn=<AddBackward0>)\n",
            "tensor(12885.2959, grad_fn=<AddBackward0>)\n",
            "tensor(13160.8906, grad_fn=<AddBackward0>)\n",
            "tensor(12806.6016, grad_fn=<AddBackward0>)\n",
            "tensor(13413.4297, grad_fn=<AddBackward0>)\n",
            "tensor(13023.3672, grad_fn=<AddBackward0>)\n",
            "tensor(12807.3105, grad_fn=<AddBackward0>)\n",
            "tensor(12727.0342, grad_fn=<AddBackward0>)\n",
            "tensor(9693.1777, grad_fn=<AddBackward0>)\n",
            "tensor(13263.2920, grad_fn=<AddBackward0>)\n",
            "tensor(13401.5137, grad_fn=<AddBackward0>)\n",
            "tensor(13151.3564, grad_fn=<AddBackward0>)\n",
            "tensor(13451.3975, grad_fn=<AddBackward0>)\n",
            "tensor(13499.4307, grad_fn=<AddBackward0>)\n",
            "tensor(13114.4648, grad_fn=<AddBackward0>)\n",
            "tensor(12889.6816, grad_fn=<AddBackward0>)\n",
            "tensor(13015.5205, grad_fn=<AddBackward0>)\n",
            "tensor(12996.1504, grad_fn=<AddBackward0>)\n",
            "tensor(12691.1504, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 74.5904, KL Div: 24.5592\n",
            "tensor(13544.1719, grad_fn=<AddBackward0>)\n",
            "tensor(13093.0371, grad_fn=<AddBackward0>)\n",
            "tensor(12786.4326, grad_fn=<AddBackward0>)\n",
            "tensor(12795.5479, grad_fn=<AddBackward0>)\n",
            "tensor(12895.2334, grad_fn=<AddBackward0>)\n",
            "tensor(12889.1660, grad_fn=<AddBackward0>)\n",
            "tensor(12682.0527, grad_fn=<AddBackward0>)\n",
            "tensor(12997.8965, grad_fn=<AddBackward0>)\n",
            "tensor(13224.4746, grad_fn=<AddBackward0>)\n",
            "tensor(12806.1055, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [20/469], Reconst Loss: 74.7216, KL Div: 25.3261\n",
            "tensor(13292.2168, grad_fn=<AddBackward0>)\n",
            "tensor(13001.6797, grad_fn=<AddBackward0>)\n",
            "tensor(13222.9941, grad_fn=<AddBackward0>)\n",
            "tensor(13054.7041, grad_fn=<AddBackward0>)\n",
            "tensor(13411.9902, grad_fn=<AddBackward0>)\n",
            "tensor(13596.5947, grad_fn=<AddBackward0>)\n",
            "tensor(13192.7783, grad_fn=<AddBackward0>)\n",
            "tensor(13806.7979, grad_fn=<AddBackward0>)\n",
            "tensor(13144.5244, grad_fn=<AddBackward0>)\n",
            "tensor(13068.1289, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [30/469], Reconst Loss: 76.6035, KL Div: 25.4913\n",
            "tensor(12737.7148, grad_fn=<AddBackward0>)\n",
            "tensor(13754.2461, grad_fn=<AddBackward0>)\n",
            "tensor(13329.2041, grad_fn=<AddBackward0>)\n",
            "tensor(13450.2178, grad_fn=<AddBackward0>)\n",
            "tensor(13322.8193, grad_fn=<AddBackward0>)\n",
            "tensor(13561.0479, grad_fn=<AddBackward0>)\n",
            "tensor(13088.7578, grad_fn=<AddBackward0>)\n",
            "tensor(12651.9482, grad_fn=<AddBackward0>)\n",
            "tensor(13321.4414, grad_fn=<AddBackward0>)\n",
            "tensor(13271.0742, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [40/469], Reconst Loss: 78.1163, KL Div: 25.5639\n",
            "tensor(13191.3887, grad_fn=<AddBackward0>)\n",
            "tensor(12828.3711, grad_fn=<AddBackward0>)\n",
            "tensor(13107.7188, grad_fn=<AddBackward0>)\n",
            "tensor(13090.8281, grad_fn=<AddBackward0>)\n",
            "tensor(12798.2324, grad_fn=<AddBackward0>)\n",
            "tensor(13544.7637, grad_fn=<AddBackward0>)\n",
            "tensor(12700.3291, grad_fn=<AddBackward0>)\n",
            "tensor(13656.5986, grad_fn=<AddBackward0>)\n",
            "tensor(12911.1738, grad_fn=<AddBackward0>)\n",
            "tensor(13248.1191, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [50/469], Reconst Loss: 78.0539, KL Div: 25.4470\n",
            "tensor(13356.6250, grad_fn=<AddBackward0>)\n",
            "tensor(13045.9375, grad_fn=<AddBackward0>)\n",
            "tensor(13095.5000, grad_fn=<AddBackward0>)\n",
            "tensor(13540.9238, grad_fn=<AddBackward0>)\n",
            "tensor(12951.4463, grad_fn=<AddBackward0>)\n",
            "tensor(13181.3457, grad_fn=<AddBackward0>)\n",
            "tensor(13017.0488, grad_fn=<AddBackward0>)\n",
            "tensor(13161.8379, grad_fn=<AddBackward0>)\n",
            "tensor(12881.9678, grad_fn=<AddBackward0>)\n",
            "tensor(12911.6777, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [60/469], Reconst Loss: 76.3554, KL Div: 24.5171\n",
            "tensor(13766.1572, grad_fn=<AddBackward0>)\n",
            "tensor(13164.8379, grad_fn=<AddBackward0>)\n",
            "tensor(13075.8916, grad_fn=<AddBackward0>)\n",
            "tensor(13448.4932, grad_fn=<AddBackward0>)\n",
            "tensor(13102.5117, grad_fn=<AddBackward0>)\n",
            "tensor(13556.7012, grad_fn=<AddBackward0>)\n",
            "tensor(13228.2285, grad_fn=<AddBackward0>)\n",
            "tensor(13359.0508, grad_fn=<AddBackward0>)\n",
            "tensor(13029.5547, grad_fn=<AddBackward0>)\n",
            "tensor(13257.9131, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [70/469], Reconst Loss: 77.4915, KL Div: 26.0859\n",
            "tensor(13441.5059, grad_fn=<AddBackward0>)\n",
            "tensor(12789.5098, grad_fn=<AddBackward0>)\n",
            "tensor(12865.7217, grad_fn=<AddBackward0>)\n",
            "tensor(13090.0293, grad_fn=<AddBackward0>)\n",
            "tensor(13117.5303, grad_fn=<AddBackward0>)\n",
            "tensor(13421.3008, grad_fn=<AddBackward0>)\n",
            "tensor(13042.3125, grad_fn=<AddBackward0>)\n",
            "tensor(13224.7305, grad_fn=<AddBackward0>)\n",
            "tensor(13416.4707, grad_fn=<AddBackward0>)\n",
            "tensor(14034.3770, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [80/469], Reconst Loss: 82.7598, KL Div: 26.8837\n",
            "tensor(13406.1279, grad_fn=<AddBackward0>)\n",
            "tensor(13033.1016, grad_fn=<AddBackward0>)\n",
            "tensor(13016.7861, grad_fn=<AddBackward0>)\n",
            "tensor(13339.7969, grad_fn=<AddBackward0>)\n",
            "tensor(13219.7510, grad_fn=<AddBackward0>)\n",
            "tensor(13103.0557, grad_fn=<AddBackward0>)\n",
            "tensor(13106.9443, grad_fn=<AddBackward0>)\n",
            "tensor(12901.0186, grad_fn=<AddBackward0>)\n",
            "tensor(13294.3652, grad_fn=<AddBackward0>)\n",
            "tensor(12936.5107, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [90/469], Reconst Loss: 76.1766, KL Div: 24.8899\n",
            "tensor(13165.0391, grad_fn=<AddBackward0>)\n",
            "tensor(13336.2188, grad_fn=<AddBackward0>)\n",
            "tensor(13474.5938, grad_fn=<AddBackward0>)\n",
            "tensor(13166.4932, grad_fn=<AddBackward0>)\n",
            "tensor(13408.0293, grad_fn=<AddBackward0>)\n",
            "tensor(13185.0293, grad_fn=<AddBackward0>)\n",
            "tensor(13642.0801, grad_fn=<AddBackward0>)\n",
            "tensor(13338.3057, grad_fn=<AddBackward0>)\n",
            "tensor(13124.6016, grad_fn=<AddBackward0>)\n",
            "tensor(12785.1465, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [100/469], Reconst Loss: 74.7116, KL Div: 25.1723\n",
            "tensor(12906.5557, grad_fn=<AddBackward0>)\n",
            "tensor(13122.7656, grad_fn=<AddBackward0>)\n",
            "tensor(13393.5508, grad_fn=<AddBackward0>)\n",
            "tensor(12878.8623, grad_fn=<AddBackward0>)\n",
            "tensor(13217.0771, grad_fn=<AddBackward0>)\n",
            "tensor(13252.3711, grad_fn=<AddBackward0>)\n",
            "tensor(13057.3945, grad_fn=<AddBackward0>)\n",
            "tensor(13322.0234, grad_fn=<AddBackward0>)\n",
            "tensor(12666.8467, grad_fn=<AddBackward0>)\n",
            "tensor(13071.9580, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [110/469], Reconst Loss: 76.6876, KL Div: 25.4370\n",
            "tensor(13441.2734, grad_fn=<AddBackward0>)\n",
            "tensor(13260.9082, grad_fn=<AddBackward0>)\n",
            "tensor(13328.1816, grad_fn=<AddBackward0>)\n",
            "tensor(13392.6738, grad_fn=<AddBackward0>)\n",
            "tensor(12565.4961, grad_fn=<AddBackward0>)\n",
            "tensor(13172.8555, grad_fn=<AddBackward0>)\n",
            "tensor(13088.5771, grad_fn=<AddBackward0>)\n",
            "tensor(13593.9941, grad_fn=<AddBackward0>)\n",
            "tensor(13713.9736, grad_fn=<AddBackward0>)\n",
            "tensor(13255.4219, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [120/469], Reconst Loss: 78.0100, KL Div: 25.5480\n",
            "tensor(13517.7900, grad_fn=<AddBackward0>)\n",
            "tensor(13033.7920, grad_fn=<AddBackward0>)\n",
            "tensor(13410.5605, grad_fn=<AddBackward0>)\n",
            "tensor(13149.6660, grad_fn=<AddBackward0>)\n",
            "tensor(13527.9609, grad_fn=<AddBackward0>)\n",
            "tensor(13488.6230, grad_fn=<AddBackward0>)\n",
            "tensor(13338.2354, grad_fn=<AddBackward0>)\n",
            "tensor(13393.5088, grad_fn=<AddBackward0>)\n",
            "tensor(13361.5088, grad_fn=<AddBackward0>)\n",
            "tensor(13504.9463, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [130/469], Reconst Loss: 79.4885, KL Div: 26.0189\n",
            "tensor(13013.5410, grad_fn=<AddBackward0>)\n",
            "tensor(13416.6016, grad_fn=<AddBackward0>)\n",
            "tensor(12993.3027, grad_fn=<AddBackward0>)\n",
            "tensor(12922.9795, grad_fn=<AddBackward0>)\n",
            "tensor(12896.4541, grad_fn=<AddBackward0>)\n",
            "tensor(12767.1504, grad_fn=<AddBackward0>)\n",
            "tensor(13159.9961, grad_fn=<AddBackward0>)\n",
            "tensor(13783.7412, grad_fn=<AddBackward0>)\n",
            "tensor(13616.2666, grad_fn=<AddBackward0>)\n",
            "tensor(13139.8613, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [140/469], Reconst Loss: 77.0844, KL Div: 25.5707\n",
            "tensor(13491.6104, grad_fn=<AddBackward0>)\n",
            "tensor(12819.2324, grad_fn=<AddBackward0>)\n",
            "tensor(12862.8779, grad_fn=<AddBackward0>)\n",
            "tensor(13744.7246, grad_fn=<AddBackward0>)\n",
            "tensor(13095.9414, grad_fn=<AddBackward0>)\n",
            "tensor(13836.3203, grad_fn=<AddBackward0>)\n",
            "tensor(13769.1865, grad_fn=<AddBackward0>)\n",
            "tensor(13158.9258, grad_fn=<AddBackward0>)\n",
            "tensor(12615.0332, grad_fn=<AddBackward0>)\n",
            "tensor(13302.4707, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [150/469], Reconst Loss: 78.6413, KL Div: 25.2843\n",
            "tensor(13064.3740, grad_fn=<AddBackward0>)\n",
            "tensor(13439.1230, grad_fn=<AddBackward0>)\n",
            "tensor(13066.2129, grad_fn=<AddBackward0>)\n",
            "tensor(13415.4590, grad_fn=<AddBackward0>)\n",
            "tensor(13756.0664, grad_fn=<AddBackward0>)\n",
            "tensor(13053.9297, grad_fn=<AddBackward0>)\n",
            "tensor(13015.6035, grad_fn=<AddBackward0>)\n",
            "tensor(13392.0322, grad_fn=<AddBackward0>)\n",
            "tensor(12901.5801, grad_fn=<AddBackward0>)\n",
            "tensor(13498.2490, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [160/469], Reconst Loss: 79.0886, KL Div: 26.3665\n",
            "tensor(12742.2607, grad_fn=<AddBackward0>)\n",
            "tensor(13189.3916, grad_fn=<AddBackward0>)\n",
            "tensor(13040.1758, grad_fn=<AddBackward0>)\n",
            "tensor(13970.2402, grad_fn=<AddBackward0>)\n",
            "tensor(13221.9297, grad_fn=<AddBackward0>)\n",
            "tensor(13016.6641, grad_fn=<AddBackward0>)\n",
            "tensor(12960.9980, grad_fn=<AddBackward0>)\n",
            "tensor(12991.4229, grad_fn=<AddBackward0>)\n",
            "tensor(12989.3857, grad_fn=<AddBackward0>)\n",
            "tensor(13519.7197, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [170/469], Reconst Loss: 79.3551, KL Div: 26.2677\n",
            "tensor(13428.5078, grad_fn=<AddBackward0>)\n",
            "tensor(13177.4463, grad_fn=<AddBackward0>)\n",
            "tensor(13055.8418, grad_fn=<AddBackward0>)\n",
            "tensor(13107.8418, grad_fn=<AddBackward0>)\n",
            "tensor(13387.6748, grad_fn=<AddBackward0>)\n",
            "tensor(12933.2129, grad_fn=<AddBackward0>)\n",
            "tensor(12994.5557, grad_fn=<AddBackward0>)\n",
            "tensor(13609.5547, grad_fn=<AddBackward0>)\n",
            "tensor(12663.7148, grad_fn=<AddBackward0>)\n",
            "tensor(13127.5791, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [180/469], Reconst Loss: 76.6682, KL Div: 25.8910\n",
            "tensor(13631.4434, grad_fn=<AddBackward0>)\n",
            "tensor(13282.0391, grad_fn=<AddBackward0>)\n",
            "tensor(13541.7539, grad_fn=<AddBackward0>)\n",
            "tensor(13468.1182, grad_fn=<AddBackward0>)\n",
            "tensor(13247.5928, grad_fn=<AddBackward0>)\n",
            "tensor(13278.4922, grad_fn=<AddBackward0>)\n",
            "tensor(13262.0059, grad_fn=<AddBackward0>)\n",
            "tensor(12909.7617, grad_fn=<AddBackward0>)\n",
            "tensor(13345.0615, grad_fn=<AddBackward0>)\n",
            "tensor(12903.7344, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [190/469], Reconst Loss: 75.5503, KL Div: 25.2601\n",
            "tensor(13501.3301, grad_fn=<AddBackward0>)\n",
            "tensor(12813.9658, grad_fn=<AddBackward0>)\n",
            "tensor(12290.5547, grad_fn=<AddBackward0>)\n",
            "tensor(13244.2090, grad_fn=<AddBackward0>)\n",
            "tensor(13387.7500, grad_fn=<AddBackward0>)\n",
            "tensor(13147.3115, grad_fn=<AddBackward0>)\n",
            "tensor(13003.4316, grad_fn=<AddBackward0>)\n",
            "tensor(13080.8418, grad_fn=<AddBackward0>)\n",
            "tensor(12751.9355, grad_fn=<AddBackward0>)\n",
            "tensor(12685.7061, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [200/469], Reconst Loss: 73.7260, KL Div: 25.3811\n",
            "tensor(13495.0146, grad_fn=<AddBackward0>)\n",
            "tensor(13167.8145, grad_fn=<AddBackward0>)\n",
            "tensor(13052.2070, grad_fn=<AddBackward0>)\n",
            "tensor(13495.4404, grad_fn=<AddBackward0>)\n",
            "tensor(13949.7168, grad_fn=<AddBackward0>)\n",
            "tensor(13337.3662, grad_fn=<AddBackward0>)\n",
            "tensor(13005.7139, grad_fn=<AddBackward0>)\n",
            "tensor(12989.7988, grad_fn=<AddBackward0>)\n",
            "tensor(13146.7354, grad_fn=<AddBackward0>)\n",
            "tensor(13218.4473, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [210/469], Reconst Loss: 78.3296, KL Div: 24.9395\n",
            "tensor(12465.2188, grad_fn=<AddBackward0>)\n",
            "tensor(12982.8438, grad_fn=<AddBackward0>)\n",
            "tensor(12857.0039, grad_fn=<AddBackward0>)\n",
            "tensor(12992.1055, grad_fn=<AddBackward0>)\n",
            "tensor(13771.3701, grad_fn=<AddBackward0>)\n",
            "tensor(13557.6201, grad_fn=<AddBackward0>)\n",
            "tensor(12963.4062, grad_fn=<AddBackward0>)\n",
            "tensor(13387.6182, grad_fn=<AddBackward0>)\n",
            "tensor(13077.1748, grad_fn=<AddBackward0>)\n",
            "tensor(13205.7461, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [220/469], Reconst Loss: 77.8187, KL Div: 25.3512\n",
            "tensor(13285.8789, grad_fn=<AddBackward0>)\n",
            "tensor(12882.2979, grad_fn=<AddBackward0>)\n",
            "tensor(13399.2734, grad_fn=<AddBackward0>)\n",
            "tensor(13169.8438, grad_fn=<AddBackward0>)\n",
            "tensor(12961.9883, grad_fn=<AddBackward0>)\n",
            "tensor(13258.3408, grad_fn=<AddBackward0>)\n",
            "tensor(13076.7520, grad_fn=<AddBackward0>)\n",
            "tensor(13005.0254, grad_fn=<AddBackward0>)\n",
            "tensor(13117.4473, grad_fn=<AddBackward0>)\n",
            "tensor(12399.2490, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [230/469], Reconst Loss: 71.8940, KL Div: 24.9752\n",
            "tensor(13093.1807, grad_fn=<AddBackward0>)\n",
            "tensor(13124.7246, grad_fn=<AddBackward0>)\n",
            "tensor(13106.0332, grad_fn=<AddBackward0>)\n",
            "tensor(13123.9814, grad_fn=<AddBackward0>)\n",
            "tensor(12817.0762, grad_fn=<AddBackward0>)\n",
            "tensor(13026.9043, grad_fn=<AddBackward0>)\n",
            "tensor(13537.1836, grad_fn=<AddBackward0>)\n",
            "tensor(13290.2393, grad_fn=<AddBackward0>)\n",
            "tensor(12257.6875, grad_fn=<AddBackward0>)\n",
            "tensor(12694.1641, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [240/469], Reconst Loss: 73.7742, KL Div: 25.3989\n",
            "tensor(13141.6826, grad_fn=<AddBackward0>)\n",
            "tensor(13150.5801, grad_fn=<AddBackward0>)\n",
            "tensor(13027.1182, grad_fn=<AddBackward0>)\n",
            "tensor(13184.8613, grad_fn=<AddBackward0>)\n",
            "tensor(13152.4102, grad_fn=<AddBackward0>)\n",
            "tensor(12788.8027, grad_fn=<AddBackward0>)\n",
            "tensor(13043.7441, grad_fn=<AddBackward0>)\n",
            "tensor(13243.0156, grad_fn=<AddBackward0>)\n",
            "tensor(13027.8086, grad_fn=<AddBackward0>)\n",
            "tensor(13536.4434, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [250/469], Reconst Loss: 79.0218, KL Div: 26.7317\n",
            "tensor(12986.0342, grad_fn=<AddBackward0>)\n",
            "tensor(13023.6035, grad_fn=<AddBackward0>)\n",
            "tensor(13015.5801, grad_fn=<AddBackward0>)\n",
            "tensor(12922.6807, grad_fn=<AddBackward0>)\n",
            "tensor(12482.6279, grad_fn=<AddBackward0>)\n",
            "tensor(13334.6934, grad_fn=<AddBackward0>)\n",
            "tensor(13061.5527, grad_fn=<AddBackward0>)\n",
            "tensor(13095.4316, grad_fn=<AddBackward0>)\n",
            "tensor(13458.9238, grad_fn=<AddBackward0>)\n",
            "tensor(13157.1914, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [260/469], Reconst Loss: 78.0876, KL Div: 24.7029\n",
            "tensor(12729.9434, grad_fn=<AddBackward0>)\n",
            "tensor(12912.9258, grad_fn=<AddBackward0>)\n",
            "tensor(13490.0430, grad_fn=<AddBackward0>)\n",
            "tensor(12688.0098, grad_fn=<AddBackward0>)\n",
            "tensor(13181.6348, grad_fn=<AddBackward0>)\n",
            "tensor(13115.5312, grad_fn=<AddBackward0>)\n",
            "tensor(13052.0273, grad_fn=<AddBackward0>)\n",
            "tensor(12999.9844, grad_fn=<AddBackward0>)\n",
            "tensor(13576.9707, grad_fn=<AddBackward0>)\n",
            "tensor(13058.2754, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [270/469], Reconst Loss: 76.5824, KL Div: 25.4354\n",
            "tensor(13457.0781, grad_fn=<AddBackward0>)\n",
            "tensor(13849.7090, grad_fn=<AddBackward0>)\n",
            "tensor(13044.9707, grad_fn=<AddBackward0>)\n",
            "tensor(13184.8125, grad_fn=<AddBackward0>)\n",
            "tensor(12806.5605, grad_fn=<AddBackward0>)\n",
            "tensor(13272.8594, grad_fn=<AddBackward0>)\n",
            "tensor(12887.8076, grad_fn=<AddBackward0>)\n",
            "tensor(13245.2617, grad_fn=<AddBackward0>)\n",
            "tensor(13453.6631, grad_fn=<AddBackward0>)\n",
            "tensor(13306.8770, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [280/469], Reconst Loss: 78.0774, KL Div: 25.8825\n",
            "tensor(13377.2871, grad_fn=<AddBackward0>)\n",
            "tensor(12818.0723, grad_fn=<AddBackward0>)\n",
            "tensor(13654.5078, grad_fn=<AddBackward0>)\n",
            "tensor(13591.3672, grad_fn=<AddBackward0>)\n",
            "tensor(13062.3770, grad_fn=<AddBackward0>)\n",
            "tensor(12994.8857, grad_fn=<AddBackward0>)\n",
            "tensor(13077.7305, grad_fn=<AddBackward0>)\n",
            "tensor(13228.3379, grad_fn=<AddBackward0>)\n",
            "tensor(12695.1162, grad_fn=<AddBackward0>)\n",
            "tensor(13176.1758, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [290/469], Reconst Loss: 77.5436, KL Div: 25.3952\n",
            "tensor(12811.3047, grad_fn=<AddBackward0>)\n",
            "tensor(13278.6729, grad_fn=<AddBackward0>)\n",
            "tensor(13317.2871, grad_fn=<AddBackward0>)\n",
            "tensor(13777.0098, grad_fn=<AddBackward0>)\n",
            "tensor(13075., grad_fn=<AddBackward0>)\n",
            "tensor(13411.4961, grad_fn=<AddBackward0>)\n",
            "tensor(13494.5215, grad_fn=<AddBackward0>)\n",
            "tensor(12946.1396, grad_fn=<AddBackward0>)\n",
            "tensor(13489.2822, grad_fn=<AddBackward0>)\n",
            "tensor(12649.0732, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [300/469], Reconst Loss: 73.9260, KL Div: 24.8948\n",
            "tensor(13507.2910, grad_fn=<AddBackward0>)\n",
            "tensor(13300.5410, grad_fn=<AddBackward0>)\n",
            "tensor(13069.6035, grad_fn=<AddBackward0>)\n",
            "tensor(13290.7451, grad_fn=<AddBackward0>)\n",
            "tensor(13235.0928, grad_fn=<AddBackward0>)\n",
            "tensor(13419.7178, grad_fn=<AddBackward0>)\n",
            "tensor(12998.1602, grad_fn=<AddBackward0>)\n",
            "tensor(13195.7568, grad_fn=<AddBackward0>)\n",
            "tensor(13138.1270, grad_fn=<AddBackward0>)\n",
            "tensor(13267.4980, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [310/469], Reconst Loss: 78.4431, KL Div: 25.2092\n",
            "tensor(13300.4785, grad_fn=<AddBackward0>)\n",
            "tensor(13457.8438, grad_fn=<AddBackward0>)\n",
            "tensor(12630.6406, grad_fn=<AddBackward0>)\n",
            "tensor(13686.6328, grad_fn=<AddBackward0>)\n",
            "tensor(13356.7236, grad_fn=<AddBackward0>)\n",
            "tensor(12779.0479, grad_fn=<AddBackward0>)\n",
            "tensor(13008.0781, grad_fn=<AddBackward0>)\n",
            "tensor(13546.7217, grad_fn=<AddBackward0>)\n",
            "tensor(12936.4824, grad_fn=<AddBackward0>)\n",
            "tensor(12966.1289, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [320/469], Reconst Loss: 75.9879, KL Div: 25.3100\n",
            "tensor(13642.3652, grad_fn=<AddBackward0>)\n",
            "tensor(13075.6152, grad_fn=<AddBackward0>)\n",
            "tensor(13092.2246, grad_fn=<AddBackward0>)\n",
            "tensor(13265.0527, grad_fn=<AddBackward0>)\n",
            "tensor(12988.2021, grad_fn=<AddBackward0>)\n",
            "tensor(13111.7715, grad_fn=<AddBackward0>)\n",
            "tensor(12475.9688, grad_fn=<AddBackward0>)\n",
            "tensor(13115.6885, grad_fn=<AddBackward0>)\n",
            "tensor(13342.5830, grad_fn=<AddBackward0>)\n",
            "tensor(13076.4004, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [330/469], Reconst Loss: 77.0501, KL Div: 25.1093\n",
            "tensor(13427.0537, grad_fn=<AddBackward0>)\n",
            "tensor(13000.7939, grad_fn=<AddBackward0>)\n",
            "tensor(13173.5898, grad_fn=<AddBackward0>)\n",
            "tensor(12713.6211, grad_fn=<AddBackward0>)\n",
            "tensor(12789.8789, grad_fn=<AddBackward0>)\n",
            "tensor(13400.4160, grad_fn=<AddBackward0>)\n",
            "tensor(13043.0078, grad_fn=<AddBackward0>)\n",
            "tensor(13225.2900, grad_fn=<AddBackward0>)\n",
            "tensor(12706.3955, grad_fn=<AddBackward0>)\n",
            "tensor(13321.1709, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [340/469], Reconst Loss: 78.8604, KL Div: 25.2113\n",
            "tensor(13305.2295, grad_fn=<AddBackward0>)\n",
            "tensor(12750.5752, grad_fn=<AddBackward0>)\n",
            "tensor(13697.4355, grad_fn=<AddBackward0>)\n",
            "tensor(13551.5977, grad_fn=<AddBackward0>)\n",
            "tensor(13184.4492, grad_fn=<AddBackward0>)\n",
            "tensor(13233.2754, grad_fn=<AddBackward0>)\n",
            "tensor(13186.6475, grad_fn=<AddBackward0>)\n",
            "tensor(13592.2969, grad_fn=<AddBackward0>)\n",
            "tensor(13314.5654, grad_fn=<AddBackward0>)\n",
            "tensor(13069.0713, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [350/469], Reconst Loss: 76.8358, KL Div: 25.2663\n",
            "tensor(12782.1172, grad_fn=<AddBackward0>)\n",
            "tensor(13156.3330, grad_fn=<AddBackward0>)\n",
            "tensor(13197.1904, grad_fn=<AddBackward0>)\n",
            "tensor(13110.5928, grad_fn=<AddBackward0>)\n",
            "tensor(13425.4551, grad_fn=<AddBackward0>)\n",
            "tensor(13076.9180, grad_fn=<AddBackward0>)\n",
            "tensor(12869.4512, grad_fn=<AddBackward0>)\n",
            "tensor(12846.8027, grad_fn=<AddBackward0>)\n",
            "tensor(13503.9795, grad_fn=<AddBackward0>)\n",
            "tensor(13195.7891, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [360/469], Reconst Loss: 77.5360, KL Div: 25.5561\n",
            "tensor(13270.1992, grad_fn=<AddBackward0>)\n",
            "tensor(13436.7871, grad_fn=<AddBackward0>)\n",
            "tensor(12973.0859, grad_fn=<AddBackward0>)\n",
            "tensor(13660.8252, grad_fn=<AddBackward0>)\n",
            "tensor(13552.8076, grad_fn=<AddBackward0>)\n",
            "tensor(13564.1777, grad_fn=<AddBackward0>)\n",
            "tensor(13369.3047, grad_fn=<AddBackward0>)\n",
            "tensor(13367.7168, grad_fn=<AddBackward0>)\n",
            "tensor(12698.7422, grad_fn=<AddBackward0>)\n",
            "tensor(13409.5439, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [370/469], Reconst Loss: 79.6414, KL Div: 25.1206\n",
            "tensor(13255.1094, grad_fn=<AddBackward0>)\n",
            "tensor(13178.9502, grad_fn=<AddBackward0>)\n",
            "tensor(12709.0234, grad_fn=<AddBackward0>)\n",
            "tensor(13243.8018, grad_fn=<AddBackward0>)\n",
            "tensor(13610.8301, grad_fn=<AddBackward0>)\n",
            "tensor(13298.1172, grad_fn=<AddBackward0>)\n",
            "tensor(13100.2861, grad_fn=<AddBackward0>)\n",
            "tensor(13240.8955, grad_fn=<AddBackward0>)\n",
            "tensor(13433.8145, grad_fn=<AddBackward0>)\n",
            "tensor(13124.5566, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [380/469], Reconst Loss: 77.4654, KL Div: 25.0702\n",
            "tensor(12942.1709, grad_fn=<AddBackward0>)\n",
            "tensor(12643.5586, grad_fn=<AddBackward0>)\n",
            "tensor(12678.8213, grad_fn=<AddBackward0>)\n",
            "tensor(13348.5596, grad_fn=<AddBackward0>)\n",
            "tensor(13043.1113, grad_fn=<AddBackward0>)\n",
            "tensor(12700.1270, grad_fn=<AddBackward0>)\n",
            "tensor(13432.0391, grad_fn=<AddBackward0>)\n",
            "tensor(12866.7627, grad_fn=<AddBackward0>)\n",
            "tensor(12401.7441, grad_fn=<AddBackward0>)\n",
            "tensor(13337.0273, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [390/469], Reconst Loss: 78.3511, KL Div: 25.8445\n",
            "tensor(12727.1396, grad_fn=<AddBackward0>)\n",
            "tensor(13640.2432, grad_fn=<AddBackward0>)\n",
            "tensor(12988.2305, grad_fn=<AddBackward0>)\n",
            "tensor(13053.2090, grad_fn=<AddBackward0>)\n",
            "tensor(12994.8965, grad_fn=<AddBackward0>)\n",
            "tensor(13452.2676, grad_fn=<AddBackward0>)\n",
            "tensor(13200.0840, grad_fn=<AddBackward0>)\n",
            "tensor(13229.6816, grad_fn=<AddBackward0>)\n",
            "tensor(13794.4297, grad_fn=<AddBackward0>)\n",
            "tensor(13000.2637, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [400/469], Reconst Loss: 76.7897, KL Div: 24.7748\n",
            "tensor(13957.4873, grad_fn=<AddBackward0>)\n",
            "tensor(13366.7109, grad_fn=<AddBackward0>)\n",
            "tensor(13581.8105, grad_fn=<AddBackward0>)\n",
            "tensor(13073.3955, grad_fn=<AddBackward0>)\n",
            "tensor(12843.1680, grad_fn=<AddBackward0>)\n",
            "tensor(13340.6543, grad_fn=<AddBackward0>)\n",
            "tensor(13225.4648, grad_fn=<AddBackward0>)\n",
            "tensor(13322.8945, grad_fn=<AddBackward0>)\n",
            "tensor(12570.6455, grad_fn=<AddBackward0>)\n",
            "tensor(13498.3711, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [410/469], Reconst Loss: 80.2400, KL Div: 25.2161\n",
            "tensor(13427.0820, grad_fn=<AddBackward0>)\n",
            "tensor(13549.4756, grad_fn=<AddBackward0>)\n",
            "tensor(13138.3418, grad_fn=<AddBackward0>)\n",
            "tensor(13938.5605, grad_fn=<AddBackward0>)\n",
            "tensor(13801.9033, grad_fn=<AddBackward0>)\n",
            "tensor(13003.1660, grad_fn=<AddBackward0>)\n",
            "tensor(13515.6279, grad_fn=<AddBackward0>)\n",
            "tensor(13092.8301, grad_fn=<AddBackward0>)\n",
            "tensor(13150.9873, grad_fn=<AddBackward0>)\n",
            "tensor(12818.8027, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [420/469], Reconst Loss: 74.6150, KL Div: 25.5319\n",
            "tensor(13283.3506, grad_fn=<AddBackward0>)\n",
            "tensor(13275.2852, grad_fn=<AddBackward0>)\n",
            "tensor(13171.9541, grad_fn=<AddBackward0>)\n",
            "tensor(12662.7393, grad_fn=<AddBackward0>)\n",
            "tensor(13104.2363, grad_fn=<AddBackward0>)\n",
            "tensor(13242.5293, grad_fn=<AddBackward0>)\n",
            "tensor(13063.3848, grad_fn=<AddBackward0>)\n",
            "tensor(13686.3320, grad_fn=<AddBackward0>)\n",
            "tensor(13099.8457, grad_fn=<AddBackward0>)\n",
            "tensor(13666.3047, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [430/469], Reconst Loss: 81.0518, KL Div: 25.7162\n",
            "tensor(13471.5312, grad_fn=<AddBackward0>)\n",
            "tensor(13201.9902, grad_fn=<AddBackward0>)\n",
            "tensor(12886.4805, grad_fn=<AddBackward0>)\n",
            "tensor(13558.3789, grad_fn=<AddBackward0>)\n",
            "tensor(13386.6787, grad_fn=<AddBackward0>)\n",
            "tensor(13511.9375, grad_fn=<AddBackward0>)\n",
            "tensor(13325.5449, grad_fn=<AddBackward0>)\n",
            "tensor(12942.0137, grad_fn=<AddBackward0>)\n",
            "tensor(13191.4697, grad_fn=<AddBackward0>)\n",
            "tensor(12985.5430, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [440/469], Reconst Loss: 75.9306, KL Div: 25.5189\n",
            "tensor(12712.6240, grad_fn=<AddBackward0>)\n",
            "tensor(12830.1367, grad_fn=<AddBackward0>)\n",
            "tensor(12949.8564, grad_fn=<AddBackward0>)\n",
            "tensor(13602.9551, grad_fn=<AddBackward0>)\n",
            "tensor(13165.5059, grad_fn=<AddBackward0>)\n",
            "tensor(13366.2891, grad_fn=<AddBackward0>)\n",
            "tensor(12888.2324, grad_fn=<AddBackward0>)\n",
            "tensor(13312.2939, grad_fn=<AddBackward0>)\n",
            "tensor(12822.5371, grad_fn=<AddBackward0>)\n",
            "tensor(13243.6641, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [450/469], Reconst Loss: 77.9978, KL Div: 25.4684\n",
            "tensor(13135.1895, grad_fn=<AddBackward0>)\n",
            "tensor(12888.0215, grad_fn=<AddBackward0>)\n",
            "tensor(12902.4980, grad_fn=<AddBackward0>)\n",
            "tensor(13109.8418, grad_fn=<AddBackward0>)\n",
            "tensor(13443.0449, grad_fn=<AddBackward0>)\n",
            "tensor(13614.6865, grad_fn=<AddBackward0>)\n",
            "tensor(13460.9746, grad_fn=<AddBackward0>)\n",
            "tensor(12978.9922, grad_fn=<AddBackward0>)\n",
            "tensor(13061.8398, grad_fn=<AddBackward0>)\n",
            "tensor(12950.3975, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [460/469], Reconst Loss: 75.3159, KL Div: 25.8591\n",
            "tensor(13285.3057, grad_fn=<AddBackward0>)\n",
            "tensor(13363.7842, grad_fn=<AddBackward0>)\n",
            "tensor(12838.8027, grad_fn=<AddBackward0>)\n",
            "tensor(12559.0615, grad_fn=<AddBackward0>)\n",
            "tensor(12962.1533, grad_fn=<AddBackward0>)\n",
            "tensor(13712.0791, grad_fn=<AddBackward0>)\n",
            "tensor(13107.4414, grad_fn=<AddBackward0>)\n",
            "tensor(13187.5225, grad_fn=<AddBackward0>)\n",
            "tensor(9842.2891, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normal VAE Outputs"
      ],
      "metadata": {
        "id": "UcHR9C1x3sky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Reconstruction Using Normal VAE"
      ],
      "metadata": {
        "id": "vrFEByZ23xKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_reconstruction(model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "P3dcY-tOHcfA",
        "outputId": "e37e7732-1977-43dc-ea95-10ea07d45d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Generation using Normal VAE"
      ],
      "metadata": {
        "id": "pnrC1a7x33f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_generation(model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "tKjxLJF3HoV6",
        "outputId": "88ba3a1d-256c-43e9-cc50-cd73db3970f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Beta VAE Training where Beta = 15"
      ],
      "metadata": {
        "id": "rgIbCLTf3_5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_loss_beta_vae = []\n",
        "iteration_loss_beta_vae = []\n",
        "for epoch in range(25):\n",
        "    epoch_loss = []\n",
        "    for i, (x, _) in enumerate(data_loader):\n",
        "        # Forward pass\n",
        "        x = x.to(device).view(-1, image_size)\n",
        "        x_reconst, mu, log_var = model3(x)\n",
        "        \n",
        "        # Compute reconstruction loss and kl divergence\n",
        "        # For KL divergence between Gaussians, see Appendix B in VAE paper or (Doersch, 2016):\n",
        "        # https://arxiv.org/abs/1606.05908\n",
        "        reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        loss = reconst_loss + 15 * kl_div\n",
        "        print(loss)\n",
        "        optimizer3.zero_grad()\n",
        "        loss.backward(retain_graph=False)\n",
        "        optimizer3.step()\n",
        "        \n",
        "        if (i+1) % 10 == 0:\n",
        "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
        "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size, kl_div.item()/batch_size))\n",
        "            \n",
        "        iteration_loss_beta_vae.append(loss)\n",
        "    torch.cuda.empty_cache()\n",
        "    epoch_loss_beta_vae.append(sum(iteration_loss_beta_vae)/len(iteration_loss_beta_vae))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRaIx6feIBGs",
        "outputId": "209862d4-fa90-4539-d3a2-8ff1eb6340fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "tensor(25590.4160, grad_fn=<AddBackward0>)\n",
            "tensor(24558.8184, grad_fn=<AddBackward0>)\n",
            "tensor(25673.3984, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [140/469], Reconst Loss: 173.5879, KL Div: 1.7990\n",
            "tensor(25233.5098, grad_fn=<AddBackward0>)\n",
            "tensor(24240.6953, grad_fn=<AddBackward0>)\n",
            "tensor(24908.2988, grad_fn=<AddBackward0>)\n",
            "tensor(25066.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25805.7344, grad_fn=<AddBackward0>)\n",
            "tensor(24737.2852, grad_fn=<AddBackward0>)\n",
            "tensor(25144.1270, grad_fn=<AddBackward0>)\n",
            "tensor(25369.2734, grad_fn=<AddBackward0>)\n",
            "tensor(25701.2930, grad_fn=<AddBackward0>)\n",
            "tensor(25856.5898, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [150/469], Reconst Loss: 173.9104, KL Div: 1.8729\n",
            "tensor(25308.3750, grad_fn=<AddBackward0>)\n",
            "tensor(25587.0273, grad_fn=<AddBackward0>)\n",
            "tensor(25174.4121, grad_fn=<AddBackward0>)\n",
            "tensor(25352.2910, grad_fn=<AddBackward0>)\n",
            "tensor(24232.0547, grad_fn=<AddBackward0>)\n",
            "tensor(25631.7168, grad_fn=<AddBackward0>)\n",
            "tensor(25417.7305, grad_fn=<AddBackward0>)\n",
            "tensor(25623.5195, grad_fn=<AddBackward0>)\n",
            "tensor(24335.1816, grad_fn=<AddBackward0>)\n",
            "tensor(26478.7441, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [160/469], Reconst Loss: 179.6143, KL Div: 1.8167\n",
            "tensor(24143.6016, grad_fn=<AddBackward0>)\n",
            "tensor(25317.1289, grad_fn=<AddBackward0>)\n",
            "tensor(25326.0234, grad_fn=<AddBackward0>)\n",
            "tensor(25760.9102, grad_fn=<AddBackward0>)\n",
            "tensor(24378.7383, grad_fn=<AddBackward0>)\n",
            "tensor(24771.0430, grad_fn=<AddBackward0>)\n",
            "tensor(25172.7949, grad_fn=<AddBackward0>)\n",
            "tensor(24344.9590, grad_fn=<AddBackward0>)\n",
            "tensor(25839.1895, grad_fn=<AddBackward0>)\n",
            "tensor(25586.7812, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [170/469], Reconst Loss: 172.7768, KL Div: 1.8080\n",
            "tensor(25566.8438, grad_fn=<AddBackward0>)\n",
            "tensor(25454.8438, grad_fn=<AddBackward0>)\n",
            "tensor(25258.7930, grad_fn=<AddBackward0>)\n",
            "tensor(25625.4805, grad_fn=<AddBackward0>)\n",
            "tensor(25487.6094, grad_fn=<AddBackward0>)\n",
            "tensor(25556.0605, grad_fn=<AddBackward0>)\n",
            "tensor(25190.4453, grad_fn=<AddBackward0>)\n",
            "tensor(25596.8359, grad_fn=<AddBackward0>)\n",
            "tensor(25190.3008, grad_fn=<AddBackward0>)\n",
            "tensor(24787.9023, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [180/469], Reconst Loss: 167.2566, KL Div: 1.7599\n",
            "tensor(24508.0938, grad_fn=<AddBackward0>)\n",
            "tensor(25872.0352, grad_fn=<AddBackward0>)\n",
            "tensor(24623.0605, grad_fn=<AddBackward0>)\n",
            "tensor(24860.8262, grad_fn=<AddBackward0>)\n",
            "tensor(24803.3438, grad_fn=<AddBackward0>)\n",
            "tensor(25925.6465, grad_fn=<AddBackward0>)\n",
            "tensor(25466.3008, grad_fn=<AddBackward0>)\n",
            "tensor(25997.0762, grad_fn=<AddBackward0>)\n",
            "tensor(25443.6582, grad_fn=<AddBackward0>)\n",
            "tensor(25030.0684, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [190/469], Reconst Loss: 169.0036, KL Div: 1.7696\n",
            "tensor(25013.1387, grad_fn=<AddBackward0>)\n",
            "tensor(25057.0918, grad_fn=<AddBackward0>)\n",
            "tensor(25194.5547, grad_fn=<AddBackward0>)\n",
            "tensor(25809.7168, grad_fn=<AddBackward0>)\n",
            "tensor(25097.6055, grad_fn=<AddBackward0>)\n",
            "tensor(25139.8906, grad_fn=<AddBackward0>)\n",
            "tensor(25298.0176, grad_fn=<AddBackward0>)\n",
            "tensor(25383.9805, grad_fn=<AddBackward0>)\n",
            "tensor(26498.2715, grad_fn=<AddBackward0>)\n",
            "tensor(25009.2988, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [200/469], Reconst Loss: 169.1640, KL Div: 1.7481\n",
            "tensor(24715.7500, grad_fn=<AddBackward0>)\n",
            "tensor(24544.6719, grad_fn=<AddBackward0>)\n",
            "tensor(24826.6387, grad_fn=<AddBackward0>)\n",
            "tensor(25450.4355, grad_fn=<AddBackward0>)\n",
            "tensor(24912.4297, grad_fn=<AddBackward0>)\n",
            "tensor(25970.5957, grad_fn=<AddBackward0>)\n",
            "tensor(25771.4902, grad_fn=<AddBackward0>)\n",
            "tensor(25642.1367, grad_fn=<AddBackward0>)\n",
            "tensor(25994.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25249.8027, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [210/469], Reconst Loss: 171.3923, KL Div: 1.7248\n",
            "tensor(25484.3145, grad_fn=<AddBackward0>)\n",
            "tensor(26923.6309, grad_fn=<AddBackward0>)\n",
            "tensor(25720.7617, grad_fn=<AddBackward0>)\n",
            "tensor(25060.2617, grad_fn=<AddBackward0>)\n",
            "tensor(25165.0117, grad_fn=<AddBackward0>)\n",
            "tensor(25012.5000, grad_fn=<AddBackward0>)\n",
            "tensor(25534.2441, grad_fn=<AddBackward0>)\n",
            "tensor(25132.3027, grad_fn=<AddBackward0>)\n",
            "tensor(25302.8105, grad_fn=<AddBackward0>)\n",
            "tensor(25661.8164, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [220/469], Reconst Loss: 173.9154, KL Div: 1.7712\n",
            "tensor(25178.1797, grad_fn=<AddBackward0>)\n",
            "tensor(25204.7500, grad_fn=<AddBackward0>)\n",
            "tensor(25767.2734, grad_fn=<AddBackward0>)\n",
            "tensor(25162.2676, grad_fn=<AddBackward0>)\n",
            "tensor(25036.6758, grad_fn=<AddBackward0>)\n",
            "tensor(24510.1875, grad_fn=<AddBackward0>)\n",
            "tensor(24999.6816, grad_fn=<AddBackward0>)\n",
            "tensor(26115.4941, grad_fn=<AddBackward0>)\n",
            "tensor(25347.5957, grad_fn=<AddBackward0>)\n",
            "tensor(25901.5195, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [230/469], Reconst Loss: 175.5730, KL Div: 1.7855\n",
            "tensor(25872.7461, grad_fn=<AddBackward0>)\n",
            "tensor(24511.5957, grad_fn=<AddBackward0>)\n",
            "tensor(25841.6484, grad_fn=<AddBackward0>)\n",
            "tensor(25358.3105, grad_fn=<AddBackward0>)\n",
            "tensor(24473.2305, grad_fn=<AddBackward0>)\n",
            "tensor(25087.1758, grad_fn=<AddBackward0>)\n",
            "tensor(25456.9727, grad_fn=<AddBackward0>)\n",
            "tensor(25204.3008, grad_fn=<AddBackward0>)\n",
            "tensor(26099.0645, grad_fn=<AddBackward0>)\n",
            "tensor(25360.7852, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [240/469], Reconst Loss: 173.2914, KL Div: 1.6560\n",
            "tensor(25949.2012, grad_fn=<AddBackward0>)\n",
            "tensor(24578.5059, grad_fn=<AddBackward0>)\n",
            "tensor(25239.0352, grad_fn=<AddBackward0>)\n",
            "tensor(24228.5664, grad_fn=<AddBackward0>)\n",
            "tensor(25204.5645, grad_fn=<AddBackward0>)\n",
            "tensor(25518.9473, grad_fn=<AddBackward0>)\n",
            "tensor(25959.6719, grad_fn=<AddBackward0>)\n",
            "tensor(24158.2227, grad_fn=<AddBackward0>)\n",
            "tensor(25144.8711, grad_fn=<AddBackward0>)\n",
            "tensor(25501.8340, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [250/469], Reconst Loss: 171.6681, KL Div: 1.8377\n",
            "tensor(24946.7695, grad_fn=<AddBackward0>)\n",
            "tensor(25674.8145, grad_fn=<AddBackward0>)\n",
            "tensor(25537.2188, grad_fn=<AddBackward0>)\n",
            "tensor(25143.8887, grad_fn=<AddBackward0>)\n",
            "tensor(24588.6016, grad_fn=<AddBackward0>)\n",
            "tensor(25109.1992, grad_fn=<AddBackward0>)\n",
            "tensor(24909.9043, grad_fn=<AddBackward0>)\n",
            "tensor(25673.0020, grad_fn=<AddBackward0>)\n",
            "tensor(25190.9844, grad_fn=<AddBackward0>)\n",
            "tensor(24402.3809, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [260/469], Reconst Loss: 164.0806, KL Div: 1.7709\n",
            "tensor(24270.1816, grad_fn=<AddBackward0>)\n",
            "tensor(24483.6562, grad_fn=<AddBackward0>)\n",
            "tensor(26285.7500, grad_fn=<AddBackward0>)\n",
            "tensor(25903.7676, grad_fn=<AddBackward0>)\n",
            "tensor(25528.8359, grad_fn=<AddBackward0>)\n",
            "tensor(25042.6836, grad_fn=<AddBackward0>)\n",
            "tensor(26966.3047, grad_fn=<AddBackward0>)\n",
            "tensor(24400.3516, grad_fn=<AddBackward0>)\n",
            "tensor(24845.2227, grad_fn=<AddBackward0>)\n",
            "tensor(24933.3105, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [270/469], Reconst Loss: 167.8046, KL Div: 1.7991\n",
            "tensor(25747.9141, grad_fn=<AddBackward0>)\n",
            "tensor(25129.5449, grad_fn=<AddBackward0>)\n",
            "tensor(25422.0664, grad_fn=<AddBackward0>)\n",
            "tensor(25828.4844, grad_fn=<AddBackward0>)\n",
            "tensor(25308.4531, grad_fn=<AddBackward0>)\n",
            "tensor(25669.5547, grad_fn=<AddBackward0>)\n",
            "tensor(24762.1348, grad_fn=<AddBackward0>)\n",
            "tensor(25318.7656, grad_fn=<AddBackward0>)\n",
            "tensor(25764.0273, grad_fn=<AddBackward0>)\n",
            "tensor(25532.8867, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [280/469], Reconst Loss: 172.7858, KL Div: 1.7793\n",
            "tensor(25124.0078, grad_fn=<AddBackward0>)\n",
            "tensor(24791.4395, grad_fn=<AddBackward0>)\n",
            "tensor(25961.2676, grad_fn=<AddBackward0>)\n",
            "tensor(25318.5391, grad_fn=<AddBackward0>)\n",
            "tensor(25282.3125, grad_fn=<AddBackward0>)\n",
            "tensor(24886.2402, grad_fn=<AddBackward0>)\n",
            "tensor(24986.3633, grad_fn=<AddBackward0>)\n",
            "tensor(25710.7227, grad_fn=<AddBackward0>)\n",
            "tensor(25660.2715, grad_fn=<AddBackward0>)\n",
            "tensor(25629.8340, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [290/469], Reconst Loss: 174.3799, KL Div: 1.7235\n",
            "tensor(25418.3359, grad_fn=<AddBackward0>)\n",
            "tensor(25662.4941, grad_fn=<AddBackward0>)\n",
            "tensor(25885.3398, grad_fn=<AddBackward0>)\n",
            "tensor(25231.6523, grad_fn=<AddBackward0>)\n",
            "tensor(24638.0957, grad_fn=<AddBackward0>)\n",
            "tensor(25349.4375, grad_fn=<AddBackward0>)\n",
            "tensor(24992.2227, grad_fn=<AddBackward0>)\n",
            "tensor(25333.6758, grad_fn=<AddBackward0>)\n",
            "tensor(25192.3457, grad_fn=<AddBackward0>)\n",
            "tensor(25421.6934, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [300/469], Reconst Loss: 172.9484, KL Div: 1.7106\n",
            "tensor(24830.0957, grad_fn=<AddBackward0>)\n",
            "tensor(25026.0879, grad_fn=<AddBackward0>)\n",
            "tensor(25969.3066, grad_fn=<AddBackward0>)\n",
            "tensor(24538.6152, grad_fn=<AddBackward0>)\n",
            "tensor(25046.7617, grad_fn=<AddBackward0>)\n",
            "tensor(25497.1113, grad_fn=<AddBackward0>)\n",
            "tensor(24888.3008, grad_fn=<AddBackward0>)\n",
            "tensor(25832.0723, grad_fn=<AddBackward0>)\n",
            "tensor(24884.3750, grad_fn=<AddBackward0>)\n",
            "tensor(25932.2539, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [310/469], Reconst Loss: 175.6303, KL Div: 1.7977\n",
            "tensor(25601.2812, grad_fn=<AddBackward0>)\n",
            "tensor(25750.9395, grad_fn=<AddBackward0>)\n",
            "tensor(25768.4551, grad_fn=<AddBackward0>)\n",
            "tensor(25006.7383, grad_fn=<AddBackward0>)\n",
            "tensor(25299.6152, grad_fn=<AddBackward0>)\n",
            "tensor(25823.7266, grad_fn=<AddBackward0>)\n",
            "tensor(24890.1836, grad_fn=<AddBackward0>)\n",
            "tensor(25022.6191, grad_fn=<AddBackward0>)\n",
            "tensor(24800.0879, grad_fn=<AddBackward0>)\n",
            "tensor(25018.6152, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [320/469], Reconst Loss: 167.6984, KL Div: 1.8506\n",
            "tensor(25977.9258, grad_fn=<AddBackward0>)\n",
            "tensor(24427.0391, grad_fn=<AddBackward0>)\n",
            "tensor(25944.8984, grad_fn=<AddBackward0>)\n",
            "tensor(25880.8301, grad_fn=<AddBackward0>)\n",
            "tensor(25687.5020, grad_fn=<AddBackward0>)\n",
            "tensor(25976.4609, grad_fn=<AddBackward0>)\n",
            "tensor(25253.5664, grad_fn=<AddBackward0>)\n",
            "tensor(25530.9199, grad_fn=<AddBackward0>)\n",
            "tensor(24775.3008, grad_fn=<AddBackward0>)\n",
            "tensor(24995.0410, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [330/469], Reconst Loss: 168.1128, KL Div: 1.8107\n",
            "tensor(25961.1230, grad_fn=<AddBackward0>)\n",
            "tensor(26000.3301, grad_fn=<AddBackward0>)\n",
            "tensor(24968.2188, grad_fn=<AddBackward0>)\n",
            "tensor(24882.7793, grad_fn=<AddBackward0>)\n",
            "tensor(25338.3281, grad_fn=<AddBackward0>)\n",
            "tensor(25430.9785, grad_fn=<AddBackward0>)\n",
            "tensor(25294.9746, grad_fn=<AddBackward0>)\n",
            "tensor(25208.1562, grad_fn=<AddBackward0>)\n",
            "tensor(26155.3750, grad_fn=<AddBackward0>)\n",
            "tensor(23914.0352, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [340/469], Reconst Loss: 161.9233, KL Div: 1.6603\n",
            "tensor(25744.7480, grad_fn=<AddBackward0>)\n",
            "tensor(25261.2852, grad_fn=<AddBackward0>)\n",
            "tensor(24550.0410, grad_fn=<AddBackward0>)\n",
            "tensor(25185.3398, grad_fn=<AddBackward0>)\n",
            "tensor(25008.5273, grad_fn=<AddBackward0>)\n",
            "tensor(25458.9961, grad_fn=<AddBackward0>)\n",
            "tensor(25468.6875, grad_fn=<AddBackward0>)\n",
            "tensor(25654.9512, grad_fn=<AddBackward0>)\n",
            "tensor(25146.3730, grad_fn=<AddBackward0>)\n",
            "tensor(24808.5820, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [350/469], Reconst Loss: 165.5445, KL Div: 1.8848\n",
            "tensor(25344.3223, grad_fn=<AddBackward0>)\n",
            "tensor(24558.7422, grad_fn=<AddBackward0>)\n",
            "tensor(25932.4102, grad_fn=<AddBackward0>)\n",
            "tensor(25842.8086, grad_fn=<AddBackward0>)\n",
            "tensor(24718.0352, grad_fn=<AddBackward0>)\n",
            "tensor(25998.5195, grad_fn=<AddBackward0>)\n",
            "tensor(24856.2266, grad_fn=<AddBackward0>)\n",
            "tensor(24858.2656, grad_fn=<AddBackward0>)\n",
            "tensor(25523.6133, grad_fn=<AddBackward0>)\n",
            "tensor(25329.9414, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [360/469], Reconst Loss: 170.6627, KL Div: 1.8152\n",
            "tensor(25143.9707, grad_fn=<AddBackward0>)\n",
            "tensor(25329.6074, grad_fn=<AddBackward0>)\n",
            "tensor(24973.1230, grad_fn=<AddBackward0>)\n",
            "tensor(25525.3438, grad_fn=<AddBackward0>)\n",
            "tensor(25563.5781, grad_fn=<AddBackward0>)\n",
            "tensor(25363.4941, grad_fn=<AddBackward0>)\n",
            "tensor(24496.3652, grad_fn=<AddBackward0>)\n",
            "tensor(24887.5723, grad_fn=<AddBackward0>)\n",
            "tensor(25041.5078, grad_fn=<AddBackward0>)\n",
            "tensor(25234.6543, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [370/469], Reconst Loss: 167.1068, KL Div: 2.0026\n",
            "tensor(25136.6289, grad_fn=<AddBackward0>)\n",
            "tensor(25471.6602, grad_fn=<AddBackward0>)\n",
            "tensor(24977.7578, grad_fn=<AddBackward0>)\n",
            "tensor(25444.4727, grad_fn=<AddBackward0>)\n",
            "tensor(25261.1914, grad_fn=<AddBackward0>)\n",
            "tensor(24933.1270, grad_fn=<AddBackward0>)\n",
            "tensor(24957.6426, grad_fn=<AddBackward0>)\n",
            "tensor(25732.6270, grad_fn=<AddBackward0>)\n",
            "tensor(25197.1465, grad_fn=<AddBackward0>)\n",
            "tensor(26432.4883, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [380/469], Reconst Loss: 179.3911, KL Div: 1.8075\n",
            "tensor(25726.5469, grad_fn=<AddBackward0>)\n",
            "tensor(25157.8164, grad_fn=<AddBackward0>)\n",
            "tensor(25047.2891, grad_fn=<AddBackward0>)\n",
            "tensor(25412.6914, grad_fn=<AddBackward0>)\n",
            "tensor(24914.9844, grad_fn=<AddBackward0>)\n",
            "tensor(25438.7168, grad_fn=<AddBackward0>)\n",
            "tensor(25902.5977, grad_fn=<AddBackward0>)\n",
            "tensor(24659.7598, grad_fn=<AddBackward0>)\n",
            "tensor(26147.7520, grad_fn=<AddBackward0>)\n",
            "tensor(25420.4297, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [390/469], Reconst Loss: 172.4439, KL Div: 1.7435\n",
            "tensor(26231.6836, grad_fn=<AddBackward0>)\n",
            "tensor(24641.9902, grad_fn=<AddBackward0>)\n",
            "tensor(26056.5703, grad_fn=<AddBackward0>)\n",
            "tensor(25088.5898, grad_fn=<AddBackward0>)\n",
            "tensor(24679.0332, grad_fn=<AddBackward0>)\n",
            "tensor(25835.9863, grad_fn=<AddBackward0>)\n",
            "tensor(25268.6816, grad_fn=<AddBackward0>)\n",
            "tensor(26222.4805, grad_fn=<AddBackward0>)\n",
            "tensor(24747.8613, grad_fn=<AddBackward0>)\n",
            "tensor(25381.5547, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [400/469], Reconst Loss: 171.5995, KL Div: 1.7796\n",
            "tensor(25639.0703, grad_fn=<AddBackward0>)\n",
            "tensor(25144.7500, grad_fn=<AddBackward0>)\n",
            "tensor(26231.5586, grad_fn=<AddBackward0>)\n",
            "tensor(24530.0273, grad_fn=<AddBackward0>)\n",
            "tensor(25144.5820, grad_fn=<AddBackward0>)\n",
            "tensor(25000.3008, grad_fn=<AddBackward0>)\n",
            "tensor(25980.6250, grad_fn=<AddBackward0>)\n",
            "tensor(24929.4102, grad_fn=<AddBackward0>)\n",
            "tensor(25669.9258, grad_fn=<AddBackward0>)\n",
            "tensor(25265.6934, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [410/469], Reconst Loss: 168.8458, KL Div: 1.9028\n",
            "tensor(26178.1250, grad_fn=<AddBackward0>)\n",
            "tensor(24512.8281, grad_fn=<AddBackward0>)\n",
            "tensor(25519.9355, grad_fn=<AddBackward0>)\n",
            "tensor(25390.0078, grad_fn=<AddBackward0>)\n",
            "tensor(25651.3672, grad_fn=<AddBackward0>)\n",
            "tensor(24405.0332, grad_fn=<AddBackward0>)\n",
            "tensor(26310.8262, grad_fn=<AddBackward0>)\n",
            "tensor(24834.3320, grad_fn=<AddBackward0>)\n",
            "tensor(25820.1680, grad_fn=<AddBackward0>)\n",
            "tensor(25034.1035, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [420/469], Reconst Loss: 168.3244, KL Div: 1.8170\n",
            "tensor(25112.1992, grad_fn=<AddBackward0>)\n",
            "tensor(25093.9902, grad_fn=<AddBackward0>)\n",
            "tensor(24863.4805, grad_fn=<AddBackward0>)\n",
            "tensor(25524.0391, grad_fn=<AddBackward0>)\n",
            "tensor(25342.8340, grad_fn=<AddBackward0>)\n",
            "tensor(25156.1641, grad_fn=<AddBackward0>)\n",
            "tensor(25164.7793, grad_fn=<AddBackward0>)\n",
            "tensor(24973.0449, grad_fn=<AddBackward0>)\n",
            "tensor(23763.5840, grad_fn=<AddBackward0>)\n",
            "tensor(25476.7812, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [430/469], Reconst Loss: 171.5169, KL Div: 1.8347\n",
            "tensor(24200.1719, grad_fn=<AddBackward0>)\n",
            "tensor(25262.0547, grad_fn=<AddBackward0>)\n",
            "tensor(25220.0586, grad_fn=<AddBackward0>)\n",
            "tensor(24824.6445, grad_fn=<AddBackward0>)\n",
            "tensor(24624.3457, grad_fn=<AddBackward0>)\n",
            "tensor(25124.0391, grad_fn=<AddBackward0>)\n",
            "tensor(24761.0957, grad_fn=<AddBackward0>)\n",
            "tensor(25590.7422, grad_fn=<AddBackward0>)\n",
            "tensor(26176.3945, grad_fn=<AddBackward0>)\n",
            "tensor(25300.7715, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [440/469], Reconst Loss: 170.3155, KL Div: 1.8231\n",
            "tensor(24819.9629, grad_fn=<AddBackward0>)\n",
            "tensor(25329.9375, grad_fn=<AddBackward0>)\n",
            "tensor(24969.4473, grad_fn=<AddBackward0>)\n",
            "tensor(25462.6211, grad_fn=<AddBackward0>)\n",
            "tensor(25457.8711, grad_fn=<AddBackward0>)\n",
            "tensor(25012.1680, grad_fn=<AddBackward0>)\n",
            "tensor(25531.5566, grad_fn=<AddBackward0>)\n",
            "tensor(24870.7402, grad_fn=<AddBackward0>)\n",
            "tensor(25078.8281, grad_fn=<AddBackward0>)\n",
            "tensor(24841.7012, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [450/469], Reconst Loss: 167.6172, KL Div: 1.7639\n",
            "tensor(24873.6016, grad_fn=<AddBackward0>)\n",
            "tensor(25299.3730, grad_fn=<AddBackward0>)\n",
            "tensor(24896.7598, grad_fn=<AddBackward0>)\n",
            "tensor(25768.0195, grad_fn=<AddBackward0>)\n",
            "tensor(25111.6602, grad_fn=<AddBackward0>)\n",
            "tensor(25185.2402, grad_fn=<AddBackward0>)\n",
            "tensor(24901.3203, grad_fn=<AddBackward0>)\n",
            "tensor(25033.1758, grad_fn=<AddBackward0>)\n",
            "tensor(24497.9961, grad_fn=<AddBackward0>)\n",
            "tensor(26027.5293, grad_fn=<AddBackward0>)\n",
            "Epoch[16/25], Step [460/469], Reconst Loss: 175.6275, KL Div: 1.8475\n",
            "tensor(26159.3496, grad_fn=<AddBackward0>)\n",
            "tensor(25034.4414, grad_fn=<AddBackward0>)\n",
            "tensor(25978.5625, grad_fn=<AddBackward0>)\n",
            "tensor(24214.7637, grad_fn=<AddBackward0>)\n",
            "tensor(25233.6445, grad_fn=<AddBackward0>)\n",
            "tensor(25981.7070, grad_fn=<AddBackward0>)\n",
            "tensor(24996.0605, grad_fn=<AddBackward0>)\n",
            "tensor(24764.1504, grad_fn=<AddBackward0>)\n",
            "tensor(18696.7852, grad_fn=<AddBackward0>)\n",
            "tensor(26183.3711, grad_fn=<AddBackward0>)\n",
            "tensor(24694.1113, grad_fn=<AddBackward0>)\n",
            "tensor(25907.1074, grad_fn=<AddBackward0>)\n",
            "tensor(25360.6426, grad_fn=<AddBackward0>)\n",
            "tensor(26054.1602, grad_fn=<AddBackward0>)\n",
            "tensor(24919.0469, grad_fn=<AddBackward0>)\n",
            "tensor(24981.9473, grad_fn=<AddBackward0>)\n",
            "tensor(25359.3906, grad_fn=<AddBackward0>)\n",
            "tensor(25482.7461, grad_fn=<AddBackward0>)\n",
            "tensor(25238.6719, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [10/469], Reconst Loss: 170.8349, KL Div: 1.7561\n",
            "tensor(25126.1328, grad_fn=<AddBackward0>)\n",
            "tensor(25519.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25440.5039, grad_fn=<AddBackward0>)\n",
            "tensor(24744.2578, grad_fn=<AddBackward0>)\n",
            "tensor(25339.9746, grad_fn=<AddBackward0>)\n",
            "tensor(26259.2891, grad_fn=<AddBackward0>)\n",
            "tensor(25609.6621, grad_fn=<AddBackward0>)\n",
            "tensor(25435.3320, grad_fn=<AddBackward0>)\n",
            "tensor(25634.8828, grad_fn=<AddBackward0>)\n",
            "tensor(25344.7324, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [20/469], Reconst Loss: 170.6716, KL Div: 1.8223\n",
            "tensor(25912.0938, grad_fn=<AddBackward0>)\n",
            "tensor(25163.6309, grad_fn=<AddBackward0>)\n",
            "tensor(25465.5391, grad_fn=<AddBackward0>)\n",
            "tensor(24971.5938, grad_fn=<AddBackward0>)\n",
            "tensor(26229.8945, grad_fn=<AddBackward0>)\n",
            "tensor(24895.7930, grad_fn=<AddBackward0>)\n",
            "tensor(24185.8887, grad_fn=<AddBackward0>)\n",
            "tensor(25201.7461, grad_fn=<AddBackward0>)\n",
            "tensor(25606.6523, grad_fn=<AddBackward0>)\n",
            "tensor(24959.6133, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [30/469], Reconst Loss: 168.3257, KL Div: 1.7781\n",
            "tensor(25801.0781, grad_fn=<AddBackward0>)\n",
            "tensor(25276.6328, grad_fn=<AddBackward0>)\n",
            "tensor(24590.5020, grad_fn=<AddBackward0>)\n",
            "tensor(25352.4375, grad_fn=<AddBackward0>)\n",
            "tensor(23713.6602, grad_fn=<AddBackward0>)\n",
            "tensor(25564.2285, grad_fn=<AddBackward0>)\n",
            "tensor(25260.3008, grad_fn=<AddBackward0>)\n",
            "tensor(25406.1719, grad_fn=<AddBackward0>)\n",
            "tensor(24840.4531, grad_fn=<AddBackward0>)\n",
            "tensor(25761.0586, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [40/469], Reconst Loss: 172.8799, KL Div: 1.8919\n",
            "tensor(25092.7207, grad_fn=<AddBackward0>)\n",
            "tensor(24853.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25456.5332, grad_fn=<AddBackward0>)\n",
            "tensor(25459.6387, grad_fn=<AddBackward0>)\n",
            "tensor(24718.5039, grad_fn=<AddBackward0>)\n",
            "tensor(24830.8926, grad_fn=<AddBackward0>)\n",
            "tensor(24917.8906, grad_fn=<AddBackward0>)\n",
            "tensor(24775.4355, grad_fn=<AddBackward0>)\n",
            "tensor(25280.8711, grad_fn=<AddBackward0>)\n",
            "tensor(24596.9258, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [50/469], Reconst Loss: 166.2596, KL Div: 1.7269\n",
            "tensor(25186.4336, grad_fn=<AddBackward0>)\n",
            "tensor(24788.1895, grad_fn=<AddBackward0>)\n",
            "tensor(25045.8672, grad_fn=<AddBackward0>)\n",
            "tensor(26153.0801, grad_fn=<AddBackward0>)\n",
            "tensor(25297.6348, grad_fn=<AddBackward0>)\n",
            "tensor(25246.0215, grad_fn=<AddBackward0>)\n",
            "tensor(24346.7969, grad_fn=<AddBackward0>)\n",
            "tensor(25518.1367, grad_fn=<AddBackward0>)\n",
            "tensor(25391.0527, grad_fn=<AddBackward0>)\n",
            "tensor(24980.0742, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [60/469], Reconst Loss: 166.7257, KL Div: 1.8954\n",
            "tensor(26415.0156, grad_fn=<AddBackward0>)\n",
            "tensor(25251.4043, grad_fn=<AddBackward0>)\n",
            "tensor(24911.7305, grad_fn=<AddBackward0>)\n",
            "tensor(25000.9902, grad_fn=<AddBackward0>)\n",
            "tensor(25396.2949, grad_fn=<AddBackward0>)\n",
            "tensor(24077.5781, grad_fn=<AddBackward0>)\n",
            "tensor(25872.6289, grad_fn=<AddBackward0>)\n",
            "tensor(26339.1992, grad_fn=<AddBackward0>)\n",
            "tensor(24317.2441, grad_fn=<AddBackward0>)\n",
            "tensor(26074.4805, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [70/469], Reconst Loss: 174.9861, KL Div: 1.9147\n",
            "tensor(25469.8457, grad_fn=<AddBackward0>)\n",
            "tensor(25795.1074, grad_fn=<AddBackward0>)\n",
            "tensor(25608.3359, grad_fn=<AddBackward0>)\n",
            "tensor(26205.0957, grad_fn=<AddBackward0>)\n",
            "tensor(25432.8613, grad_fn=<AddBackward0>)\n",
            "tensor(25044.9062, grad_fn=<AddBackward0>)\n",
            "tensor(24072.1953, grad_fn=<AddBackward0>)\n",
            "tensor(25114.1758, grad_fn=<AddBackward0>)\n",
            "tensor(25424.7578, grad_fn=<AddBackward0>)\n",
            "tensor(25651.7168, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [80/469], Reconst Loss: 172.3419, KL Div: 1.8708\n",
            "tensor(24788.1445, grad_fn=<AddBackward0>)\n",
            "tensor(24550.3223, grad_fn=<AddBackward0>)\n",
            "tensor(25206.5898, grad_fn=<AddBackward0>)\n",
            "tensor(25213.6738, grad_fn=<AddBackward0>)\n",
            "tensor(25856.4434, grad_fn=<AddBackward0>)\n",
            "tensor(25841.9883, grad_fn=<AddBackward0>)\n",
            "tensor(24924.3984, grad_fn=<AddBackward0>)\n",
            "tensor(26141.2070, grad_fn=<AddBackward0>)\n",
            "tensor(24628.9238, grad_fn=<AddBackward0>)\n",
            "tensor(24712.8633, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [90/469], Reconst Loss: 165.9366, KL Div: 1.8088\n",
            "tensor(25575.8418, grad_fn=<AddBackward0>)\n",
            "tensor(26063.9805, grad_fn=<AddBackward0>)\n",
            "tensor(25801.4316, grad_fn=<AddBackward0>)\n",
            "tensor(24760.8789, grad_fn=<AddBackward0>)\n",
            "tensor(24741.3320, grad_fn=<AddBackward0>)\n",
            "tensor(25916.6289, grad_fn=<AddBackward0>)\n",
            "tensor(25330.2520, grad_fn=<AddBackward0>)\n",
            "tensor(25148.7773, grad_fn=<AddBackward0>)\n",
            "tensor(24811.6992, grad_fn=<AddBackward0>)\n",
            "tensor(26633.4082, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [100/469], Reconst Loss: 178.7034, KL Div: 1.9580\n",
            "tensor(24483.0625, grad_fn=<AddBackward0>)\n",
            "tensor(24831.4883, grad_fn=<AddBackward0>)\n",
            "tensor(25502.2363, grad_fn=<AddBackward0>)\n",
            "tensor(24942.9590, grad_fn=<AddBackward0>)\n",
            "tensor(24763.4648, grad_fn=<AddBackward0>)\n",
            "tensor(25839.4805, grad_fn=<AddBackward0>)\n",
            "tensor(25204.6289, grad_fn=<AddBackward0>)\n",
            "tensor(24949.0938, grad_fn=<AddBackward0>)\n",
            "tensor(25281.2891, grad_fn=<AddBackward0>)\n",
            "tensor(25150.6641, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [110/469], Reconst Loss: 169.2520, KL Div: 1.8158\n",
            "tensor(25290.4824, grad_fn=<AddBackward0>)\n",
            "tensor(25522.0273, grad_fn=<AddBackward0>)\n",
            "tensor(26044.3184, grad_fn=<AddBackward0>)\n",
            "tensor(24588.6738, grad_fn=<AddBackward0>)\n",
            "tensor(24521.8906, grad_fn=<AddBackward0>)\n",
            "tensor(24414.7441, grad_fn=<AddBackward0>)\n",
            "tensor(26260.0391, grad_fn=<AddBackward0>)\n",
            "tensor(24833.7988, grad_fn=<AddBackward0>)\n",
            "tensor(25367.0527, grad_fn=<AddBackward0>)\n",
            "tensor(24357.2461, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [120/469], Reconst Loss: 165.1699, KL Div: 1.6747\n",
            "tensor(25915.4180, grad_fn=<AddBackward0>)\n",
            "tensor(24840.3867, grad_fn=<AddBackward0>)\n",
            "tensor(25475.5059, grad_fn=<AddBackward0>)\n",
            "tensor(25960.2949, grad_fn=<AddBackward0>)\n",
            "tensor(24871.6465, grad_fn=<AddBackward0>)\n",
            "tensor(25322.5684, grad_fn=<AddBackward0>)\n",
            "tensor(23847.9297, grad_fn=<AddBackward0>)\n",
            "tensor(25743.5957, grad_fn=<AddBackward0>)\n",
            "tensor(25886.6562, grad_fn=<AddBackward0>)\n",
            "tensor(24700.9492, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [130/469], Reconst Loss: 167.0981, KL Div: 1.7252\n",
            "tensor(24995.8711, grad_fn=<AddBackward0>)\n",
            "tensor(24737.6777, grad_fn=<AddBackward0>)\n",
            "tensor(24441.6211, grad_fn=<AddBackward0>)\n",
            "tensor(24802., grad_fn=<AddBackward0>)\n",
            "tensor(25122.1523, grad_fn=<AddBackward0>)\n",
            "tensor(24482.2812, grad_fn=<AddBackward0>)\n",
            "tensor(23729.0820, grad_fn=<AddBackward0>)\n",
            "tensor(25454.8516, grad_fn=<AddBackward0>)\n",
            "tensor(24946.8574, grad_fn=<AddBackward0>)\n",
            "tensor(25226.3301, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [140/469], Reconst Loss: 170.4141, KL Div: 1.7778\n",
            "tensor(25476.4414, grad_fn=<AddBackward0>)\n",
            "tensor(25354.1992, grad_fn=<AddBackward0>)\n",
            "tensor(25476.0176, grad_fn=<AddBackward0>)\n",
            "tensor(24536.7207, grad_fn=<AddBackward0>)\n",
            "tensor(24949.4980, grad_fn=<AddBackward0>)\n",
            "tensor(24755.0684, grad_fn=<AddBackward0>)\n",
            "tensor(25825.0234, grad_fn=<AddBackward0>)\n",
            "tensor(25268.2578, grad_fn=<AddBackward0>)\n",
            "tensor(25535.6641, grad_fn=<AddBackward0>)\n",
            "tensor(24596.5273, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [150/469], Reconst Loss: 165.9563, KL Div: 1.7469\n",
            "tensor(24734.2734, grad_fn=<AddBackward0>)\n",
            "tensor(24263.9180, grad_fn=<AddBackward0>)\n",
            "tensor(25209.7891, grad_fn=<AddBackward0>)\n",
            "tensor(24721.2988, grad_fn=<AddBackward0>)\n",
            "tensor(25938.5801, grad_fn=<AddBackward0>)\n",
            "tensor(25054.0332, grad_fn=<AddBackward0>)\n",
            "tensor(24710.3066, grad_fn=<AddBackward0>)\n",
            "tensor(26457.6211, grad_fn=<AddBackward0>)\n",
            "tensor(25628.5000, grad_fn=<AddBackward0>)\n",
            "tensor(25536.6953, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [160/469], Reconst Loss: 172.5417, KL Div: 1.7976\n",
            "tensor(25491.2891, grad_fn=<AddBackward0>)\n",
            "tensor(25294.7891, grad_fn=<AddBackward0>)\n",
            "tensor(26042.4824, grad_fn=<AddBackward0>)\n",
            "tensor(25025.2383, grad_fn=<AddBackward0>)\n",
            "tensor(25362.6250, grad_fn=<AddBackward0>)\n",
            "tensor(25342.0527, grad_fn=<AddBackward0>)\n",
            "tensor(24852.4453, grad_fn=<AddBackward0>)\n",
            "tensor(25858.1328, grad_fn=<AddBackward0>)\n",
            "tensor(24956.0566, grad_fn=<AddBackward0>)\n",
            "tensor(24994.1836, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [170/469], Reconst Loss: 168.5753, KL Div: 1.7794\n",
            "tensor(24965.5859, grad_fn=<AddBackward0>)\n",
            "tensor(25256.6562, grad_fn=<AddBackward0>)\n",
            "tensor(25000.6543, grad_fn=<AddBackward0>)\n",
            "tensor(25590.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25080.8320, grad_fn=<AddBackward0>)\n",
            "tensor(26048.6699, grad_fn=<AddBackward0>)\n",
            "tensor(23884.9043, grad_fn=<AddBackward0>)\n",
            "tensor(25610.4902, grad_fn=<AddBackward0>)\n",
            "tensor(24439.6660, grad_fn=<AddBackward0>)\n",
            "tensor(25265.4648, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [180/469], Reconst Loss: 170.2512, KL Div: 1.8090\n",
            "tensor(24865.0215, grad_fn=<AddBackward0>)\n",
            "tensor(25180.5078, grad_fn=<AddBackward0>)\n",
            "tensor(25817.7324, grad_fn=<AddBackward0>)\n",
            "tensor(24988.7207, grad_fn=<AddBackward0>)\n",
            "tensor(25672.0312, grad_fn=<AddBackward0>)\n",
            "tensor(24944.9199, grad_fn=<AddBackward0>)\n",
            "tensor(25037.3867, grad_fn=<AddBackward0>)\n",
            "tensor(25605.3125, grad_fn=<AddBackward0>)\n",
            "tensor(24845.0430, grad_fn=<AddBackward0>)\n",
            "tensor(26749.5977, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [190/469], Reconst Loss: 180.4226, KL Div: 1.9039\n",
            "tensor(25274.8164, grad_fn=<AddBackward0>)\n",
            "tensor(25566.2891, grad_fn=<AddBackward0>)\n",
            "tensor(24736.6738, grad_fn=<AddBackward0>)\n",
            "tensor(25152.5977, grad_fn=<AddBackward0>)\n",
            "tensor(25572.4102, grad_fn=<AddBackward0>)\n",
            "tensor(25176.1797, grad_fn=<AddBackward0>)\n",
            "tensor(25362.4395, grad_fn=<AddBackward0>)\n",
            "tensor(25606.8555, grad_fn=<AddBackward0>)\n",
            "tensor(25790.3555, grad_fn=<AddBackward0>)\n",
            "tensor(25117.2285, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [200/469], Reconst Loss: 169.3145, KL Div: 1.7943\n",
            "tensor(24836.2441, grad_fn=<AddBackward0>)\n",
            "tensor(24800.3945, grad_fn=<AddBackward0>)\n",
            "tensor(24222.3320, grad_fn=<AddBackward0>)\n",
            "tensor(25103.3340, grad_fn=<AddBackward0>)\n",
            "tensor(24539.7344, grad_fn=<AddBackward0>)\n",
            "tensor(25563.0117, grad_fn=<AddBackward0>)\n",
            "tensor(25741.1484, grad_fn=<AddBackward0>)\n",
            "tensor(25795.0156, grad_fn=<AddBackward0>)\n",
            "tensor(25675.2051, grad_fn=<AddBackward0>)\n",
            "tensor(25624.5371, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [210/469], Reconst Loss: 173.2012, KL Div: 1.7994\n",
            "tensor(25676.7324, grad_fn=<AddBackward0>)\n",
            "tensor(25818.2637, grad_fn=<AddBackward0>)\n",
            "tensor(24660.1133, grad_fn=<AddBackward0>)\n",
            "tensor(24800.0801, grad_fn=<AddBackward0>)\n",
            "tensor(25729.4980, grad_fn=<AddBackward0>)\n",
            "tensor(25810.3867, grad_fn=<AddBackward0>)\n",
            "tensor(25591.3242, grad_fn=<AddBackward0>)\n",
            "tensor(25040.3125, grad_fn=<AddBackward0>)\n",
            "tensor(25939.3086, grad_fn=<AddBackward0>)\n",
            "tensor(25611.7207, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [220/469], Reconst Loss: 170.6198, KL Div: 1.9648\n",
            "tensor(24839.1094, grad_fn=<AddBackward0>)\n",
            "tensor(25236.7891, grad_fn=<AddBackward0>)\n",
            "tensor(24404.9551, grad_fn=<AddBackward0>)\n",
            "tensor(25503.6895, grad_fn=<AddBackward0>)\n",
            "tensor(25199.7422, grad_fn=<AddBackward0>)\n",
            "tensor(25019.1230, grad_fn=<AddBackward0>)\n",
            "tensor(25365.6250, grad_fn=<AddBackward0>)\n",
            "tensor(24684.0391, grad_fn=<AddBackward0>)\n",
            "tensor(25538.6445, grad_fn=<AddBackward0>)\n",
            "tensor(25400.4434, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [230/469], Reconst Loss: 170.6172, KL Div: 1.8549\n",
            "tensor(26125.0781, grad_fn=<AddBackward0>)\n",
            "tensor(25488.7500, grad_fn=<AddBackward0>)\n",
            "tensor(26242.3906, grad_fn=<AddBackward0>)\n",
            "tensor(25502.4434, grad_fn=<AddBackward0>)\n",
            "tensor(24580.2734, grad_fn=<AddBackward0>)\n",
            "tensor(24755.8730, grad_fn=<AddBackward0>)\n",
            "tensor(26059.1172, grad_fn=<AddBackward0>)\n",
            "tensor(25148.4434, grad_fn=<AddBackward0>)\n",
            "tensor(23664.6758, grad_fn=<AddBackward0>)\n",
            "tensor(24825.1016, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [240/469], Reconst Loss: 166.2684, KL Div: 1.8452\n",
            "tensor(24057.4219, grad_fn=<AddBackward0>)\n",
            "tensor(24330.5371, grad_fn=<AddBackward0>)\n",
            "tensor(25355.5664, grad_fn=<AddBackward0>)\n",
            "tensor(24898.7480, grad_fn=<AddBackward0>)\n",
            "tensor(25062.9316, grad_fn=<AddBackward0>)\n",
            "tensor(24891.4062, grad_fn=<AddBackward0>)\n",
            "tensor(25299.1094, grad_fn=<AddBackward0>)\n",
            "tensor(25033.3926, grad_fn=<AddBackward0>)\n",
            "tensor(26130.6797, grad_fn=<AddBackward0>)\n",
            "tensor(24888.8750, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [250/469], Reconst Loss: 167.0841, KL Div: 1.8240\n",
            "tensor(25449.0254, grad_fn=<AddBackward0>)\n",
            "tensor(25080.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25346.7051, grad_fn=<AddBackward0>)\n",
            "tensor(25118.6367, grad_fn=<AddBackward0>)\n",
            "tensor(24875.2441, grad_fn=<AddBackward0>)\n",
            "tensor(25040.7266, grad_fn=<AddBackward0>)\n",
            "tensor(25637.2500, grad_fn=<AddBackward0>)\n",
            "tensor(25609.5781, grad_fn=<AddBackward0>)\n",
            "tensor(25012.6484, grad_fn=<AddBackward0>)\n",
            "tensor(25633.2207, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [260/469], Reconst Loss: 172.7548, KL Div: 1.8337\n",
            "tensor(24833.8320, grad_fn=<AddBackward0>)\n",
            "tensor(24344.0117, grad_fn=<AddBackward0>)\n",
            "tensor(25350.4082, grad_fn=<AddBackward0>)\n",
            "tensor(25575.5391, grad_fn=<AddBackward0>)\n",
            "tensor(25805.2852, grad_fn=<AddBackward0>)\n",
            "tensor(25561.5312, grad_fn=<AddBackward0>)\n",
            "tensor(24408.7617, grad_fn=<AddBackward0>)\n",
            "tensor(24584.4277, grad_fn=<AddBackward0>)\n",
            "tensor(25177.1289, grad_fn=<AddBackward0>)\n",
            "tensor(26408.9902, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [270/469], Reconst Loss: 180.1974, KL Div: 1.7415\n",
            "tensor(24658.8047, grad_fn=<AddBackward0>)\n",
            "tensor(25131.2129, grad_fn=<AddBackward0>)\n",
            "tensor(25331.5898, grad_fn=<AddBackward0>)\n",
            "tensor(24693.6309, grad_fn=<AddBackward0>)\n",
            "tensor(25386.4160, grad_fn=<AddBackward0>)\n",
            "tensor(25452.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25282.9473, grad_fn=<AddBackward0>)\n",
            "tensor(24804.1816, grad_fn=<AddBackward0>)\n",
            "tensor(25503.3691, grad_fn=<AddBackward0>)\n",
            "tensor(26074.2500, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [280/469], Reconst Loss: 178.3740, KL Div: 1.6887\n",
            "tensor(25057.4922, grad_fn=<AddBackward0>)\n",
            "tensor(24370.7793, grad_fn=<AddBackward0>)\n",
            "tensor(25183.9238, grad_fn=<AddBackward0>)\n",
            "tensor(24945.0098, grad_fn=<AddBackward0>)\n",
            "tensor(25259.6660, grad_fn=<AddBackward0>)\n",
            "tensor(26416.8359, grad_fn=<AddBackward0>)\n",
            "tensor(25753.2031, grad_fn=<AddBackward0>)\n",
            "tensor(25313.5254, grad_fn=<AddBackward0>)\n",
            "tensor(25324.4258, grad_fn=<AddBackward0>)\n",
            "tensor(26209.2109, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [290/469], Reconst Loss: 175.5802, KL Div: 1.9453\n",
            "tensor(24993.3047, grad_fn=<AddBackward0>)\n",
            "tensor(25648.2285, grad_fn=<AddBackward0>)\n",
            "tensor(26330.8770, grad_fn=<AddBackward0>)\n",
            "tensor(25147.0039, grad_fn=<AddBackward0>)\n",
            "tensor(25059.1758, grad_fn=<AddBackward0>)\n",
            "tensor(25764.2266, grad_fn=<AddBackward0>)\n",
            "tensor(25670.3359, grad_fn=<AddBackward0>)\n",
            "tensor(25529.6406, grad_fn=<AddBackward0>)\n",
            "tensor(25656.4180, grad_fn=<AddBackward0>)\n",
            "tensor(24445.2070, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [300/469], Reconst Loss: 165.0636, KL Div: 1.7276\n",
            "tensor(25902.3516, grad_fn=<AddBackward0>)\n",
            "tensor(24394.3301, grad_fn=<AddBackward0>)\n",
            "tensor(24513.5664, grad_fn=<AddBackward0>)\n",
            "tensor(24763.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25408.5820, grad_fn=<AddBackward0>)\n",
            "tensor(25900.5820, grad_fn=<AddBackward0>)\n",
            "tensor(25827.8047, grad_fn=<AddBackward0>)\n",
            "tensor(24951.1992, grad_fn=<AddBackward0>)\n",
            "tensor(25565.4336, grad_fn=<AddBackward0>)\n",
            "tensor(25385.2559, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [310/469], Reconst Loss: 171.2496, KL Div: 1.8048\n",
            "tensor(24529.4355, grad_fn=<AddBackward0>)\n",
            "tensor(25261.4590, grad_fn=<AddBackward0>)\n",
            "tensor(24467.5918, grad_fn=<AddBackward0>)\n",
            "tensor(25034.6348, grad_fn=<AddBackward0>)\n",
            "tensor(25060.9199, grad_fn=<AddBackward0>)\n",
            "tensor(25547.7812, grad_fn=<AddBackward0>)\n",
            "tensor(24329.2461, grad_fn=<AddBackward0>)\n",
            "tensor(24909.4824, grad_fn=<AddBackward0>)\n",
            "tensor(25542.7012, grad_fn=<AddBackward0>)\n",
            "tensor(25126.6191, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [320/469], Reconst Loss: 170.3351, KL Div: 1.7311\n",
            "tensor(25268.8379, grad_fn=<AddBackward0>)\n",
            "tensor(24699.2285, grad_fn=<AddBackward0>)\n",
            "tensor(25559.7148, grad_fn=<AddBackward0>)\n",
            "tensor(25351.4473, grad_fn=<AddBackward0>)\n",
            "tensor(26251.2402, grad_fn=<AddBackward0>)\n",
            "tensor(25062.6074, grad_fn=<AddBackward0>)\n",
            "tensor(24915.9238, grad_fn=<AddBackward0>)\n",
            "tensor(26915.4121, grad_fn=<AddBackward0>)\n",
            "tensor(25149.4922, grad_fn=<AddBackward0>)\n",
            "tensor(26392.5977, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [330/469], Reconst Loss: 178.8090, KL Div: 1.8255\n",
            "tensor(25639.3418, grad_fn=<AddBackward0>)\n",
            "tensor(24568.4199, grad_fn=<AddBackward0>)\n",
            "tensor(25691.0352, grad_fn=<AddBackward0>)\n",
            "tensor(25994.3906, grad_fn=<AddBackward0>)\n",
            "tensor(25932.6680, grad_fn=<AddBackward0>)\n",
            "tensor(24898.5195, grad_fn=<AddBackward0>)\n",
            "tensor(25108.2266, grad_fn=<AddBackward0>)\n",
            "tensor(24576.9453, grad_fn=<AddBackward0>)\n",
            "tensor(24710.0312, grad_fn=<AddBackward0>)\n",
            "tensor(25009.0469, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [340/469], Reconst Loss: 168.6838, KL Div: 1.7800\n",
            "tensor(24430.8984, grad_fn=<AddBackward0>)\n",
            "tensor(25553.3945, grad_fn=<AddBackward0>)\n",
            "tensor(25183.5000, grad_fn=<AddBackward0>)\n",
            "tensor(25347.0410, grad_fn=<AddBackward0>)\n",
            "tensor(24834.8457, grad_fn=<AddBackward0>)\n",
            "tensor(24559.1172, grad_fn=<AddBackward0>)\n",
            "tensor(25334.2930, grad_fn=<AddBackward0>)\n",
            "tensor(25269.9258, grad_fn=<AddBackward0>)\n",
            "tensor(25378.9746, grad_fn=<AddBackward0>)\n",
            "tensor(24913.8652, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [350/469], Reconst Loss: 168.1671, KL Div: 1.7648\n",
            "tensor(25446.3594, grad_fn=<AddBackward0>)\n",
            "tensor(24802.8477, grad_fn=<AddBackward0>)\n",
            "tensor(25072.1484, grad_fn=<AddBackward0>)\n",
            "tensor(25268.3184, grad_fn=<AddBackward0>)\n",
            "tensor(25061.2227, grad_fn=<AddBackward0>)\n",
            "tensor(25905.8496, grad_fn=<AddBackward0>)\n",
            "tensor(25454.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25125.5762, grad_fn=<AddBackward0>)\n",
            "tensor(25525.9277, grad_fn=<AddBackward0>)\n",
            "tensor(24581.5703, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [360/469], Reconst Loss: 163.5121, KL Div: 1.9021\n",
            "tensor(25530.6328, grad_fn=<AddBackward0>)\n",
            "tensor(25323.1289, grad_fn=<AddBackward0>)\n",
            "tensor(25334.5254, grad_fn=<AddBackward0>)\n",
            "tensor(24742.9375, grad_fn=<AddBackward0>)\n",
            "tensor(25356.8438, grad_fn=<AddBackward0>)\n",
            "tensor(25582.6035, grad_fn=<AddBackward0>)\n",
            "tensor(24823.4961, grad_fn=<AddBackward0>)\n",
            "tensor(25996.1660, grad_fn=<AddBackward0>)\n",
            "tensor(25455.1660, grad_fn=<AddBackward0>)\n",
            "tensor(24712.1523, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [370/469], Reconst Loss: 167.1594, KL Div: 1.7270\n",
            "tensor(25348.4316, grad_fn=<AddBackward0>)\n",
            "tensor(25485.9688, grad_fn=<AddBackward0>)\n",
            "tensor(25067.4375, grad_fn=<AddBackward0>)\n",
            "tensor(25649.9766, grad_fn=<AddBackward0>)\n",
            "tensor(24942.9902, grad_fn=<AddBackward0>)\n",
            "tensor(25373.8828, grad_fn=<AddBackward0>)\n",
            "tensor(24888.3008, grad_fn=<AddBackward0>)\n",
            "tensor(25330.9727, grad_fn=<AddBackward0>)\n",
            "tensor(25202.2656, grad_fn=<AddBackward0>)\n",
            "tensor(24868.4473, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [380/469], Reconst Loss: 167.1028, KL Div: 1.8121\n",
            "tensor(26058.7891, grad_fn=<AddBackward0>)\n",
            "tensor(25983.5508, grad_fn=<AddBackward0>)\n",
            "tensor(25336.8320, grad_fn=<AddBackward0>)\n",
            "tensor(25417.2734, grad_fn=<AddBackward0>)\n",
            "tensor(25533.9258, grad_fn=<AddBackward0>)\n",
            "tensor(25827.9883, grad_fn=<AddBackward0>)\n",
            "tensor(25403.8379, grad_fn=<AddBackward0>)\n",
            "tensor(24811.6113, grad_fn=<AddBackward0>)\n",
            "tensor(25397.4043, grad_fn=<AddBackward0>)\n",
            "tensor(24202.0898, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [390/469], Reconst Loss: 161.1130, KL Div: 1.8644\n",
            "tensor(24622.8398, grad_fn=<AddBackward0>)\n",
            "tensor(25085.2285, grad_fn=<AddBackward0>)\n",
            "tensor(26030.8633, grad_fn=<AddBackward0>)\n",
            "tensor(25762.5703, grad_fn=<AddBackward0>)\n",
            "tensor(25202.4922, grad_fn=<AddBackward0>)\n",
            "tensor(25090.3945, grad_fn=<AddBackward0>)\n",
            "tensor(24579.2383, grad_fn=<AddBackward0>)\n",
            "tensor(25878.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25200.6660, grad_fn=<AddBackward0>)\n",
            "tensor(25642.6484, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [400/469], Reconst Loss: 171.3590, KL Div: 1.9316\n",
            "tensor(25044.7168, grad_fn=<AddBackward0>)\n",
            "tensor(25516.5234, grad_fn=<AddBackward0>)\n",
            "tensor(25361.9453, grad_fn=<AddBackward0>)\n",
            "tensor(24765.8242, grad_fn=<AddBackward0>)\n",
            "tensor(26063.7891, grad_fn=<AddBackward0>)\n",
            "tensor(24766.3965, grad_fn=<AddBackward0>)\n",
            "tensor(25366.4844, grad_fn=<AddBackward0>)\n",
            "tensor(26179.5645, grad_fn=<AddBackward0>)\n",
            "tensor(25455.5371, grad_fn=<AddBackward0>)\n",
            "tensor(25060.6758, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [410/469], Reconst Loss: 168.7937, KL Div: 1.7995\n",
            "tensor(24778.2949, grad_fn=<AddBackward0>)\n",
            "tensor(25904.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25179.9531, grad_fn=<AddBackward0>)\n",
            "tensor(25284.2598, grad_fn=<AddBackward0>)\n",
            "tensor(24837.2793, grad_fn=<AddBackward0>)\n",
            "tensor(25413.3633, grad_fn=<AddBackward0>)\n",
            "tensor(25010.0352, grad_fn=<AddBackward0>)\n",
            "tensor(25700.5703, grad_fn=<AddBackward0>)\n",
            "tensor(25310.2969, grad_fn=<AddBackward0>)\n",
            "tensor(26119.6855, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [420/469], Reconst Loss: 176.1895, KL Div: 1.8580\n",
            "tensor(24842.6367, grad_fn=<AddBackward0>)\n",
            "tensor(24907.8691, grad_fn=<AddBackward0>)\n",
            "tensor(25208.4805, grad_fn=<AddBackward0>)\n",
            "tensor(24800.7734, grad_fn=<AddBackward0>)\n",
            "tensor(25180.9336, grad_fn=<AddBackward0>)\n",
            "tensor(25883.7539, grad_fn=<AddBackward0>)\n",
            "tensor(24817.3613, grad_fn=<AddBackward0>)\n",
            "tensor(26519.1504, grad_fn=<AddBackward0>)\n",
            "tensor(25581.8672, grad_fn=<AddBackward0>)\n",
            "tensor(25991.9023, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [430/469], Reconst Loss: 174.1924, KL Div: 1.9246\n",
            "tensor(25779.5312, grad_fn=<AddBackward0>)\n",
            "tensor(24785.4395, grad_fn=<AddBackward0>)\n",
            "tensor(24634.2949, grad_fn=<AddBackward0>)\n",
            "tensor(25633.2520, grad_fn=<AddBackward0>)\n",
            "tensor(25265.8555, grad_fn=<AddBackward0>)\n",
            "tensor(24237.7598, grad_fn=<AddBackward0>)\n",
            "tensor(25044.8379, grad_fn=<AddBackward0>)\n",
            "tensor(24676.9375, grad_fn=<AddBackward0>)\n",
            "tensor(24977.6133, grad_fn=<AddBackward0>)\n",
            "tensor(25409.9453, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [440/469], Reconst Loss: 171.5535, KL Div: 1.7974\n",
            "tensor(25054.3203, grad_fn=<AddBackward0>)\n",
            "tensor(24804.5996, grad_fn=<AddBackward0>)\n",
            "tensor(25703.3047, grad_fn=<AddBackward0>)\n",
            "tensor(25561.2031, grad_fn=<AddBackward0>)\n",
            "tensor(25714.8984, grad_fn=<AddBackward0>)\n",
            "tensor(25779.8496, grad_fn=<AddBackward0>)\n",
            "tensor(25889.3770, grad_fn=<AddBackward0>)\n",
            "tensor(25428.7773, grad_fn=<AddBackward0>)\n",
            "tensor(25499.9961, grad_fn=<AddBackward0>)\n",
            "tensor(25896.8965, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [450/469], Reconst Loss: 173.7448, KL Div: 1.9050\n",
            "tensor(26001.7148, grad_fn=<AddBackward0>)\n",
            "tensor(25717.6211, grad_fn=<AddBackward0>)\n",
            "tensor(25190.4492, grad_fn=<AddBackward0>)\n",
            "tensor(25221.5527, grad_fn=<AddBackward0>)\n",
            "tensor(25166.8105, grad_fn=<AddBackward0>)\n",
            "tensor(26004.5273, grad_fn=<AddBackward0>)\n",
            "tensor(24523.9688, grad_fn=<AddBackward0>)\n",
            "tensor(25321.1133, grad_fn=<AddBackward0>)\n",
            "tensor(25666.2891, grad_fn=<AddBackward0>)\n",
            "tensor(26710.0098, grad_fn=<AddBackward0>)\n",
            "Epoch[17/25], Step [460/469], Reconst Loss: 178.2162, KL Div: 2.0304\n",
            "tensor(25370.5938, grad_fn=<AddBackward0>)\n",
            "tensor(25159.7207, grad_fn=<AddBackward0>)\n",
            "tensor(25316.9688, grad_fn=<AddBackward0>)\n",
            "tensor(24978.7949, grad_fn=<AddBackward0>)\n",
            "tensor(25160.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25608.3750, grad_fn=<AddBackward0>)\n",
            "tensor(25057.8320, grad_fn=<AddBackward0>)\n",
            "tensor(25074.0020, grad_fn=<AddBackward0>)\n",
            "tensor(19213.6738, grad_fn=<AddBackward0>)\n",
            "tensor(25408.3203, grad_fn=<AddBackward0>)\n",
            "tensor(26148.9531, grad_fn=<AddBackward0>)\n",
            "tensor(25219.4277, grad_fn=<AddBackward0>)\n",
            "tensor(25370.1094, grad_fn=<AddBackward0>)\n",
            "tensor(25485.8164, grad_fn=<AddBackward0>)\n",
            "tensor(25954.4180, grad_fn=<AddBackward0>)\n",
            "tensor(25924.1172, grad_fn=<AddBackward0>)\n",
            "tensor(25072.3125, grad_fn=<AddBackward0>)\n",
            "tensor(25641.1445, grad_fn=<AddBackward0>)\n",
            "tensor(25971.2559, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [10/469], Reconst Loss: 174.1613, KL Div: 1.9159\n",
            "tensor(24911.8516, grad_fn=<AddBackward0>)\n",
            "tensor(24595.3984, grad_fn=<AddBackward0>)\n",
            "tensor(25407.5742, grad_fn=<AddBackward0>)\n",
            "tensor(26257.2773, grad_fn=<AddBackward0>)\n",
            "tensor(24801.6895, grad_fn=<AddBackward0>)\n",
            "tensor(23975.9805, grad_fn=<AddBackward0>)\n",
            "tensor(24424.7324, grad_fn=<AddBackward0>)\n",
            "tensor(25063.3145, grad_fn=<AddBackward0>)\n",
            "tensor(25170.6113, grad_fn=<AddBackward0>)\n",
            "tensor(24468.4160, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [20/469], Reconst Loss: 165.6487, KL Div: 1.7007\n",
            "tensor(25346.8828, grad_fn=<AddBackward0>)\n",
            "tensor(24940.4219, grad_fn=<AddBackward0>)\n",
            "tensor(24880.4336, grad_fn=<AddBackward0>)\n",
            "tensor(24018.7617, grad_fn=<AddBackward0>)\n",
            "tensor(25552.0391, grad_fn=<AddBackward0>)\n",
            "tensor(25186.5938, grad_fn=<AddBackward0>)\n",
            "tensor(26059.4512, grad_fn=<AddBackward0>)\n",
            "tensor(25048.4434, grad_fn=<AddBackward0>)\n",
            "tensor(24526.0176, grad_fn=<AddBackward0>)\n",
            "tensor(24735.5352, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [30/469], Reconst Loss: 167.3529, KL Div: 1.7262\n",
            "tensor(23823.9375, grad_fn=<AddBackward0>)\n",
            "tensor(25792.2480, grad_fn=<AddBackward0>)\n",
            "tensor(25419.1523, grad_fn=<AddBackward0>)\n",
            "tensor(25050.8711, grad_fn=<AddBackward0>)\n",
            "tensor(25036.0840, grad_fn=<AddBackward0>)\n",
            "tensor(25630.7344, grad_fn=<AddBackward0>)\n",
            "tensor(24545.6719, grad_fn=<AddBackward0>)\n",
            "tensor(25129.7363, grad_fn=<AddBackward0>)\n",
            "tensor(26028.4863, grad_fn=<AddBackward0>)\n",
            "tensor(25258.5039, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [40/469], Reconst Loss: 171.6820, KL Div: 1.7100\n",
            "tensor(25184.8730, grad_fn=<AddBackward0>)\n",
            "tensor(25602.6270, grad_fn=<AddBackward0>)\n",
            "tensor(24815.3438, grad_fn=<AddBackward0>)\n",
            "tensor(24958.2344, grad_fn=<AddBackward0>)\n",
            "tensor(24082.5488, grad_fn=<AddBackward0>)\n",
            "tensor(24928.5977, grad_fn=<AddBackward0>)\n",
            "tensor(25032., grad_fn=<AddBackward0>)\n",
            "tensor(24659.3848, grad_fn=<AddBackward0>)\n",
            "tensor(24149.4355, grad_fn=<AddBackward0>)\n",
            "tensor(24370.7129, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [50/469], Reconst Loss: 160.5943, KL Div: 1.9868\n",
            "tensor(25065.3496, grad_fn=<AddBackward0>)\n",
            "tensor(25598.6758, grad_fn=<AddBackward0>)\n",
            "tensor(25103.2969, grad_fn=<AddBackward0>)\n",
            "tensor(25988.6328, grad_fn=<AddBackward0>)\n",
            "tensor(25235.6055, grad_fn=<AddBackward0>)\n",
            "tensor(25188.7520, grad_fn=<AddBackward0>)\n",
            "tensor(26640.1582, grad_fn=<AddBackward0>)\n",
            "tensor(25825.3613, grad_fn=<AddBackward0>)\n",
            "tensor(25451.5469, grad_fn=<AddBackward0>)\n",
            "tensor(25525.6035, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [60/469], Reconst Loss: 173.1274, KL Div: 1.7528\n",
            "tensor(26424.7422, grad_fn=<AddBackward0>)\n",
            "tensor(25276.2773, grad_fn=<AddBackward0>)\n",
            "tensor(24887.3574, grad_fn=<AddBackward0>)\n",
            "tensor(24852.1895, grad_fn=<AddBackward0>)\n",
            "tensor(25367.6172, grad_fn=<AddBackward0>)\n",
            "tensor(24935.5840, grad_fn=<AddBackward0>)\n",
            "tensor(24661.3340, grad_fn=<AddBackward0>)\n",
            "tensor(24437.8125, grad_fn=<AddBackward0>)\n",
            "tensor(24863.8613, grad_fn=<AddBackward0>)\n",
            "tensor(24136.8867, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [70/469], Reconst Loss: 163.6135, KL Div: 1.6637\n",
            "tensor(25868.5469, grad_fn=<AddBackward0>)\n",
            "tensor(25229.4160, grad_fn=<AddBackward0>)\n",
            "tensor(24310.0547, grad_fn=<AddBackward0>)\n",
            "tensor(25716.5996, grad_fn=<AddBackward0>)\n",
            "tensor(26496.8789, grad_fn=<AddBackward0>)\n",
            "tensor(25782.7871, grad_fn=<AddBackward0>)\n",
            "tensor(24921.9297, grad_fn=<AddBackward0>)\n",
            "tensor(24197.9316, grad_fn=<AddBackward0>)\n",
            "tensor(25690.6797, grad_fn=<AddBackward0>)\n",
            "tensor(25771.4570, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [80/469], Reconst Loss: 173.2628, KL Div: 1.8718\n",
            "tensor(25385.2539, grad_fn=<AddBackward0>)\n",
            "tensor(26694.5332, grad_fn=<AddBackward0>)\n",
            "tensor(25507.1387, grad_fn=<AddBackward0>)\n",
            "tensor(25368.6367, grad_fn=<AddBackward0>)\n",
            "tensor(25507.8633, grad_fn=<AddBackward0>)\n",
            "tensor(24815.8262, grad_fn=<AddBackward0>)\n",
            "tensor(24573.8223, grad_fn=<AddBackward0>)\n",
            "tensor(24763.1992, grad_fn=<AddBackward0>)\n",
            "tensor(26580.3555, grad_fn=<AddBackward0>)\n",
            "tensor(25788.7402, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [90/469], Reconst Loss: 173.5026, KL Div: 1.8648\n",
            "tensor(25282.0117, grad_fn=<AddBackward0>)\n",
            "tensor(25602.5703, grad_fn=<AddBackward0>)\n",
            "tensor(25838.7676, grad_fn=<AddBackward0>)\n",
            "tensor(24999.5215, grad_fn=<AddBackward0>)\n",
            "tensor(24722.9531, grad_fn=<AddBackward0>)\n",
            "tensor(25539.1055, grad_fn=<AddBackward0>)\n",
            "tensor(25605.7168, grad_fn=<AddBackward0>)\n",
            "tensor(24722.0801, grad_fn=<AddBackward0>)\n",
            "tensor(24971.8691, grad_fn=<AddBackward0>)\n",
            "tensor(25295.0508, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [100/469], Reconst Loss: 171.7304, KL Div: 1.7258\n",
            "tensor(26038.9062, grad_fn=<AddBackward0>)\n",
            "tensor(24847.3398, grad_fn=<AddBackward0>)\n",
            "tensor(24416.2793, grad_fn=<AddBackward0>)\n",
            "tensor(24006.3105, grad_fn=<AddBackward0>)\n",
            "tensor(25357.7734, grad_fn=<AddBackward0>)\n",
            "tensor(25331.7891, grad_fn=<AddBackward0>)\n",
            "tensor(25126.1543, grad_fn=<AddBackward0>)\n",
            "tensor(25559.2969, grad_fn=<AddBackward0>)\n",
            "tensor(24449.1406, grad_fn=<AddBackward0>)\n",
            "tensor(25255.2695, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [110/469], Reconst Loss: 169.9385, KL Div: 1.8246\n",
            "tensor(25131.4648, grad_fn=<AddBackward0>)\n",
            "tensor(25729.4648, grad_fn=<AddBackward0>)\n",
            "tensor(24920.3516, grad_fn=<AddBackward0>)\n",
            "tensor(24985.6406, grad_fn=<AddBackward0>)\n",
            "tensor(25393.1719, grad_fn=<AddBackward0>)\n",
            "tensor(24866.8066, grad_fn=<AddBackward0>)\n",
            "tensor(25696.9961, grad_fn=<AddBackward0>)\n",
            "tensor(25902.8359, grad_fn=<AddBackward0>)\n",
            "tensor(25729.6660, grad_fn=<AddBackward0>)\n",
            "tensor(24480.7930, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [120/469], Reconst Loss: 165.0893, KL Div: 1.7445\n",
            "tensor(26902.1484, grad_fn=<AddBackward0>)\n",
            "tensor(25043.0781, grad_fn=<AddBackward0>)\n",
            "tensor(26113.4473, grad_fn=<AddBackward0>)\n",
            "tensor(24653.3066, grad_fn=<AddBackward0>)\n",
            "tensor(25100.5996, grad_fn=<AddBackward0>)\n",
            "tensor(24557.9219, grad_fn=<AddBackward0>)\n",
            "tensor(25453.5312, grad_fn=<AddBackward0>)\n",
            "tensor(24979.6582, grad_fn=<AddBackward0>)\n",
            "tensor(24480.6289, grad_fn=<AddBackward0>)\n",
            "tensor(25285.1016, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [130/469], Reconst Loss: 170.8569, KL Div: 1.7789\n",
            "tensor(25484.8984, grad_fn=<AddBackward0>)\n",
            "tensor(25212.7305, grad_fn=<AddBackward0>)\n",
            "tensor(24188.6562, grad_fn=<AddBackward0>)\n",
            "tensor(25667.7969, grad_fn=<AddBackward0>)\n",
            "tensor(24379.3984, grad_fn=<AddBackward0>)\n",
            "tensor(25653.3672, grad_fn=<AddBackward0>)\n",
            "tensor(25584.5469, grad_fn=<AddBackward0>)\n",
            "tensor(25093.2656, grad_fn=<AddBackward0>)\n",
            "tensor(24346.2324, grad_fn=<AddBackward0>)\n",
            "tensor(25828.3613, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [140/469], Reconst Loss: 173.3104, KL Div: 1.8982\n",
            "tensor(25426.5352, grad_fn=<AddBackward0>)\n",
            "tensor(24891.5840, grad_fn=<AddBackward0>)\n",
            "tensor(24358.1172, grad_fn=<AddBackward0>)\n",
            "tensor(26048.0898, grad_fn=<AddBackward0>)\n",
            "tensor(24438.3809, grad_fn=<AddBackward0>)\n",
            "tensor(24844.7285, grad_fn=<AddBackward0>)\n",
            "tensor(25681.0156, grad_fn=<AddBackward0>)\n",
            "tensor(25726.6016, grad_fn=<AddBackward0>)\n",
            "tensor(25526.6016, grad_fn=<AddBackward0>)\n",
            "tensor(25456.5098, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [150/469], Reconst Loss: 172.6418, KL Div: 1.7491\n",
            "tensor(24920.4707, grad_fn=<AddBackward0>)\n",
            "tensor(24924.3848, grad_fn=<AddBackward0>)\n",
            "tensor(25519.0684, grad_fn=<AddBackward0>)\n",
            "tensor(25352.9043, grad_fn=<AddBackward0>)\n",
            "tensor(24804.6230, grad_fn=<AddBackward0>)\n",
            "tensor(25009.3184, grad_fn=<AddBackward0>)\n",
            "tensor(25723.0137, grad_fn=<AddBackward0>)\n",
            "tensor(23903.9609, grad_fn=<AddBackward0>)\n",
            "tensor(24713.0488, grad_fn=<AddBackward0>)\n",
            "tensor(25616.7617, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [160/469], Reconst Loss: 175.1982, KL Div: 1.6622\n",
            "tensor(24367.0078, grad_fn=<AddBackward0>)\n",
            "tensor(25547.2188, grad_fn=<AddBackward0>)\n",
            "tensor(25285.5801, grad_fn=<AddBackward0>)\n",
            "tensor(25952.0430, grad_fn=<AddBackward0>)\n",
            "tensor(25609.6875, grad_fn=<AddBackward0>)\n",
            "tensor(25204.9082, grad_fn=<AddBackward0>)\n",
            "tensor(24474.2168, grad_fn=<AddBackward0>)\n",
            "tensor(25657.1172, grad_fn=<AddBackward0>)\n",
            "tensor(24880.6660, grad_fn=<AddBackward0>)\n",
            "tensor(24969.2695, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [170/469], Reconst Loss: 170.2473, KL Div: 1.6550\n",
            "tensor(25553.7520, grad_fn=<AddBackward0>)\n",
            "tensor(25190.8281, grad_fn=<AddBackward0>)\n",
            "tensor(26156.5332, grad_fn=<AddBackward0>)\n",
            "tensor(26471.9531, grad_fn=<AddBackward0>)\n",
            "tensor(24751.5039, grad_fn=<AddBackward0>)\n",
            "tensor(24380.0527, grad_fn=<AddBackward0>)\n",
            "tensor(24668.1758, grad_fn=<AddBackward0>)\n",
            "tensor(25456.7832, grad_fn=<AddBackward0>)\n",
            "tensor(24041.8281, grad_fn=<AddBackward0>)\n",
            "tensor(24981.2246, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [180/469], Reconst Loss: 167.2409, KL Div: 1.8617\n",
            "tensor(25336.1133, grad_fn=<AddBackward0>)\n",
            "tensor(25294.0742, grad_fn=<AddBackward0>)\n",
            "tensor(25008.0098, grad_fn=<AddBackward0>)\n",
            "tensor(26520.6094, grad_fn=<AddBackward0>)\n",
            "tensor(25273.7930, grad_fn=<AddBackward0>)\n",
            "tensor(25495.7344, grad_fn=<AddBackward0>)\n",
            "tensor(25354.9238, grad_fn=<AddBackward0>)\n",
            "tensor(25135.4707, grad_fn=<AddBackward0>)\n",
            "tensor(25277.0703, grad_fn=<AddBackward0>)\n",
            "tensor(25479.2441, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [190/469], Reconst Loss: 173.0607, KL Div: 1.7331\n",
            "tensor(24669.2812, grad_fn=<AddBackward0>)\n",
            "tensor(26272.7109, grad_fn=<AddBackward0>)\n",
            "tensor(23859.3359, grad_fn=<AddBackward0>)\n",
            "tensor(25690.7773, grad_fn=<AddBackward0>)\n",
            "tensor(25905.0801, grad_fn=<AddBackward0>)\n",
            "tensor(24668.1836, grad_fn=<AddBackward0>)\n",
            "tensor(25961.9238, grad_fn=<AddBackward0>)\n",
            "tensor(24740.6426, grad_fn=<AddBackward0>)\n",
            "tensor(24890.9492, grad_fn=<AddBackward0>)\n",
            "tensor(25612.7305, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [200/469], Reconst Loss: 173.5287, KL Div: 1.7714\n",
            "tensor(24547.8066, grad_fn=<AddBackward0>)\n",
            "tensor(25180.9082, grad_fn=<AddBackward0>)\n",
            "tensor(24805.4883, grad_fn=<AddBackward0>)\n",
            "tensor(25031.0820, grad_fn=<AddBackward0>)\n",
            "tensor(23997.5137, grad_fn=<AddBackward0>)\n",
            "tensor(25260.2441, grad_fn=<AddBackward0>)\n",
            "tensor(24825.9473, grad_fn=<AddBackward0>)\n",
            "tensor(25030.3633, grad_fn=<AddBackward0>)\n",
            "tensor(25235.1621, grad_fn=<AddBackward0>)\n",
            "tensor(26110.6172, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [210/469], Reconst Loss: 177.1561, KL Div: 1.7889\n",
            "tensor(24535.3184, grad_fn=<AddBackward0>)\n",
            "tensor(24959.8887, grad_fn=<AddBackward0>)\n",
            "tensor(26346.1738, grad_fn=<AddBackward0>)\n",
            "tensor(25744.0625, grad_fn=<AddBackward0>)\n",
            "tensor(25685.5156, grad_fn=<AddBackward0>)\n",
            "tensor(25652.1367, grad_fn=<AddBackward0>)\n",
            "tensor(25680.7266, grad_fn=<AddBackward0>)\n",
            "tensor(26051.3359, grad_fn=<AddBackward0>)\n",
            "tensor(25235.3125, grad_fn=<AddBackward0>)\n",
            "tensor(25576.5137, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [220/469], Reconst Loss: 172.1526, KL Div: 1.8443\n",
            "tensor(25178.2734, grad_fn=<AddBackward0>)\n",
            "tensor(25041.8457, grad_fn=<AddBackward0>)\n",
            "tensor(24898.9336, grad_fn=<AddBackward0>)\n",
            "tensor(25167.0684, grad_fn=<AddBackward0>)\n",
            "tensor(25339.0176, grad_fn=<AddBackward0>)\n",
            "tensor(25546.3477, grad_fn=<AddBackward0>)\n",
            "tensor(26074.9004, grad_fn=<AddBackward0>)\n",
            "tensor(25919.5137, grad_fn=<AddBackward0>)\n",
            "tensor(24926.3965, grad_fn=<AddBackward0>)\n",
            "tensor(25431.2441, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [230/469], Reconst Loss: 169.4236, KL Div: 1.9505\n",
            "tensor(24892.8691, grad_fn=<AddBackward0>)\n",
            "tensor(25423.8613, grad_fn=<AddBackward0>)\n",
            "tensor(25626.7441, grad_fn=<AddBackward0>)\n",
            "tensor(25247.9316, grad_fn=<AddBackward0>)\n",
            "tensor(25928.8633, grad_fn=<AddBackward0>)\n",
            "tensor(25492.2129, grad_fn=<AddBackward0>)\n",
            "tensor(25918.1484, grad_fn=<AddBackward0>)\n",
            "tensor(25686.1934, grad_fn=<AddBackward0>)\n",
            "tensor(24250.5547, grad_fn=<AddBackward0>)\n",
            "tensor(24955.6777, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [240/469], Reconst Loss: 168.1053, KL Div: 1.7907\n",
            "tensor(25335.4609, grad_fn=<AddBackward0>)\n",
            "tensor(24897.0352, grad_fn=<AddBackward0>)\n",
            "tensor(26122.5312, grad_fn=<AddBackward0>)\n",
            "tensor(25240.7461, grad_fn=<AddBackward0>)\n",
            "tensor(24975.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25915.4062, grad_fn=<AddBackward0>)\n",
            "tensor(26018.0195, grad_fn=<AddBackward0>)\n",
            "tensor(24838.4922, grad_fn=<AddBackward0>)\n",
            "tensor(25137.8848, grad_fn=<AddBackward0>)\n",
            "tensor(24925.7891, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [250/469], Reconst Loss: 168.3029, KL Div: 1.7620\n",
            "tensor(25056.9922, grad_fn=<AddBackward0>)\n",
            "tensor(24769.0137, grad_fn=<AddBackward0>)\n",
            "tensor(24521.2852, grad_fn=<AddBackward0>)\n",
            "tensor(25705.1699, grad_fn=<AddBackward0>)\n",
            "tensor(26122.3711, grad_fn=<AddBackward0>)\n",
            "tensor(25066.7402, grad_fn=<AddBackward0>)\n",
            "tensor(25656.6211, grad_fn=<AddBackward0>)\n",
            "tensor(24540.5840, grad_fn=<AddBackward0>)\n",
            "tensor(25630.8223, grad_fn=<AddBackward0>)\n",
            "tensor(24767.7324, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [260/469], Reconst Loss: 167.5556, KL Div: 1.7295\n",
            "tensor(24953.3984, grad_fn=<AddBackward0>)\n",
            "tensor(25315.6777, grad_fn=<AddBackward0>)\n",
            "tensor(24077.7012, grad_fn=<AddBackward0>)\n",
            "tensor(25866.4141, grad_fn=<AddBackward0>)\n",
            "tensor(25802.4707, grad_fn=<AddBackward0>)\n",
            "tensor(24886.0391, grad_fn=<AddBackward0>)\n",
            "tensor(24941.9355, grad_fn=<AddBackward0>)\n",
            "tensor(24890.5703, grad_fn=<AddBackward0>)\n",
            "tensor(24788.8457, grad_fn=<AddBackward0>)\n",
            "tensor(24391.7070, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [270/469], Reconst Loss: 164.3559, KL Div: 1.7470\n",
            "tensor(25344.1035, grad_fn=<AddBackward0>)\n",
            "tensor(24962.3242, grad_fn=<AddBackward0>)\n",
            "tensor(25760.6953, grad_fn=<AddBackward0>)\n",
            "tensor(24968.8496, grad_fn=<AddBackward0>)\n",
            "tensor(25386.7910, grad_fn=<AddBackward0>)\n",
            "tensor(25626.3125, grad_fn=<AddBackward0>)\n",
            "tensor(25653.9883, grad_fn=<AddBackward0>)\n",
            "tensor(25002.0703, grad_fn=<AddBackward0>)\n",
            "tensor(24691.0078, grad_fn=<AddBackward0>)\n",
            "tensor(24851.1133, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [280/469], Reconst Loss: 166.5461, KL Div: 1.8402\n",
            "tensor(24814.0215, grad_fn=<AddBackward0>)\n",
            "tensor(24017.6719, grad_fn=<AddBackward0>)\n",
            "tensor(24489.5703, grad_fn=<AddBackward0>)\n",
            "tensor(25387.4688, grad_fn=<AddBackward0>)\n",
            "tensor(25147.6133, grad_fn=<AddBackward0>)\n",
            "tensor(24279.7754, grad_fn=<AddBackward0>)\n",
            "tensor(24440.7754, grad_fn=<AddBackward0>)\n",
            "tensor(24761.0625, grad_fn=<AddBackward0>)\n",
            "tensor(25659.6387, grad_fn=<AddBackward0>)\n",
            "tensor(26036.5352, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [290/469], Reconst Loss: 176.3250, KL Div: 1.8057\n",
            "tensor(24114.0215, grad_fn=<AddBackward0>)\n",
            "tensor(24854.0977, grad_fn=<AddBackward0>)\n",
            "tensor(25391.6289, grad_fn=<AddBackward0>)\n",
            "tensor(25571.8418, grad_fn=<AddBackward0>)\n",
            "tensor(25189.3945, grad_fn=<AddBackward0>)\n",
            "tensor(25415.5039, grad_fn=<AddBackward0>)\n",
            "tensor(25289.1836, grad_fn=<AddBackward0>)\n",
            "tensor(26030.5723, grad_fn=<AddBackward0>)\n",
            "tensor(24997.1328, grad_fn=<AddBackward0>)\n",
            "tensor(25191.3574, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [300/469], Reconst Loss: 168.2314, KL Div: 1.9051\n",
            "tensor(25469.6992, grad_fn=<AddBackward0>)\n",
            "tensor(24972.9492, grad_fn=<AddBackward0>)\n",
            "tensor(24296.1914, grad_fn=<AddBackward0>)\n",
            "tensor(25124.6934, grad_fn=<AddBackward0>)\n",
            "tensor(24608.2129, grad_fn=<AddBackward0>)\n",
            "tensor(25437.2969, grad_fn=<AddBackward0>)\n",
            "tensor(25531.3633, grad_fn=<AddBackward0>)\n",
            "tensor(25143.1797, grad_fn=<AddBackward0>)\n",
            "tensor(26453.7461, grad_fn=<AddBackward0>)\n",
            "tensor(25085.0195, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [310/469], Reconst Loss: 170.5355, KL Div: 1.6961\n",
            "tensor(26163.5625, grad_fn=<AddBackward0>)\n",
            "tensor(25320.5879, grad_fn=<AddBackward0>)\n",
            "tensor(24766.9785, grad_fn=<AddBackward0>)\n",
            "tensor(25501.8398, grad_fn=<AddBackward0>)\n",
            "tensor(24653.0273, grad_fn=<AddBackward0>)\n",
            "tensor(26036.8242, grad_fn=<AddBackward0>)\n",
            "tensor(25794.3789, grad_fn=<AddBackward0>)\n",
            "tensor(25117.1621, grad_fn=<AddBackward0>)\n",
            "tensor(24783.0625, grad_fn=<AddBackward0>)\n",
            "tensor(24944.3379, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [320/469], Reconst Loss: 167.1398, KL Div: 1.8492\n",
            "tensor(25340.3184, grad_fn=<AddBackward0>)\n",
            "tensor(25502.1582, grad_fn=<AddBackward0>)\n",
            "tensor(24733.9219, grad_fn=<AddBackward0>)\n",
            "tensor(25348.4336, grad_fn=<AddBackward0>)\n",
            "tensor(25191.4219, grad_fn=<AddBackward0>)\n",
            "tensor(23908.4121, grad_fn=<AddBackward0>)\n",
            "tensor(25741.8281, grad_fn=<AddBackward0>)\n",
            "tensor(25129.9531, grad_fn=<AddBackward0>)\n",
            "tensor(24950.3164, grad_fn=<AddBackward0>)\n",
            "tensor(25257.3711, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [330/469], Reconst Loss: 168.9955, KL Div: 1.8885\n",
            "tensor(25630.3145, grad_fn=<AddBackward0>)\n",
            "tensor(25550.3848, grad_fn=<AddBackward0>)\n",
            "tensor(24206.5801, grad_fn=<AddBackward0>)\n",
            "tensor(25636.1055, grad_fn=<AddBackward0>)\n",
            "tensor(25683.2305, grad_fn=<AddBackward0>)\n",
            "tensor(25648.1484, grad_fn=<AddBackward0>)\n",
            "tensor(25616.1406, grad_fn=<AddBackward0>)\n",
            "tensor(25354.0273, grad_fn=<AddBackward0>)\n",
            "tensor(24768.0195, grad_fn=<AddBackward0>)\n",
            "tensor(26087.7227, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [340/469], Reconst Loss: 176.8699, KL Div: 1.7960\n",
            "tensor(25177.7773, grad_fn=<AddBackward0>)\n",
            "tensor(26289.0820, grad_fn=<AddBackward0>)\n",
            "tensor(25453.1504, grad_fn=<AddBackward0>)\n",
            "tensor(25027.4219, grad_fn=<AddBackward0>)\n",
            "tensor(25579.8555, grad_fn=<AddBackward0>)\n",
            "tensor(25732.3438, grad_fn=<AddBackward0>)\n",
            "tensor(26006.0117, grad_fn=<AddBackward0>)\n",
            "tensor(24963.4160, grad_fn=<AddBackward0>)\n",
            "tensor(24904.9336, grad_fn=<AddBackward0>)\n",
            "tensor(24778.5703, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [350/469], Reconst Loss: 166.9678, KL Div: 1.7743\n",
            "tensor(24798.2969, grad_fn=<AddBackward0>)\n",
            "tensor(24881.5137, grad_fn=<AddBackward0>)\n",
            "tensor(26146.5410, grad_fn=<AddBackward0>)\n",
            "tensor(24242.6621, grad_fn=<AddBackward0>)\n",
            "tensor(24845.5527, grad_fn=<AddBackward0>)\n",
            "tensor(25478.1582, grad_fn=<AddBackward0>)\n",
            "tensor(25029.8047, grad_fn=<AddBackward0>)\n",
            "tensor(25067.4453, grad_fn=<AddBackward0>)\n",
            "tensor(24945.2754, grad_fn=<AddBackward0>)\n",
            "tensor(24954.0449, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [360/469], Reconst Loss: 168.8264, KL Div: 1.7418\n",
            "tensor(24830.2578, grad_fn=<AddBackward0>)\n",
            "tensor(24990.7754, grad_fn=<AddBackward0>)\n",
            "tensor(24915.1387, grad_fn=<AddBackward0>)\n",
            "tensor(24689.8086, grad_fn=<AddBackward0>)\n",
            "tensor(25488.5664, grad_fn=<AddBackward0>)\n",
            "tensor(25861.6953, grad_fn=<AddBackward0>)\n",
            "tensor(25102.5078, grad_fn=<AddBackward0>)\n",
            "tensor(25368.8047, grad_fn=<AddBackward0>)\n",
            "tensor(24413.8730, grad_fn=<AddBackward0>)\n",
            "tensor(25720.7246, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [370/469], Reconst Loss: 173.5093, KL Div: 1.8289\n",
            "tensor(26248.3672, grad_fn=<AddBackward0>)\n",
            "tensor(24677.7617, grad_fn=<AddBackward0>)\n",
            "tensor(26393.7930, grad_fn=<AddBackward0>)\n",
            "tensor(26402.6855, grad_fn=<AddBackward0>)\n",
            "tensor(24497.0391, grad_fn=<AddBackward0>)\n",
            "tensor(24990.0137, grad_fn=<AddBackward0>)\n",
            "tensor(26055.8301, grad_fn=<AddBackward0>)\n",
            "tensor(25693.4707, grad_fn=<AddBackward0>)\n",
            "tensor(25446.5215, grad_fn=<AddBackward0>)\n",
            "tensor(25609.4727, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [380/469], Reconst Loss: 173.4432, KL Div: 1.7754\n",
            "tensor(25755.0664, grad_fn=<AddBackward0>)\n",
            "tensor(25000.9160, grad_fn=<AddBackward0>)\n",
            "tensor(25412.0742, grad_fn=<AddBackward0>)\n",
            "tensor(25111.1094, grad_fn=<AddBackward0>)\n",
            "tensor(24900.1543, grad_fn=<AddBackward0>)\n",
            "tensor(25575.3281, grad_fn=<AddBackward0>)\n",
            "tensor(25055.8164, grad_fn=<AddBackward0>)\n",
            "tensor(25380.4277, grad_fn=<AddBackward0>)\n",
            "tensor(25412.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25512.0742, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [390/469], Reconst Loss: 172.5231, KL Div: 1.7860\n",
            "tensor(25364.3809, grad_fn=<AddBackward0>)\n",
            "tensor(24916.5664, grad_fn=<AddBackward0>)\n",
            "tensor(25457.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25630.6152, grad_fn=<AddBackward0>)\n",
            "tensor(26001.4883, grad_fn=<AddBackward0>)\n",
            "tensor(25993.4062, grad_fn=<AddBackward0>)\n",
            "tensor(24485.5645, grad_fn=<AddBackward0>)\n",
            "tensor(25100.7715, grad_fn=<AddBackward0>)\n",
            "tensor(24948.1680, grad_fn=<AddBackward0>)\n",
            "tensor(25962.8008, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [400/469], Reconst Loss: 175.2034, KL Div: 1.8421\n",
            "tensor(25137.0645, grad_fn=<AddBackward0>)\n",
            "tensor(25552.7051, grad_fn=<AddBackward0>)\n",
            "tensor(24707.2969, grad_fn=<AddBackward0>)\n",
            "tensor(25872.7441, grad_fn=<AddBackward0>)\n",
            "tensor(26490.3086, grad_fn=<AddBackward0>)\n",
            "tensor(24519.7832, grad_fn=<AddBackward0>)\n",
            "tensor(25977.1426, grad_fn=<AddBackward0>)\n",
            "tensor(24400.3496, grad_fn=<AddBackward0>)\n",
            "tensor(24766.7012, grad_fn=<AddBackward0>)\n",
            "tensor(24241.9531, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [410/469], Reconst Loss: 162.1366, KL Div: 1.8169\n",
            "tensor(24309.2031, grad_fn=<AddBackward0>)\n",
            "tensor(26078.4766, grad_fn=<AddBackward0>)\n",
            "tensor(25839.6270, grad_fn=<AddBackward0>)\n",
            "tensor(24983.7832, grad_fn=<AddBackward0>)\n",
            "tensor(25769.5938, grad_fn=<AddBackward0>)\n",
            "tensor(25669.2188, grad_fn=<AddBackward0>)\n",
            "tensor(24496.7363, grad_fn=<AddBackward0>)\n",
            "tensor(25555.0684, grad_fn=<AddBackward0>)\n",
            "tensor(25828.9980, grad_fn=<AddBackward0>)\n",
            "tensor(25052.5684, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [420/469], Reconst Loss: 167.3086, KL Div: 1.8943\n",
            "tensor(25041.2539, grad_fn=<AddBackward0>)\n",
            "tensor(24508.8828, grad_fn=<AddBackward0>)\n",
            "tensor(24977.1777, grad_fn=<AddBackward0>)\n",
            "tensor(25456.6699, grad_fn=<AddBackward0>)\n",
            "tensor(24891.6875, grad_fn=<AddBackward0>)\n",
            "tensor(25454.3301, grad_fn=<AddBackward0>)\n",
            "tensor(24843.6836, grad_fn=<AddBackward0>)\n",
            "tensor(25158.6426, grad_fn=<AddBackward0>)\n",
            "tensor(24591.7246, grad_fn=<AddBackward0>)\n",
            "tensor(25110.8223, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [430/469], Reconst Loss: 167.0250, KL Div: 1.9436\n",
            "tensor(25541.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25195.2207, grad_fn=<AddBackward0>)\n",
            "tensor(25935.1094, grad_fn=<AddBackward0>)\n",
            "tensor(25591.5508, grad_fn=<AddBackward0>)\n",
            "tensor(24785.3789, grad_fn=<AddBackward0>)\n",
            "tensor(26327.8047, grad_fn=<AddBackward0>)\n",
            "tensor(25535.3906, grad_fn=<AddBackward0>)\n",
            "tensor(24281.4219, grad_fn=<AddBackward0>)\n",
            "tensor(25765.8516, grad_fn=<AddBackward0>)\n",
            "tensor(25773.0430, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [440/469], Reconst Loss: 174.2043, KL Div: 1.8098\n",
            "tensor(26031.2793, grad_fn=<AddBackward0>)\n",
            "tensor(24978.2422, grad_fn=<AddBackward0>)\n",
            "tensor(25416.2441, grad_fn=<AddBackward0>)\n",
            "tensor(26125.7305, grad_fn=<AddBackward0>)\n",
            "tensor(25263.0859, grad_fn=<AddBackward0>)\n",
            "tensor(25930.7578, grad_fn=<AddBackward0>)\n",
            "tensor(24782.0488, grad_fn=<AddBackward0>)\n",
            "tensor(26079.7109, grad_fn=<AddBackward0>)\n",
            "tensor(25032.3574, grad_fn=<AddBackward0>)\n",
            "tensor(25349.1816, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [450/469], Reconst Loss: 169.2955, KL Div: 1.9163\n",
            "tensor(25891.6816, grad_fn=<AddBackward0>)\n",
            "tensor(26068.6426, grad_fn=<AddBackward0>)\n",
            "tensor(24510.6562, grad_fn=<AddBackward0>)\n",
            "tensor(26416.2617, grad_fn=<AddBackward0>)\n",
            "tensor(25491.3359, grad_fn=<AddBackward0>)\n",
            "tensor(26439.6250, grad_fn=<AddBackward0>)\n",
            "tensor(24034.6855, grad_fn=<AddBackward0>)\n",
            "tensor(25917.3594, grad_fn=<AddBackward0>)\n",
            "tensor(25527.9238, grad_fn=<AddBackward0>)\n",
            "tensor(24922.3848, grad_fn=<AddBackward0>)\n",
            "Epoch[18/25], Step [460/469], Reconst Loss: 167.3733, KL Div: 1.8222\n",
            "tensor(25330.2539, grad_fn=<AddBackward0>)\n",
            "tensor(25299.2324, grad_fn=<AddBackward0>)\n",
            "tensor(25751.7812, grad_fn=<AddBackward0>)\n",
            "tensor(25428.8691, grad_fn=<AddBackward0>)\n",
            "tensor(25424.1074, grad_fn=<AddBackward0>)\n",
            "tensor(25425.8828, grad_fn=<AddBackward0>)\n",
            "tensor(25110.7988, grad_fn=<AddBackward0>)\n",
            "tensor(24978.7207, grad_fn=<AddBackward0>)\n",
            "tensor(19002.3027, grad_fn=<AddBackward0>)\n",
            "tensor(25037.0469, grad_fn=<AddBackward0>)\n",
            "tensor(25368.4043, grad_fn=<AddBackward0>)\n",
            "tensor(25803.4395, grad_fn=<AddBackward0>)\n",
            "tensor(25554.9492, grad_fn=<AddBackward0>)\n",
            "tensor(25290.7383, grad_fn=<AddBackward0>)\n",
            "tensor(24923.2480, grad_fn=<AddBackward0>)\n",
            "tensor(25036.7344, grad_fn=<AddBackward0>)\n",
            "tensor(25790.5605, grad_fn=<AddBackward0>)\n",
            "tensor(25236.0254, grad_fn=<AddBackward0>)\n",
            "tensor(24633.0820, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [10/469], Reconst Loss: 166.4119, KL Div: 1.7356\n",
            "tensor(25190.6270, grad_fn=<AddBackward0>)\n",
            "tensor(25645.5977, grad_fn=<AddBackward0>)\n",
            "tensor(24788.6094, grad_fn=<AddBackward0>)\n",
            "tensor(25985.9473, grad_fn=<AddBackward0>)\n",
            "tensor(25976.5566, grad_fn=<AddBackward0>)\n",
            "tensor(25652.1758, grad_fn=<AddBackward0>)\n",
            "tensor(26139.8613, grad_fn=<AddBackward0>)\n",
            "tensor(25609.5566, grad_fn=<AddBackward0>)\n",
            "tensor(25473.6660, grad_fn=<AddBackward0>)\n",
            "tensor(26036.1016, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [20/469], Reconst Loss: 176.3075, KL Div: 1.8066\n",
            "tensor(24980.5879, grad_fn=<AddBackward0>)\n",
            "tensor(25279.8340, grad_fn=<AddBackward0>)\n",
            "tensor(25334.6758, grad_fn=<AddBackward0>)\n",
            "tensor(25719.2559, grad_fn=<AddBackward0>)\n",
            "tensor(24630.6094, grad_fn=<AddBackward0>)\n",
            "tensor(25209.2344, grad_fn=<AddBackward0>)\n",
            "tensor(24238.0430, grad_fn=<AddBackward0>)\n",
            "tensor(25545.6094, grad_fn=<AddBackward0>)\n",
            "tensor(26378.5723, grad_fn=<AddBackward0>)\n",
            "tensor(25670.0195, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [30/469], Reconst Loss: 171.9353, KL Div: 1.9075\n",
            "tensor(25524.0215, grad_fn=<AddBackward0>)\n",
            "tensor(25199.5078, grad_fn=<AddBackward0>)\n",
            "tensor(25203.3203, grad_fn=<AddBackward0>)\n",
            "tensor(25611.0918, grad_fn=<AddBackward0>)\n",
            "tensor(25398.3887, grad_fn=<AddBackward0>)\n",
            "tensor(26002.8945, grad_fn=<AddBackward0>)\n",
            "tensor(25705.8125, grad_fn=<AddBackward0>)\n",
            "tensor(24960.4922, grad_fn=<AddBackward0>)\n",
            "tensor(24639.5547, grad_fn=<AddBackward0>)\n",
            "tensor(26024.6855, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [40/469], Reconst Loss: 174.0709, KL Div: 1.9498\n",
            "tensor(26277.0312, grad_fn=<AddBackward0>)\n",
            "tensor(26017.1543, grad_fn=<AddBackward0>)\n",
            "tensor(24305.3438, grad_fn=<AddBackward0>)\n",
            "tensor(25289.1387, grad_fn=<AddBackward0>)\n",
            "tensor(25560.1660, grad_fn=<AddBackward0>)\n",
            "tensor(25253.2637, grad_fn=<AddBackward0>)\n",
            "tensor(24842.5234, grad_fn=<AddBackward0>)\n",
            "tensor(24982.5117, grad_fn=<AddBackward0>)\n",
            "tensor(25781.1738, grad_fn=<AddBackward0>)\n",
            "tensor(25309.6270, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [50/469], Reconst Loss: 169.1674, KL Div: 1.9043\n",
            "tensor(24329.0547, grad_fn=<AddBackward0>)\n",
            "tensor(24946.0293, grad_fn=<AddBackward0>)\n",
            "tensor(25527.7578, grad_fn=<AddBackward0>)\n",
            "tensor(24934.6465, grad_fn=<AddBackward0>)\n",
            "tensor(24917.5977, grad_fn=<AddBackward0>)\n",
            "tensor(25354.3965, grad_fn=<AddBackward0>)\n",
            "tensor(24961.8770, grad_fn=<AddBackward0>)\n",
            "tensor(24216.7891, grad_fn=<AddBackward0>)\n",
            "tensor(25293.7227, grad_fn=<AddBackward0>)\n",
            "tensor(24727.7715, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [60/469], Reconst Loss: 165.7250, KL Div: 1.8307\n",
            "tensor(25031.1738, grad_fn=<AddBackward0>)\n",
            "tensor(25587.0195, grad_fn=<AddBackward0>)\n",
            "tensor(25187.1797, grad_fn=<AddBackward0>)\n",
            "tensor(24168.8555, grad_fn=<AddBackward0>)\n",
            "tensor(25923.9648, grad_fn=<AddBackward0>)\n",
            "tensor(25309.3496, grad_fn=<AddBackward0>)\n",
            "tensor(25127.4863, grad_fn=<AddBackward0>)\n",
            "tensor(25135.2285, grad_fn=<AddBackward0>)\n",
            "tensor(25555.4980, grad_fn=<AddBackward0>)\n",
            "tensor(25617.3262, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [70/469], Reconst Loss: 175.2287, KL Div: 1.6604\n",
            "tensor(25216.3945, grad_fn=<AddBackward0>)\n",
            "tensor(26161.7227, grad_fn=<AddBackward0>)\n",
            "tensor(25607., grad_fn=<AddBackward0>)\n",
            "tensor(25908.6250, grad_fn=<AddBackward0>)\n",
            "tensor(24692.1426, grad_fn=<AddBackward0>)\n",
            "tensor(25998.9805, grad_fn=<AddBackward0>)\n",
            "tensor(25825.9258, grad_fn=<AddBackward0>)\n",
            "tensor(25506.1465, grad_fn=<AddBackward0>)\n",
            "tensor(25567.6348, grad_fn=<AddBackward0>)\n",
            "tensor(24740.0762, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [80/469], Reconst Loss: 164.2181, KL Div: 1.9376\n",
            "tensor(25569.3574, grad_fn=<AddBackward0>)\n",
            "tensor(25008.8613, grad_fn=<AddBackward0>)\n",
            "tensor(26337.9902, grad_fn=<AddBackward0>)\n",
            "tensor(25934.3965, grad_fn=<AddBackward0>)\n",
            "tensor(25706.3945, grad_fn=<AddBackward0>)\n",
            "tensor(25256.7109, grad_fn=<AddBackward0>)\n",
            "tensor(25631.9961, grad_fn=<AddBackward0>)\n",
            "tensor(24434.7988, grad_fn=<AddBackward0>)\n",
            "tensor(25534.0840, grad_fn=<AddBackward0>)\n",
            "tensor(25506.8516, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [90/469], Reconst Loss: 169.7938, KL Div: 1.9652\n",
            "tensor(24974.8008, grad_fn=<AddBackward0>)\n",
            "tensor(24936.2441, grad_fn=<AddBackward0>)\n",
            "tensor(25958.9902, grad_fn=<AddBackward0>)\n",
            "tensor(25434.6074, grad_fn=<AddBackward0>)\n",
            "tensor(24808.1680, grad_fn=<AddBackward0>)\n",
            "tensor(24052.2656, grad_fn=<AddBackward0>)\n",
            "tensor(25711.4531, grad_fn=<AddBackward0>)\n",
            "tensor(25968.8359, grad_fn=<AddBackward0>)\n",
            "tensor(25418.6758, grad_fn=<AddBackward0>)\n",
            "tensor(25020.5508, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [100/469], Reconst Loss: 168.7771, KL Div: 1.7797\n",
            "tensor(25177.1934, grad_fn=<AddBackward0>)\n",
            "tensor(24953.1602, grad_fn=<AddBackward0>)\n",
            "tensor(25234.0938, grad_fn=<AddBackward0>)\n",
            "tensor(25640.4141, grad_fn=<AddBackward0>)\n",
            "tensor(25083.4961, grad_fn=<AddBackward0>)\n",
            "tensor(26348.8105, grad_fn=<AddBackward0>)\n",
            "tensor(24826.9395, grad_fn=<AddBackward0>)\n",
            "tensor(24487.7578, grad_fn=<AddBackward0>)\n",
            "tensor(24047.3145, grad_fn=<AddBackward0>)\n",
            "tensor(26166.9375, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [110/469], Reconst Loss: 177.1913, KL Div: 1.8159\n",
            "tensor(25248.9238, grad_fn=<AddBackward0>)\n",
            "tensor(25243.0898, grad_fn=<AddBackward0>)\n",
            "tensor(25648.0762, grad_fn=<AddBackward0>)\n",
            "tensor(26730.3301, grad_fn=<AddBackward0>)\n",
            "tensor(25034.1270, grad_fn=<AddBackward0>)\n",
            "tensor(24717.6191, grad_fn=<AddBackward0>)\n",
            "tensor(24171.8594, grad_fn=<AddBackward0>)\n",
            "tensor(24204.2969, grad_fn=<AddBackward0>)\n",
            "tensor(26359.8848, grad_fn=<AddBackward0>)\n",
            "tensor(25329.1777, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [120/469], Reconst Loss: 172.0389, KL Div: 1.7230\n",
            "tensor(25017.9727, grad_fn=<AddBackward0>)\n",
            "tensor(25258.1270, grad_fn=<AddBackward0>)\n",
            "tensor(25697.9414, grad_fn=<AddBackward0>)\n",
            "tensor(25608.6777, grad_fn=<AddBackward0>)\n",
            "tensor(24508.7656, grad_fn=<AddBackward0>)\n",
            "tensor(26064.8262, grad_fn=<AddBackward0>)\n",
            "tensor(24657.1875, grad_fn=<AddBackward0>)\n",
            "tensor(24583.3867, grad_fn=<AddBackward0>)\n",
            "tensor(25732.4180, grad_fn=<AddBackward0>)\n",
            "tensor(24559.0664, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [130/469], Reconst Loss: 163.5261, KL Div: 1.8894\n",
            "tensor(25371.8887, grad_fn=<AddBackward0>)\n",
            "tensor(24311.1777, grad_fn=<AddBackward0>)\n",
            "tensor(24780.2012, grad_fn=<AddBackward0>)\n",
            "tensor(25252.5625, grad_fn=<AddBackward0>)\n",
            "tensor(24974.4180, grad_fn=<AddBackward0>)\n",
            "tensor(24803.9570, grad_fn=<AddBackward0>)\n",
            "tensor(24723., grad_fn=<AddBackward0>)\n",
            "tensor(25309.6875, grad_fn=<AddBackward0>)\n",
            "tensor(25236.6074, grad_fn=<AddBackward0>)\n",
            "tensor(25917.5332, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [140/469], Reconst Loss: 172.9877, KL Div: 1.9662\n",
            "tensor(25349.9883, grad_fn=<AddBackward0>)\n",
            "tensor(25626.8301, grad_fn=<AddBackward0>)\n",
            "tensor(25225.2500, grad_fn=<AddBackward0>)\n",
            "tensor(25163.9844, grad_fn=<AddBackward0>)\n",
            "tensor(25923.5352, grad_fn=<AddBackward0>)\n",
            "tensor(24669.9238, grad_fn=<AddBackward0>)\n",
            "tensor(25187.8438, grad_fn=<AddBackward0>)\n",
            "tensor(24449.9082, grad_fn=<AddBackward0>)\n",
            "tensor(26153.0469, grad_fn=<AddBackward0>)\n",
            "tensor(25205.9160, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [150/469], Reconst Loss: 168.4817, KL Div: 1.8960\n",
            "tensor(24936.9688, grad_fn=<AddBackward0>)\n",
            "tensor(23925.8281, grad_fn=<AddBackward0>)\n",
            "tensor(25245.8066, grad_fn=<AddBackward0>)\n",
            "tensor(25498.4492, grad_fn=<AddBackward0>)\n",
            "tensor(25199.4648, grad_fn=<AddBackward0>)\n",
            "tensor(25167.8730, grad_fn=<AddBackward0>)\n",
            "tensor(25320.5254, grad_fn=<AddBackward0>)\n",
            "tensor(24872.6465, grad_fn=<AddBackward0>)\n",
            "tensor(25671.0195, grad_fn=<AddBackward0>)\n",
            "tensor(24551.7305, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [160/469], Reconst Loss: 164.3702, KL Div: 1.8293\n",
            "tensor(25054.1367, grad_fn=<AddBackward0>)\n",
            "tensor(25572.7695, grad_fn=<AddBackward0>)\n",
            "tensor(24680.2363, grad_fn=<AddBackward0>)\n",
            "tensor(24870.3281, grad_fn=<AddBackward0>)\n",
            "tensor(25072.0820, grad_fn=<AddBackward0>)\n",
            "tensor(24364.4453, grad_fn=<AddBackward0>)\n",
            "tensor(25054.6914, grad_fn=<AddBackward0>)\n",
            "tensor(24304.7930, grad_fn=<AddBackward0>)\n",
            "tensor(24943.0391, grad_fn=<AddBackward0>)\n",
            "tensor(25558.5547, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [170/469], Reconst Loss: 174.1125, KL Div: 1.7042\n",
            "tensor(25432.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25895.9316, grad_fn=<AddBackward0>)\n",
            "tensor(24528.4512, grad_fn=<AddBackward0>)\n",
            "tensor(25164.7266, grad_fn=<AddBackward0>)\n",
            "tensor(25684.1133, grad_fn=<AddBackward0>)\n",
            "tensor(26248.0156, grad_fn=<AddBackward0>)\n",
            "tensor(25936.5742, grad_fn=<AddBackward0>)\n",
            "tensor(25112.7734, grad_fn=<AddBackward0>)\n",
            "tensor(25867.4219, grad_fn=<AddBackward0>)\n",
            "tensor(25695.4746, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [180/469], Reconst Loss: 172.4256, KL Div: 1.8880\n",
            "tensor(25180.7773, grad_fn=<AddBackward0>)\n",
            "tensor(24181.8223, grad_fn=<AddBackward0>)\n",
            "tensor(25526.7695, grad_fn=<AddBackward0>)\n",
            "tensor(25822.2754, grad_fn=<AddBackward0>)\n",
            "tensor(25685.9258, grad_fn=<AddBackward0>)\n",
            "tensor(25709.2109, grad_fn=<AddBackward0>)\n",
            "tensor(25460.8789, grad_fn=<AddBackward0>)\n",
            "tensor(25831.1309, grad_fn=<AddBackward0>)\n",
            "tensor(24570.4160, grad_fn=<AddBackward0>)\n",
            "tensor(25103.0977, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [190/469], Reconst Loss: 170.0484, KL Div: 1.7380\n",
            "tensor(25606.5117, grad_fn=<AddBackward0>)\n",
            "tensor(24890.4941, grad_fn=<AddBackward0>)\n",
            "tensor(24879.8047, grad_fn=<AddBackward0>)\n",
            "tensor(24574.4023, grad_fn=<AddBackward0>)\n",
            "tensor(23266.8906, grad_fn=<AddBackward0>)\n",
            "tensor(24383.3516, grad_fn=<AddBackward0>)\n",
            "tensor(25198.7812, grad_fn=<AddBackward0>)\n",
            "tensor(25681.3242, grad_fn=<AddBackward0>)\n",
            "tensor(25479.7832, grad_fn=<AddBackward0>)\n",
            "tensor(24816.0586, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [200/469], Reconst Loss: 166.6905, KL Div: 1.8123\n",
            "tensor(26560.6777, grad_fn=<AddBackward0>)\n",
            "tensor(25071.2070, grad_fn=<AddBackward0>)\n",
            "tensor(25255.3281, grad_fn=<AddBackward0>)\n",
            "tensor(25026.1094, grad_fn=<AddBackward0>)\n",
            "tensor(25230.4434, grad_fn=<AddBackward0>)\n",
            "tensor(25816.1465, grad_fn=<AddBackward0>)\n",
            "tensor(25350.8379, grad_fn=<AddBackward0>)\n",
            "tensor(24464.4395, grad_fn=<AddBackward0>)\n",
            "tensor(26190.0293, grad_fn=<AddBackward0>)\n",
            "tensor(24800.7773, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [210/469], Reconst Loss: 163.9751, KL Div: 1.9854\n",
            "tensor(24919.4297, grad_fn=<AddBackward0>)\n",
            "tensor(25549.4160, grad_fn=<AddBackward0>)\n",
            "tensor(24834.1055, grad_fn=<AddBackward0>)\n",
            "tensor(25076.0605, grad_fn=<AddBackward0>)\n",
            "tensor(25140.3203, grad_fn=<AddBackward0>)\n",
            "tensor(24900.6328, grad_fn=<AddBackward0>)\n",
            "tensor(24783.3945, grad_fn=<AddBackward0>)\n",
            "tensor(25843.0723, grad_fn=<AddBackward0>)\n",
            "tensor(25422.4434, grad_fn=<AddBackward0>)\n",
            "tensor(24318.2578, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [220/469], Reconst Loss: 163.6517, KL Div: 1.7556\n",
            "tensor(24886.6934, grad_fn=<AddBackward0>)\n",
            "tensor(25445.2227, grad_fn=<AddBackward0>)\n",
            "tensor(25168.8105, grad_fn=<AddBackward0>)\n",
            "tensor(25872.3789, grad_fn=<AddBackward0>)\n",
            "tensor(24190.3496, grad_fn=<AddBackward0>)\n",
            "tensor(24237.9727, grad_fn=<AddBackward0>)\n",
            "tensor(24593.7559, grad_fn=<AddBackward0>)\n",
            "tensor(25365.1562, grad_fn=<AddBackward0>)\n",
            "tensor(25088.9766, grad_fn=<AddBackward0>)\n",
            "tensor(25206.2344, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [230/469], Reconst Loss: 169.1970, KL Div: 1.8484\n",
            "tensor(25590.5723, grad_fn=<AddBackward0>)\n",
            "tensor(26105.2422, grad_fn=<AddBackward0>)\n",
            "tensor(25104.2617, grad_fn=<AddBackward0>)\n",
            "tensor(24609.9609, grad_fn=<AddBackward0>)\n",
            "tensor(26099.0410, grad_fn=<AddBackward0>)\n",
            "tensor(24945.4844, grad_fn=<AddBackward0>)\n",
            "tensor(25704.9316, grad_fn=<AddBackward0>)\n",
            "tensor(25558.8906, grad_fn=<AddBackward0>)\n",
            "tensor(25497.2109, grad_fn=<AddBackward0>)\n",
            "tensor(25131.1680, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [240/469], Reconst Loss: 170.5313, KL Div: 1.7204\n",
            "tensor(24961.7793, grad_fn=<AddBackward0>)\n",
            "tensor(26135.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25182.0703, grad_fn=<AddBackward0>)\n",
            "tensor(25525.0977, grad_fn=<AddBackward0>)\n",
            "tensor(26596.9355, grad_fn=<AddBackward0>)\n",
            "tensor(25839.6953, grad_fn=<AddBackward0>)\n",
            "tensor(25269.4922, grad_fn=<AddBackward0>)\n",
            "tensor(24993.4336, grad_fn=<AddBackward0>)\n",
            "tensor(25204.9414, grad_fn=<AddBackward0>)\n",
            "tensor(26217.9824, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [250/469], Reconst Loss: 176.2098, KL Div: 1.9079\n",
            "tensor(25391.8867, grad_fn=<AddBackward0>)\n",
            "tensor(23657.8633, grad_fn=<AddBackward0>)\n",
            "tensor(26570.6055, grad_fn=<AddBackward0>)\n",
            "tensor(25204.4609, grad_fn=<AddBackward0>)\n",
            "tensor(24254.7891, grad_fn=<AddBackward0>)\n",
            "tensor(25226.4648, grad_fn=<AddBackward0>)\n",
            "tensor(25123.4258, grad_fn=<AddBackward0>)\n",
            "tensor(25319.0625, grad_fn=<AddBackward0>)\n",
            "tensor(25244.4766, grad_fn=<AddBackward0>)\n",
            "tensor(25793.2910, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [260/469], Reconst Loss: 174.1801, KL Div: 1.8220\n",
            "tensor(26258.6230, grad_fn=<AddBackward0>)\n",
            "tensor(25040.5859, grad_fn=<AddBackward0>)\n",
            "tensor(25904.8008, grad_fn=<AddBackward0>)\n",
            "tensor(25913.9141, grad_fn=<AddBackward0>)\n",
            "tensor(24950.0684, grad_fn=<AddBackward0>)\n",
            "tensor(25056.1367, grad_fn=<AddBackward0>)\n",
            "tensor(25489.3262, grad_fn=<AddBackward0>)\n",
            "tensor(25135.4199, grad_fn=<AddBackward0>)\n",
            "tensor(25223.9551, grad_fn=<AddBackward0>)\n",
            "tensor(25157.4648, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [270/469], Reconst Loss: 167.7258, KL Div: 1.9211\n",
            "tensor(24657.1465, grad_fn=<AddBackward0>)\n",
            "tensor(25839.4629, grad_fn=<AddBackward0>)\n",
            "tensor(25118.6250, grad_fn=<AddBackward0>)\n",
            "tensor(24724.9746, grad_fn=<AddBackward0>)\n",
            "tensor(24642.7559, grad_fn=<AddBackward0>)\n",
            "tensor(25904.0273, grad_fn=<AddBackward0>)\n",
            "tensor(25257.3691, grad_fn=<AddBackward0>)\n",
            "tensor(25296.3496, grad_fn=<AddBackward0>)\n",
            "tensor(25219.4355, grad_fn=<AddBackward0>)\n",
            "tensor(25738.2871, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [280/469], Reconst Loss: 175.2938, KL Div: 1.7191\n",
            "tensor(25272.8711, grad_fn=<AddBackward0>)\n",
            "tensor(24868.6855, grad_fn=<AddBackward0>)\n",
            "tensor(25522.4883, grad_fn=<AddBackward0>)\n",
            "tensor(25066.2012, grad_fn=<AddBackward0>)\n",
            "tensor(24687.4609, grad_fn=<AddBackward0>)\n",
            "tensor(25901.0957, grad_fn=<AddBackward0>)\n",
            "tensor(25107.2656, grad_fn=<AddBackward0>)\n",
            "tensor(24898.4512, grad_fn=<AddBackward0>)\n",
            "tensor(25146.6094, grad_fn=<AddBackward0>)\n",
            "tensor(25802.7598, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [290/469], Reconst Loss: 172.5919, KL Div: 1.9328\n",
            "tensor(25182.1035, grad_fn=<AddBackward0>)\n",
            "tensor(24337.9941, grad_fn=<AddBackward0>)\n",
            "tensor(25089.3574, grad_fn=<AddBackward0>)\n",
            "tensor(25615.5684, grad_fn=<AddBackward0>)\n",
            "tensor(25956.4004, grad_fn=<AddBackward0>)\n",
            "tensor(25041.5938, grad_fn=<AddBackward0>)\n",
            "tensor(25673.4785, grad_fn=<AddBackward0>)\n",
            "tensor(25043.8477, grad_fn=<AddBackward0>)\n",
            "tensor(25015.0020, grad_fn=<AddBackward0>)\n",
            "tensor(25638.5000, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [300/469], Reconst Loss: 170.8550, KL Div: 1.9630\n",
            "tensor(24591.4043, grad_fn=<AddBackward0>)\n",
            "tensor(25386.2598, grad_fn=<AddBackward0>)\n",
            "tensor(25836.9922, grad_fn=<AddBackward0>)\n",
            "tensor(26540.1484, grad_fn=<AddBackward0>)\n",
            "tensor(24660.1797, grad_fn=<AddBackward0>)\n",
            "tensor(24733.6465, grad_fn=<AddBackward0>)\n",
            "tensor(25896.3223, grad_fn=<AddBackward0>)\n",
            "tensor(25494.4883, grad_fn=<AddBackward0>)\n",
            "tensor(26032.1211, grad_fn=<AddBackward0>)\n",
            "tensor(24985.0879, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [310/469], Reconst Loss: 169.5448, KL Div: 1.7101\n",
            "tensor(24921.4551, grad_fn=<AddBackward0>)\n",
            "tensor(24264.4219, grad_fn=<AddBackward0>)\n",
            "tensor(25348.8965, grad_fn=<AddBackward0>)\n",
            "tensor(25272.2617, grad_fn=<AddBackward0>)\n",
            "tensor(23831.7715, grad_fn=<AddBackward0>)\n",
            "tensor(25859.3555, grad_fn=<AddBackward0>)\n",
            "tensor(25626.2246, grad_fn=<AddBackward0>)\n",
            "tensor(26011.3555, grad_fn=<AddBackward0>)\n",
            "tensor(24863.5117, grad_fn=<AddBackward0>)\n",
            "tensor(25222.7383, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [320/469], Reconst Loss: 168.6521, KL Div: 1.8934\n",
            "tensor(25484.5684, grad_fn=<AddBackward0>)\n",
            "tensor(24397.8965, grad_fn=<AddBackward0>)\n",
            "tensor(24320.0391, grad_fn=<AddBackward0>)\n",
            "tensor(26092.2363, grad_fn=<AddBackward0>)\n",
            "tensor(25349.8945, grad_fn=<AddBackward0>)\n",
            "tensor(24302.0215, grad_fn=<AddBackward0>)\n",
            "tensor(24957.1855, grad_fn=<AddBackward0>)\n",
            "tensor(25169.2324, grad_fn=<AddBackward0>)\n",
            "tensor(25141.3418, grad_fn=<AddBackward0>)\n",
            "tensor(24814.9434, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [330/469], Reconst Loss: 166.8123, KL Div: 1.8036\n",
            "tensor(25605.7969, grad_fn=<AddBackward0>)\n",
            "tensor(25226.9062, grad_fn=<AddBackward0>)\n",
            "tensor(25515.6953, grad_fn=<AddBackward0>)\n",
            "tensor(24761.6309, grad_fn=<AddBackward0>)\n",
            "tensor(25591.2617, grad_fn=<AddBackward0>)\n",
            "tensor(26006.1992, grad_fn=<AddBackward0>)\n",
            "tensor(24679.0137, grad_fn=<AddBackward0>)\n",
            "tensor(25957.2773, grad_fn=<AddBackward0>)\n",
            "tensor(24876.2090, grad_fn=<AddBackward0>)\n",
            "tensor(25325.2188, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [340/469], Reconst Loss: 170.0497, KL Div: 1.8536\n",
            "tensor(24973.6406, grad_fn=<AddBackward0>)\n",
            "tensor(25991.7734, grad_fn=<AddBackward0>)\n",
            "tensor(24935.7383, grad_fn=<AddBackward0>)\n",
            "tensor(24708.5430, grad_fn=<AddBackward0>)\n",
            "tensor(25158.0312, grad_fn=<AddBackward0>)\n",
            "tensor(24803.2266, grad_fn=<AddBackward0>)\n",
            "tensor(25024.6660, grad_fn=<AddBackward0>)\n",
            "tensor(24597.2012, grad_fn=<AddBackward0>)\n",
            "tensor(25533.3750, grad_fn=<AddBackward0>)\n",
            "tensor(25823.4648, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [350/469], Reconst Loss: 173.1253, KL Div: 1.9080\n",
            "tensor(25586.0801, grad_fn=<AddBackward0>)\n",
            "tensor(25422.9785, grad_fn=<AddBackward0>)\n",
            "tensor(25158.7227, grad_fn=<AddBackward0>)\n",
            "tensor(25144.2578, grad_fn=<AddBackward0>)\n",
            "tensor(25079.4277, grad_fn=<AddBackward0>)\n",
            "tensor(25378.3516, grad_fn=<AddBackward0>)\n",
            "tensor(25431.1562, grad_fn=<AddBackward0>)\n",
            "tensor(25178.1426, grad_fn=<AddBackward0>)\n",
            "tensor(25742.6895, grad_fn=<AddBackward0>)\n",
            "tensor(25524.2402, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [360/469], Reconst Loss: 174.3025, KL Div: 1.6737\n",
            "tensor(24452.5723, grad_fn=<AddBackward0>)\n",
            "tensor(24479.1719, grad_fn=<AddBackward0>)\n",
            "tensor(25312.0215, grad_fn=<AddBackward0>)\n",
            "tensor(24703.6992, grad_fn=<AddBackward0>)\n",
            "tensor(25237.1348, grad_fn=<AddBackward0>)\n",
            "tensor(25655.2500, grad_fn=<AddBackward0>)\n",
            "tensor(25986.0801, grad_fn=<AddBackward0>)\n",
            "tensor(25467.3789, grad_fn=<AddBackward0>)\n",
            "tensor(25210.6543, grad_fn=<AddBackward0>)\n",
            "tensor(25144.2852, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [370/469], Reconst Loss: 170.0248, KL Div: 1.7610\n",
            "tensor(24787.5762, grad_fn=<AddBackward0>)\n",
            "tensor(25804.1016, grad_fn=<AddBackward0>)\n",
            "tensor(26023.4316, grad_fn=<AddBackward0>)\n",
            "tensor(25448.8301, grad_fn=<AddBackward0>)\n",
            "tensor(25261.0117, grad_fn=<AddBackward0>)\n",
            "tensor(25836.0859, grad_fn=<AddBackward0>)\n",
            "tensor(25085.5137, grad_fn=<AddBackward0>)\n",
            "tensor(24603.7812, grad_fn=<AddBackward0>)\n",
            "tensor(25601.0430, grad_fn=<AddBackward0>)\n",
            "tensor(24741.8535, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [380/469], Reconst Loss: 162.1099, KL Div: 2.0791\n",
            "tensor(26838.8770, grad_fn=<AddBackward0>)\n",
            "tensor(24615.4062, grad_fn=<AddBackward0>)\n",
            "tensor(25674.1758, grad_fn=<AddBackward0>)\n",
            "tensor(24924.5703, grad_fn=<AddBackward0>)\n",
            "tensor(25055.7148, grad_fn=<AddBackward0>)\n",
            "tensor(26250.3906, grad_fn=<AddBackward0>)\n",
            "tensor(25597.1738, grad_fn=<AddBackward0>)\n",
            "tensor(25327.7363, grad_fn=<AddBackward0>)\n",
            "tensor(25576.5977, grad_fn=<AddBackward0>)\n",
            "tensor(25828.0762, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [390/469], Reconst Loss: 172.1487, KL Div: 1.9755\n",
            "tensor(24666.4609, grad_fn=<AddBackward0>)\n",
            "tensor(25514.7598, grad_fn=<AddBackward0>)\n",
            "tensor(25597.5273, grad_fn=<AddBackward0>)\n",
            "tensor(24503.8203, grad_fn=<AddBackward0>)\n",
            "tensor(24285.9980, grad_fn=<AddBackward0>)\n",
            "tensor(24868.1133, grad_fn=<AddBackward0>)\n",
            "tensor(25064.1875, grad_fn=<AddBackward0>)\n",
            "tensor(25129.0078, grad_fn=<AddBackward0>)\n",
            "tensor(25623.0820, grad_fn=<AddBackward0>)\n",
            "tensor(24669.3633, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [400/469], Reconst Loss: 166.6038, KL Div: 1.7417\n",
            "tensor(25418.3320, grad_fn=<AddBackward0>)\n",
            "tensor(24629.5020, grad_fn=<AddBackward0>)\n",
            "tensor(25448.5664, grad_fn=<AddBackward0>)\n",
            "tensor(25219.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25956.8750, grad_fn=<AddBackward0>)\n",
            "tensor(25255.9414, grad_fn=<AddBackward0>)\n",
            "tensor(25380.8945, grad_fn=<AddBackward0>)\n",
            "tensor(25140.9883, grad_fn=<AddBackward0>)\n",
            "tensor(26211.9980, grad_fn=<AddBackward0>)\n",
            "tensor(25400.8145, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [410/469], Reconst Loss: 170.5718, KL Div: 1.8581\n",
            "tensor(24150.4453, grad_fn=<AddBackward0>)\n",
            "tensor(24695.2383, grad_fn=<AddBackward0>)\n",
            "tensor(24981.0605, grad_fn=<AddBackward0>)\n",
            "tensor(24439.1250, grad_fn=<AddBackward0>)\n",
            "tensor(24896.2852, grad_fn=<AddBackward0>)\n",
            "tensor(24937.8711, grad_fn=<AddBackward0>)\n",
            "tensor(24926.6895, grad_fn=<AddBackward0>)\n",
            "tensor(25470.2754, grad_fn=<AddBackward0>)\n",
            "tensor(24553.3223, grad_fn=<AddBackward0>)\n",
            "tensor(24862.6797, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [420/469], Reconst Loss: 166.9345, KL Div: 1.8203\n",
            "tensor(24966.1719, grad_fn=<AddBackward0>)\n",
            "tensor(25845.8047, grad_fn=<AddBackward0>)\n",
            "tensor(24835.2910, grad_fn=<AddBackward0>)\n",
            "tensor(24347.1465, grad_fn=<AddBackward0>)\n",
            "tensor(25336.9863, grad_fn=<AddBackward0>)\n",
            "tensor(25340.5820, grad_fn=<AddBackward0>)\n",
            "tensor(25515.3281, grad_fn=<AddBackward0>)\n",
            "tensor(25531.7148, grad_fn=<AddBackward0>)\n",
            "tensor(25159.7754, grad_fn=<AddBackward0>)\n",
            "tensor(25117.0273, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [430/469], Reconst Loss: 168.3633, KL Div: 1.8576\n",
            "tensor(25157.4180, grad_fn=<AddBackward0>)\n",
            "tensor(25476.6406, grad_fn=<AddBackward0>)\n",
            "tensor(25022.5586, grad_fn=<AddBackward0>)\n",
            "tensor(24488.8066, grad_fn=<AddBackward0>)\n",
            "tensor(24933.8691, grad_fn=<AddBackward0>)\n",
            "tensor(25364.0098, grad_fn=<AddBackward0>)\n",
            "tensor(25350.1699, grad_fn=<AddBackward0>)\n",
            "tensor(24712.5586, grad_fn=<AddBackward0>)\n",
            "tensor(26034.2598, grad_fn=<AddBackward0>)\n",
            "tensor(24957.2891, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [440/469], Reconst Loss: 165.1219, KL Div: 1.9905\n",
            "tensor(25306.3555, grad_fn=<AddBackward0>)\n",
            "tensor(24987.4336, grad_fn=<AddBackward0>)\n",
            "tensor(25792.7578, grad_fn=<AddBackward0>)\n",
            "tensor(25247.8457, grad_fn=<AddBackward0>)\n",
            "tensor(24478.3594, grad_fn=<AddBackward0>)\n",
            "tensor(24734.0352, grad_fn=<AddBackward0>)\n",
            "tensor(25279.8359, grad_fn=<AddBackward0>)\n",
            "tensor(25102.9277, grad_fn=<AddBackward0>)\n",
            "tensor(24653.4121, grad_fn=<AddBackward0>)\n",
            "tensor(25632.2148, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [450/469], Reconst Loss: 171.9585, KL Div: 1.8862\n",
            "tensor(25563.5195, grad_fn=<AddBackward0>)\n",
            "tensor(25434.2422, grad_fn=<AddBackward0>)\n",
            "tensor(24315.2324, grad_fn=<AddBackward0>)\n",
            "tensor(24380.5664, grad_fn=<AddBackward0>)\n",
            "tensor(25793.3516, grad_fn=<AddBackward0>)\n",
            "tensor(25063.8574, grad_fn=<AddBackward0>)\n",
            "tensor(24826.8203, grad_fn=<AddBackward0>)\n",
            "tensor(25575.3496, grad_fn=<AddBackward0>)\n",
            "tensor(23963.4902, grad_fn=<AddBackward0>)\n",
            "tensor(25516.5898, grad_fn=<AddBackward0>)\n",
            "Epoch[19/25], Step [460/469], Reconst Loss: 172.7272, KL Div: 1.7747\n",
            "tensor(24932.5254, grad_fn=<AddBackward0>)\n",
            "tensor(25740.6953, grad_fn=<AddBackward0>)\n",
            "tensor(24828.3438, grad_fn=<AddBackward0>)\n",
            "tensor(24993.4453, grad_fn=<AddBackward0>)\n",
            "tensor(25515.4805, grad_fn=<AddBackward0>)\n",
            "tensor(25055.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25668.7402, grad_fn=<AddBackward0>)\n",
            "tensor(24679.6914, grad_fn=<AddBackward0>)\n",
            "tensor(18953.3086, grad_fn=<AddBackward0>)\n",
            "tensor(24437.0352, grad_fn=<AddBackward0>)\n",
            "tensor(25988.7812, grad_fn=<AddBackward0>)\n",
            "tensor(24590.0117, grad_fn=<AddBackward0>)\n",
            "tensor(23637.3281, grad_fn=<AddBackward0>)\n",
            "tensor(24865.7266, grad_fn=<AddBackward0>)\n",
            "tensor(25214.1152, grad_fn=<AddBackward0>)\n",
            "tensor(24294.4102, grad_fn=<AddBackward0>)\n",
            "tensor(25551.2988, grad_fn=<AddBackward0>)\n",
            "tensor(25181.2480, grad_fn=<AddBackward0>)\n",
            "tensor(25857.9648, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [10/469], Reconst Loss: 172.3937, KL Div: 1.9748\n",
            "tensor(25970.5742, grad_fn=<AddBackward0>)\n",
            "tensor(24395.1074, grad_fn=<AddBackward0>)\n",
            "tensor(24907.3867, grad_fn=<AddBackward0>)\n",
            "tensor(24806.8848, grad_fn=<AddBackward0>)\n",
            "tensor(25631.4883, grad_fn=<AddBackward0>)\n",
            "tensor(24850.1992, grad_fn=<AddBackward0>)\n",
            "tensor(25242.7852, grad_fn=<AddBackward0>)\n",
            "tensor(25089.3418, grad_fn=<AddBackward0>)\n",
            "tensor(25477.3945, grad_fn=<AddBackward0>)\n",
            "tensor(25298.1797, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [20/469], Reconst Loss: 168.8231, KL Div: 1.9213\n",
            "tensor(24739.3711, grad_fn=<AddBackward0>)\n",
            "tensor(26024.1328, grad_fn=<AddBackward0>)\n",
            "tensor(25299.6035, grad_fn=<AddBackward0>)\n",
            "tensor(24969.2637, grad_fn=<AddBackward0>)\n",
            "tensor(25396.2051, grad_fn=<AddBackward0>)\n",
            "tensor(25781.8086, grad_fn=<AddBackward0>)\n",
            "tensor(24905.1895, grad_fn=<AddBackward0>)\n",
            "tensor(25647.6172, grad_fn=<AddBackward0>)\n",
            "tensor(25855.0059, grad_fn=<AddBackward0>)\n",
            "tensor(25384.9922, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [30/469], Reconst Loss: 171.2452, KL Div: 1.8050\n",
            "tensor(24542.7676, grad_fn=<AddBackward0>)\n",
            "tensor(25055.6992, grad_fn=<AddBackward0>)\n",
            "tensor(23071.0938, grad_fn=<AddBackward0>)\n",
            "tensor(25200.4199, grad_fn=<AddBackward0>)\n",
            "tensor(24837.9707, grad_fn=<AddBackward0>)\n",
            "tensor(25300.7285, grad_fn=<AddBackward0>)\n",
            "tensor(24925.9375, grad_fn=<AddBackward0>)\n",
            "tensor(25218.9727, grad_fn=<AddBackward0>)\n",
            "tensor(24891.1348, grad_fn=<AddBackward0>)\n",
            "tensor(25206.3047, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [40/469], Reconst Loss: 169.9890, KL Div: 1.7957\n",
            "tensor(24836.5527, grad_fn=<AddBackward0>)\n",
            "tensor(25194.7578, grad_fn=<AddBackward0>)\n",
            "tensor(25179.1797, grad_fn=<AddBackward0>)\n",
            "tensor(24596.3867, grad_fn=<AddBackward0>)\n",
            "tensor(24060.2383, grad_fn=<AddBackward0>)\n",
            "tensor(24666.3340, grad_fn=<AddBackward0>)\n",
            "tensor(25959.9336, grad_fn=<AddBackward0>)\n",
            "tensor(25548.2695, grad_fn=<AddBackward0>)\n",
            "tensor(25654.0020, grad_fn=<AddBackward0>)\n",
            "tensor(25031.7637, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [50/469], Reconst Loss: 166.1712, KL Div: 1.9593\n",
            "tensor(25202.6660, grad_fn=<AddBackward0>)\n",
            "tensor(26120.1621, grad_fn=<AddBackward0>)\n",
            "tensor(25766.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25156.7148, grad_fn=<AddBackward0>)\n",
            "tensor(25449.6934, grad_fn=<AddBackward0>)\n",
            "tensor(24821.0566, grad_fn=<AddBackward0>)\n",
            "tensor(26132.2500, grad_fn=<AddBackward0>)\n",
            "tensor(24169.3984, grad_fn=<AddBackward0>)\n",
            "tensor(25514.8145, grad_fn=<AddBackward0>)\n",
            "tensor(24755.1504, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [60/469], Reconst Loss: 167.2034, KL Div: 1.7464\n",
            "tensor(24489.9570, grad_fn=<AddBackward0>)\n",
            "tensor(24796.2891, grad_fn=<AddBackward0>)\n",
            "tensor(25715.5762, grad_fn=<AddBackward0>)\n",
            "tensor(25792.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25217.8984, grad_fn=<AddBackward0>)\n",
            "tensor(25664.5469, grad_fn=<AddBackward0>)\n",
            "tensor(25407.6250, grad_fn=<AddBackward0>)\n",
            "tensor(25262.4629, grad_fn=<AddBackward0>)\n",
            "tensor(25028.0391, grad_fn=<AddBackward0>)\n",
            "tensor(25245.3574, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [70/469], Reconst Loss: 168.6123, KL Div: 1.9078\n",
            "tensor(26122.9258, grad_fn=<AddBackward0>)\n",
            "tensor(25268.7109, grad_fn=<AddBackward0>)\n",
            "tensor(25037.6016, grad_fn=<AddBackward0>)\n",
            "tensor(25791.4004, grad_fn=<AddBackward0>)\n",
            "tensor(25402.8418, grad_fn=<AddBackward0>)\n",
            "tensor(25164.2383, grad_fn=<AddBackward0>)\n",
            "tensor(25249.5547, grad_fn=<AddBackward0>)\n",
            "tensor(25484.6875, grad_fn=<AddBackward0>)\n",
            "tensor(25781.3145, grad_fn=<AddBackward0>)\n",
            "tensor(24421.5820, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [80/469], Reconst Loss: 163.6001, KL Div: 1.8129\n",
            "tensor(25031.6914, grad_fn=<AddBackward0>)\n",
            "tensor(25681.0762, grad_fn=<AddBackward0>)\n",
            "tensor(25351.1465, grad_fn=<AddBackward0>)\n",
            "tensor(24911.1543, grad_fn=<AddBackward0>)\n",
            "tensor(25774.5742, grad_fn=<AddBackward0>)\n",
            "tensor(25067.6973, grad_fn=<AddBackward0>)\n",
            "tensor(24672.5645, grad_fn=<AddBackward0>)\n",
            "tensor(25562.1465, grad_fn=<AddBackward0>)\n",
            "tensor(25622.8965, grad_fn=<AddBackward0>)\n",
            "tensor(25405.7949, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [90/469], Reconst Loss: 172.8749, KL Div: 1.7072\n",
            "tensor(25047.2500, grad_fn=<AddBackward0>)\n",
            "tensor(25580.5195, grad_fn=<AddBackward0>)\n",
            "tensor(25524.0273, grad_fn=<AddBackward0>)\n",
            "tensor(25757.6875, grad_fn=<AddBackward0>)\n",
            "tensor(24753.2012, grad_fn=<AddBackward0>)\n",
            "tensor(24665.0469, grad_fn=<AddBackward0>)\n",
            "tensor(25091.0449, grad_fn=<AddBackward0>)\n",
            "tensor(25184.5078, grad_fn=<AddBackward0>)\n",
            "tensor(24979.4922, grad_fn=<AddBackward0>)\n",
            "tensor(25531.7188, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [100/469], Reconst Loss: 170.7271, KL Div: 1.9160\n",
            "tensor(25409.5293, grad_fn=<AddBackward0>)\n",
            "tensor(24886.2871, grad_fn=<AddBackward0>)\n",
            "tensor(25722.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25362.6973, grad_fn=<AddBackward0>)\n",
            "tensor(25342.6133, grad_fn=<AddBackward0>)\n",
            "tensor(25092.4609, grad_fn=<AddBackward0>)\n",
            "tensor(25725.3613, grad_fn=<AddBackward0>)\n",
            "tensor(25517.5410, grad_fn=<AddBackward0>)\n",
            "tensor(24450.6230, grad_fn=<AddBackward0>)\n",
            "tensor(24681.8848, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [110/469], Reconst Loss: 165.5705, KL Div: 1.8171\n",
            "tensor(25762.4570, grad_fn=<AddBackward0>)\n",
            "tensor(25710.0605, grad_fn=<AddBackward0>)\n",
            "tensor(26201.8945, grad_fn=<AddBackward0>)\n",
            "tensor(25903.6289, grad_fn=<AddBackward0>)\n",
            "tensor(24940.8164, grad_fn=<AddBackward0>)\n",
            "tensor(24765.2324, grad_fn=<AddBackward0>)\n",
            "tensor(24798.6172, grad_fn=<AddBackward0>)\n",
            "tensor(24387.3438, grad_fn=<AddBackward0>)\n",
            "tensor(25297.1562, grad_fn=<AddBackward0>)\n",
            "tensor(23810.7891, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [120/469], Reconst Loss: 157.7608, KL Div: 1.8841\n",
            "tensor(24230.2090, grad_fn=<AddBackward0>)\n",
            "tensor(26102.2500, grad_fn=<AddBackward0>)\n",
            "tensor(25477.0898, grad_fn=<AddBackward0>)\n",
            "tensor(24935.6113, grad_fn=<AddBackward0>)\n",
            "tensor(25804.2383, grad_fn=<AddBackward0>)\n",
            "tensor(25633.2031, grad_fn=<AddBackward0>)\n",
            "tensor(25445.4629, grad_fn=<AddBackward0>)\n",
            "tensor(25073.5215, grad_fn=<AddBackward0>)\n",
            "tensor(25318.1426, grad_fn=<AddBackward0>)\n",
            "tensor(25193.1348, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [130/469], Reconst Loss: 169.4304, KL Div: 1.8261\n",
            "tensor(24688.7910, grad_fn=<AddBackward0>)\n",
            "tensor(25757.4473, grad_fn=<AddBackward0>)\n",
            "tensor(25627.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25642.5137, grad_fn=<AddBackward0>)\n",
            "tensor(24999.1523, grad_fn=<AddBackward0>)\n",
            "tensor(25135.4512, grad_fn=<AddBackward0>)\n",
            "tensor(25448.0371, grad_fn=<AddBackward0>)\n",
            "tensor(25013.7832, grad_fn=<AddBackward0>)\n",
            "tensor(25737.0664, grad_fn=<AddBackward0>)\n",
            "tensor(25033.6328, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [140/469], Reconst Loss: 168.4937, KL Div: 1.8054\n",
            "tensor(25354.6055, grad_fn=<AddBackward0>)\n",
            "tensor(25260.1836, grad_fn=<AddBackward0>)\n",
            "tensor(25929.2871, grad_fn=<AddBackward0>)\n",
            "tensor(25062.2461, grad_fn=<AddBackward0>)\n",
            "tensor(24131.3652, grad_fn=<AddBackward0>)\n",
            "tensor(25001.8340, grad_fn=<AddBackward0>)\n",
            "tensor(25631.6309, grad_fn=<AddBackward0>)\n",
            "tensor(25721.3965, grad_fn=<AddBackward0>)\n",
            "tensor(26076.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25791.8027, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [150/469], Reconst Loss: 174.4138, KL Div: 1.8056\n",
            "tensor(24648.9980, grad_fn=<AddBackward0>)\n",
            "tensor(25197.5000, grad_fn=<AddBackward0>)\n",
            "tensor(25120.0332, grad_fn=<AddBackward0>)\n",
            "tensor(25034.6348, grad_fn=<AddBackward0>)\n",
            "tensor(25110.7637, grad_fn=<AddBackward0>)\n",
            "tensor(25407.2012, grad_fn=<AddBackward0>)\n",
            "tensor(25167.1855, grad_fn=<AddBackward0>)\n",
            "tensor(24638.3652, grad_fn=<AddBackward0>)\n",
            "tensor(25914.9883, grad_fn=<AddBackward0>)\n",
            "tensor(25492.0293, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [160/469], Reconst Loss: 171.4621, KL Div: 1.8463\n",
            "tensor(25400.0957, grad_fn=<AddBackward0>)\n",
            "tensor(24348.3555, grad_fn=<AddBackward0>)\n",
            "tensor(25009.1953, grad_fn=<AddBackward0>)\n",
            "tensor(25379.3574, grad_fn=<AddBackward0>)\n",
            "tensor(24728.7969, grad_fn=<AddBackward0>)\n",
            "tensor(25737.9746, grad_fn=<AddBackward0>)\n",
            "tensor(25475.3770, grad_fn=<AddBackward0>)\n",
            "tensor(25523.2832, grad_fn=<AddBackward0>)\n",
            "tensor(25440.9551, grad_fn=<AddBackward0>)\n",
            "tensor(25724.6758, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [170/469], Reconst Loss: 173.1172, KL Div: 1.8571\n",
            "tensor(25036.6191, grad_fn=<AddBackward0>)\n",
            "tensor(26086.0156, grad_fn=<AddBackward0>)\n",
            "tensor(24821.0762, grad_fn=<AddBackward0>)\n",
            "tensor(25025.2559, grad_fn=<AddBackward0>)\n",
            "tensor(24784.1465, grad_fn=<AddBackward0>)\n",
            "tensor(24811.3203, grad_fn=<AddBackward0>)\n",
            "tensor(25294.3711, grad_fn=<AddBackward0>)\n",
            "tensor(25881.2266, grad_fn=<AddBackward0>)\n",
            "tensor(24686.2734, grad_fn=<AddBackward0>)\n",
            "tensor(25248.5566, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [180/469], Reconst Loss: 167.7468, KL Div: 1.9672\n",
            "tensor(25308.6719, grad_fn=<AddBackward0>)\n",
            "tensor(25184.0801, grad_fn=<AddBackward0>)\n",
            "tensor(24858.2988, grad_fn=<AddBackward0>)\n",
            "tensor(25683.3320, grad_fn=<AddBackward0>)\n",
            "tensor(25017.2168, grad_fn=<AddBackward0>)\n",
            "tensor(26150.5098, grad_fn=<AddBackward0>)\n",
            "tensor(26351.8691, grad_fn=<AddBackward0>)\n",
            "tensor(24409.6523, grad_fn=<AddBackward0>)\n",
            "tensor(25299.3457, grad_fn=<AddBackward0>)\n",
            "tensor(25228.3750, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [190/469], Reconst Loss: 168.1858, KL Div: 1.9274\n",
            "tensor(25077.0566, grad_fn=<AddBackward0>)\n",
            "tensor(25634.3496, grad_fn=<AddBackward0>)\n",
            "tensor(25279.8535, grad_fn=<AddBackward0>)\n",
            "tensor(24769.7793, grad_fn=<AddBackward0>)\n",
            "tensor(26537.1504, grad_fn=<AddBackward0>)\n",
            "tensor(24858.9062, grad_fn=<AddBackward0>)\n",
            "tensor(24087.7949, grad_fn=<AddBackward0>)\n",
            "tensor(24989.0645, grad_fn=<AddBackward0>)\n",
            "tensor(25282.7305, grad_fn=<AddBackward0>)\n",
            "tensor(26153.9961, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [200/469], Reconst Loss: 175.5907, KL Div: 1.9158\n",
            "tensor(25744.7207, grad_fn=<AddBackward0>)\n",
            "tensor(24085.2559, grad_fn=<AddBackward0>)\n",
            "tensor(25569.5234, grad_fn=<AddBackward0>)\n",
            "tensor(24936.5508, grad_fn=<AddBackward0>)\n",
            "tensor(26122.5098, grad_fn=<AddBackward0>)\n",
            "tensor(25015.6875, grad_fn=<AddBackward0>)\n",
            "tensor(24656.0332, grad_fn=<AddBackward0>)\n",
            "tensor(24749.4844, grad_fn=<AddBackward0>)\n",
            "tensor(25057.5039, grad_fn=<AddBackward0>)\n",
            "tensor(25821.2559, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [210/469], Reconst Loss: 171.5842, KL Div: 2.0096\n",
            "tensor(25506.1426, grad_fn=<AddBackward0>)\n",
            "tensor(25499.6211, grad_fn=<AddBackward0>)\n",
            "tensor(26038.6738, grad_fn=<AddBackward0>)\n",
            "tensor(24747.3984, grad_fn=<AddBackward0>)\n",
            "tensor(25059.8848, grad_fn=<AddBackward0>)\n",
            "tensor(25423.1133, grad_fn=<AddBackward0>)\n",
            "tensor(25279.1777, grad_fn=<AddBackward0>)\n",
            "tensor(25274.1719, grad_fn=<AddBackward0>)\n",
            "tensor(24926.2754, grad_fn=<AddBackward0>)\n",
            "tensor(23902.8770, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [220/469], Reconst Loss: 161.2051, KL Div: 1.7024\n",
            "tensor(25240.0684, grad_fn=<AddBackward0>)\n",
            "tensor(25390.9727, grad_fn=<AddBackward0>)\n",
            "tensor(25760.2832, grad_fn=<AddBackward0>)\n",
            "tensor(24848.3828, grad_fn=<AddBackward0>)\n",
            "tensor(25227.9336, grad_fn=<AddBackward0>)\n",
            "tensor(24677.9121, grad_fn=<AddBackward0>)\n",
            "tensor(25433.7520, grad_fn=<AddBackward0>)\n",
            "tensor(25121.0273, grad_fn=<AddBackward0>)\n",
            "tensor(25638.5703, grad_fn=<AddBackward0>)\n",
            "tensor(25635.3262, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [230/469], Reconst Loss: 172.2000, KL Div: 1.8717\n",
            "tensor(25729.5254, grad_fn=<AddBackward0>)\n",
            "tensor(24975.0312, grad_fn=<AddBackward0>)\n",
            "tensor(25563.7754, grad_fn=<AddBackward0>)\n",
            "tensor(25783.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25274.9824, grad_fn=<AddBackward0>)\n",
            "tensor(25079.9961, grad_fn=<AddBackward0>)\n",
            "tensor(24529.2520, grad_fn=<AddBackward0>)\n",
            "tensor(24986.5059, grad_fn=<AddBackward0>)\n",
            "tensor(24563.2090, grad_fn=<AddBackward0>)\n",
            "tensor(26396.8262, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [240/469], Reconst Loss: 180.1189, KL Div: 1.7404\n",
            "tensor(24743.4922, grad_fn=<AddBackward0>)\n",
            "tensor(25037.2988, grad_fn=<AddBackward0>)\n",
            "tensor(23887.4395, grad_fn=<AddBackward0>)\n",
            "tensor(25096.8125, grad_fn=<AddBackward0>)\n",
            "tensor(24894.3184, grad_fn=<AddBackward0>)\n",
            "tensor(25245.1777, grad_fn=<AddBackward0>)\n",
            "tensor(24743.5234, grad_fn=<AddBackward0>)\n",
            "tensor(24564.0371, grad_fn=<AddBackward0>)\n",
            "tensor(24860.8164, grad_fn=<AddBackward0>)\n",
            "tensor(25506.1719, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [250/469], Reconst Loss: 168.2151, KL Div: 2.0701\n",
            "tensor(24524.0156, grad_fn=<AddBackward0>)\n",
            "tensor(25673.5352, grad_fn=<AddBackward0>)\n",
            "tensor(25425., grad_fn=<AddBackward0>)\n",
            "tensor(25987.8457, grad_fn=<AddBackward0>)\n",
            "tensor(25266.1035, grad_fn=<AddBackward0>)\n",
            "tensor(25228.3535, grad_fn=<AddBackward0>)\n",
            "tensor(25454.6035, grad_fn=<AddBackward0>)\n",
            "tensor(24630.7812, grad_fn=<AddBackward0>)\n",
            "tensor(24626.7129, grad_fn=<AddBackward0>)\n",
            "tensor(24763.6758, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [260/469], Reconst Loss: 167.5928, KL Div: 1.7249\n",
            "tensor(25786.0488, grad_fn=<AddBackward0>)\n",
            "tensor(25813.3340, grad_fn=<AddBackward0>)\n",
            "tensor(25163.1914, grad_fn=<AddBackward0>)\n",
            "tensor(25350.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25321.1680, grad_fn=<AddBackward0>)\n",
            "tensor(24680.8223, grad_fn=<AddBackward0>)\n",
            "tensor(25144.1289, grad_fn=<AddBackward0>)\n",
            "tensor(25233.2715, grad_fn=<AddBackward0>)\n",
            "tensor(24388.6680, grad_fn=<AddBackward0>)\n",
            "tensor(24991.8320, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [270/469], Reconst Loss: 166.5988, KL Div: 1.9100\n",
            "tensor(24645.7129, grad_fn=<AddBackward0>)\n",
            "tensor(25235.2793, grad_fn=<AddBackward0>)\n",
            "tensor(25182.5449, grad_fn=<AddBackward0>)\n",
            "tensor(24961.3320, grad_fn=<AddBackward0>)\n",
            "tensor(24555.7402, grad_fn=<AddBackward0>)\n",
            "tensor(25179.7871, grad_fn=<AddBackward0>)\n",
            "tensor(25430.6250, grad_fn=<AddBackward0>)\n",
            "tensor(26040.6504, grad_fn=<AddBackward0>)\n",
            "tensor(23406.2578, grad_fn=<AddBackward0>)\n",
            "tensor(25363.1562, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [280/469], Reconst Loss: 169.0958, KL Div: 1.9369\n",
            "tensor(24432.9023, grad_fn=<AddBackward0>)\n",
            "tensor(24539.1934, grad_fn=<AddBackward0>)\n",
            "tensor(24511.2930, grad_fn=<AddBackward0>)\n",
            "tensor(24508.7578, grad_fn=<AddBackward0>)\n",
            "tensor(25961.1445, grad_fn=<AddBackward0>)\n",
            "tensor(24377.8887, grad_fn=<AddBackward0>)\n",
            "tensor(24340.5312, grad_fn=<AddBackward0>)\n",
            "tensor(25012.5000, grad_fn=<AddBackward0>)\n",
            "tensor(25359.9219, grad_fn=<AddBackward0>)\n",
            "tensor(24741.7207, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [290/469], Reconst Loss: 166.6687, KL Div: 1.7751\n",
            "tensor(24888.7227, grad_fn=<AddBackward0>)\n",
            "tensor(26088.4492, grad_fn=<AddBackward0>)\n",
            "tensor(26195.4590, grad_fn=<AddBackward0>)\n",
            "tensor(24950.4531, grad_fn=<AddBackward0>)\n",
            "tensor(25232.5195, grad_fn=<AddBackward0>)\n",
            "tensor(26270.0859, grad_fn=<AddBackward0>)\n",
            "tensor(25271.2266, grad_fn=<AddBackward0>)\n",
            "tensor(25900.2051, grad_fn=<AddBackward0>)\n",
            "tensor(25150.7227, grad_fn=<AddBackward0>)\n",
            "tensor(25797.3828, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [300/469], Reconst Loss: 172.8401, KL Div: 1.9135\n",
            "tensor(24925.5840, grad_fn=<AddBackward0>)\n",
            "tensor(24121.4375, grad_fn=<AddBackward0>)\n",
            "tensor(25410.8281, grad_fn=<AddBackward0>)\n",
            "tensor(24921.6055, grad_fn=<AddBackward0>)\n",
            "tensor(24039.6992, grad_fn=<AddBackward0>)\n",
            "tensor(25717.4199, grad_fn=<AddBackward0>)\n",
            "tensor(25023.6465, grad_fn=<AddBackward0>)\n",
            "tensor(25738.4980, grad_fn=<AddBackward0>)\n",
            "tensor(25971.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25646.9043, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [310/469], Reconst Loss: 172.5062, KL Div: 1.8573\n",
            "tensor(25657.0176, grad_fn=<AddBackward0>)\n",
            "tensor(25378.3438, grad_fn=<AddBackward0>)\n",
            "tensor(25160.5527, grad_fn=<AddBackward0>)\n",
            "tensor(25800.0098, grad_fn=<AddBackward0>)\n",
            "tensor(24809.1504, grad_fn=<AddBackward0>)\n",
            "tensor(25955.7891, grad_fn=<AddBackward0>)\n",
            "tensor(25489.1328, grad_fn=<AddBackward0>)\n",
            "tensor(25286.7539, grad_fn=<AddBackward0>)\n",
            "tensor(25117., grad_fn=<AddBackward0>)\n",
            "tensor(24343.2676, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [320/469], Reconst Loss: 164.0382, KL Div: 1.7429\n",
            "tensor(26323.8633, grad_fn=<AddBackward0>)\n",
            "tensor(25389.6875, grad_fn=<AddBackward0>)\n",
            "tensor(25416.8164, grad_fn=<AddBackward0>)\n",
            "tensor(25719.9199, grad_fn=<AddBackward0>)\n",
            "tensor(25367.0352, grad_fn=<AddBackward0>)\n",
            "tensor(25245.0332, grad_fn=<AddBackward0>)\n",
            "tensor(24748.9355, grad_fn=<AddBackward0>)\n",
            "tensor(26314.6094, grad_fn=<AddBackward0>)\n",
            "tensor(25572.1172, grad_fn=<AddBackward0>)\n",
            "tensor(25819.7754, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [330/469], Reconst Loss: 171.9927, KL Div: 1.9816\n",
            "tensor(24690.6992, grad_fn=<AddBackward0>)\n",
            "tensor(25350.7910, grad_fn=<AddBackward0>)\n",
            "tensor(26244.0137, grad_fn=<AddBackward0>)\n",
            "tensor(25149.9785, grad_fn=<AddBackward0>)\n",
            "tensor(24822.6699, grad_fn=<AddBackward0>)\n",
            "tensor(25352.3965, grad_fn=<AddBackward0>)\n",
            "tensor(24440.8398, grad_fn=<AddBackward0>)\n",
            "tensor(24898.6875, grad_fn=<AddBackward0>)\n",
            "tensor(25291.2539, grad_fn=<AddBackward0>)\n",
            "tensor(25185.2773, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [340/469], Reconst Loss: 167.7996, KL Div: 1.9307\n",
            "tensor(25149.8047, grad_fn=<AddBackward0>)\n",
            "tensor(24138.9492, grad_fn=<AddBackward0>)\n",
            "tensor(25444.3535, grad_fn=<AddBackward0>)\n",
            "tensor(24147.5117, grad_fn=<AddBackward0>)\n",
            "tensor(25329.4844, grad_fn=<AddBackward0>)\n",
            "tensor(25323.4707, grad_fn=<AddBackward0>)\n",
            "tensor(26411.4082, grad_fn=<AddBackward0>)\n",
            "tensor(25619.8496, grad_fn=<AddBackward0>)\n",
            "tensor(25648.7422, grad_fn=<AddBackward0>)\n",
            "tensor(24872.9238, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [350/469], Reconst Loss: 165.7711, KL Div: 1.9032\n",
            "tensor(24983.4668, grad_fn=<AddBackward0>)\n",
            "tensor(24675.1309, grad_fn=<AddBackward0>)\n",
            "tensor(25666.9258, grad_fn=<AddBackward0>)\n",
            "tensor(23905.0078, grad_fn=<AddBackward0>)\n",
            "tensor(25709.8066, grad_fn=<AddBackward0>)\n",
            "tensor(25528.6094, grad_fn=<AddBackward0>)\n",
            "tensor(24970.6094, grad_fn=<AddBackward0>)\n",
            "tensor(24891.8027, grad_fn=<AddBackward0>)\n",
            "tensor(26021.2871, grad_fn=<AddBackward0>)\n",
            "tensor(25569.0137, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [360/469], Reconst Loss: 171.6055, KL Div: 1.8768\n",
            "tensor(25098.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25070.5820, grad_fn=<AddBackward0>)\n",
            "tensor(25319.5195, grad_fn=<AddBackward0>)\n",
            "tensor(24258.1211, grad_fn=<AddBackward0>)\n",
            "tensor(26394.5078, grad_fn=<AddBackward0>)\n",
            "tensor(25630.8027, grad_fn=<AddBackward0>)\n",
            "tensor(24977.6582, grad_fn=<AddBackward0>)\n",
            "tensor(24723.8672, grad_fn=<AddBackward0>)\n",
            "tensor(24646.0527, grad_fn=<AddBackward0>)\n",
            "tensor(25436.7480, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [370/469], Reconst Loss: 171.5763, KL Div: 1.8099\n",
            "tensor(24544.0234, grad_fn=<AddBackward0>)\n",
            "tensor(24520.7617, grad_fn=<AddBackward0>)\n",
            "tensor(23860.7109, grad_fn=<AddBackward0>)\n",
            "tensor(25704.1895, grad_fn=<AddBackward0>)\n",
            "tensor(24791.0781, grad_fn=<AddBackward0>)\n",
            "tensor(25369.3398, grad_fn=<AddBackward0>)\n",
            "tensor(25538.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25105.6738, grad_fn=<AddBackward0>)\n",
            "tensor(24615.6562, grad_fn=<AddBackward0>)\n",
            "tensor(25308.4570, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [380/469], Reconst Loss: 170.5257, KL Div: 1.8131\n",
            "tensor(25244.8789, grad_fn=<AddBackward0>)\n",
            "tensor(25258.9766, grad_fn=<AddBackward0>)\n",
            "tensor(25590.8672, grad_fn=<AddBackward0>)\n",
            "tensor(25448.3379, grad_fn=<AddBackward0>)\n",
            "tensor(25101.8203, grad_fn=<AddBackward0>)\n",
            "tensor(25408.7090, grad_fn=<AddBackward0>)\n",
            "tensor(25266.5312, grad_fn=<AddBackward0>)\n",
            "tensor(25373.2812, grad_fn=<AddBackward0>)\n",
            "tensor(25766.8555, grad_fn=<AddBackward0>)\n",
            "tensor(25899.7227, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [390/469], Reconst Loss: 173.1983, KL Div: 1.9429\n",
            "tensor(26244.5410, grad_fn=<AddBackward0>)\n",
            "tensor(25391.6035, grad_fn=<AddBackward0>)\n",
            "tensor(25623.4160, grad_fn=<AddBackward0>)\n",
            "tensor(25082.6074, grad_fn=<AddBackward0>)\n",
            "tensor(25740.8555, grad_fn=<AddBackward0>)\n",
            "tensor(25165.2910, grad_fn=<AddBackward0>)\n",
            "tensor(25466.1621, grad_fn=<AddBackward0>)\n",
            "tensor(25271.7363, grad_fn=<AddBackward0>)\n",
            "tensor(26034.5273, grad_fn=<AddBackward0>)\n",
            "tensor(25917.8457, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [400/469], Reconst Loss: 174.1304, KL Div: 1.8902\n",
            "tensor(24978.2090, grad_fn=<AddBackward0>)\n",
            "tensor(25507.4688, grad_fn=<AddBackward0>)\n",
            "tensor(24994.4141, grad_fn=<AddBackward0>)\n",
            "tensor(25842.5273, grad_fn=<AddBackward0>)\n",
            "tensor(25472.0684, grad_fn=<AddBackward0>)\n",
            "tensor(25120.0020, grad_fn=<AddBackward0>)\n",
            "tensor(25183.4766, grad_fn=<AddBackward0>)\n",
            "tensor(24646.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25347.9590, grad_fn=<AddBackward0>)\n",
            "tensor(25402.1816, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [410/469], Reconst Loss: 169.9639, KL Div: 1.8994\n",
            "tensor(25229.9922, grad_fn=<AddBackward0>)\n",
            "tensor(24980.7461, grad_fn=<AddBackward0>)\n",
            "tensor(25879.6250, grad_fn=<AddBackward0>)\n",
            "tensor(26465.6621, grad_fn=<AddBackward0>)\n",
            "tensor(25309.4688, grad_fn=<AddBackward0>)\n",
            "tensor(24999.5156, grad_fn=<AddBackward0>)\n",
            "tensor(24284.1133, grad_fn=<AddBackward0>)\n",
            "tensor(25333.8535, grad_fn=<AddBackward0>)\n",
            "tensor(24802.9844, grad_fn=<AddBackward0>)\n",
            "tensor(24546.7773, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [420/469], Reconst Loss: 160.8812, KL Div: 2.0594\n",
            "tensor(24723.4844, grad_fn=<AddBackward0>)\n",
            "tensor(25357.8203, grad_fn=<AddBackward0>)\n",
            "tensor(25959.2246, grad_fn=<AddBackward0>)\n",
            "tensor(24566.7949, grad_fn=<AddBackward0>)\n",
            "tensor(24623.0625, grad_fn=<AddBackward0>)\n",
            "tensor(26452.3066, grad_fn=<AddBackward0>)\n",
            "tensor(25702.9961, grad_fn=<AddBackward0>)\n",
            "tensor(25069.7852, grad_fn=<AddBackward0>)\n",
            "tensor(24953.1484, grad_fn=<AddBackward0>)\n",
            "tensor(24694.3398, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [430/469], Reconst Loss: 166.3272, KL Div: 1.7732\n",
            "tensor(25043.7168, grad_fn=<AddBackward0>)\n",
            "tensor(25604.9180, grad_fn=<AddBackward0>)\n",
            "tensor(25757.8359, grad_fn=<AddBackward0>)\n",
            "tensor(24820.7520, grad_fn=<AddBackward0>)\n",
            "tensor(25749.2656, grad_fn=<AddBackward0>)\n",
            "tensor(25801.0762, grad_fn=<AddBackward0>)\n",
            "tensor(25382.2402, grad_fn=<AddBackward0>)\n",
            "tensor(26202.8086, grad_fn=<AddBackward0>)\n",
            "tensor(25969.8086, grad_fn=<AddBackward0>)\n",
            "tensor(24713.8203, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [440/469], Reconst Loss: 166.0074, KL Div: 1.8046\n",
            "tensor(24663.3613, grad_fn=<AddBackward0>)\n",
            "tensor(25745.2930, grad_fn=<AddBackward0>)\n",
            "tensor(25013.7266, grad_fn=<AddBackward0>)\n",
            "tensor(25061.7715, grad_fn=<AddBackward0>)\n",
            "tensor(25291.7852, grad_fn=<AddBackward0>)\n",
            "tensor(24529.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25078.2891, grad_fn=<AddBackward0>)\n",
            "tensor(24330.8457, grad_fn=<AddBackward0>)\n",
            "tensor(25315.6289, grad_fn=<AddBackward0>)\n",
            "tensor(26120.9297, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [450/469], Reconst Loss: 176.5645, KL Div: 1.8337\n",
            "tensor(25520.3750, grad_fn=<AddBackward0>)\n",
            "tensor(25507.1914, grad_fn=<AddBackward0>)\n",
            "tensor(25438.7285, grad_fn=<AddBackward0>)\n",
            "tensor(25327.0859, grad_fn=<AddBackward0>)\n",
            "tensor(26047.0996, grad_fn=<AddBackward0>)\n",
            "tensor(24374.3633, grad_fn=<AddBackward0>)\n",
            "tensor(24714.1094, grad_fn=<AddBackward0>)\n",
            "tensor(25293.2441, grad_fn=<AddBackward0>)\n",
            "tensor(25388.9023, grad_fn=<AddBackward0>)\n",
            "tensor(25277.5547, grad_fn=<AddBackward0>)\n",
            "Epoch[20/25], Step [460/469], Reconst Loss: 169.7756, KL Div: 1.8470\n",
            "tensor(24481.7969, grad_fn=<AddBackward0>)\n",
            "tensor(25941.9160, grad_fn=<AddBackward0>)\n",
            "tensor(25266.6289, grad_fn=<AddBackward0>)\n",
            "tensor(25162.8145, grad_fn=<AddBackward0>)\n",
            "tensor(25626.7031, grad_fn=<AddBackward0>)\n",
            "tensor(24791.4082, grad_fn=<AddBackward0>)\n",
            "tensor(24850.8594, grad_fn=<AddBackward0>)\n",
            "tensor(23994.5352, grad_fn=<AddBackward0>)\n",
            "tensor(18647.6113, grad_fn=<AddBackward0>)\n",
            "tensor(25659.2441, grad_fn=<AddBackward0>)\n",
            "tensor(24129.9336, grad_fn=<AddBackward0>)\n",
            "tensor(25736.4590, grad_fn=<AddBackward0>)\n",
            "tensor(25782.1699, grad_fn=<AddBackward0>)\n",
            "tensor(24665.8379, grad_fn=<AddBackward0>)\n",
            "tensor(25510.1113, grad_fn=<AddBackward0>)\n",
            "tensor(25020.5840, grad_fn=<AddBackward0>)\n",
            "tensor(25431.8652, grad_fn=<AddBackward0>)\n",
            "tensor(26017.4590, grad_fn=<AddBackward0>)\n",
            "tensor(24444.7773, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [10/469], Reconst Loss: 161.6941, KL Div: 1.9520\n",
            "tensor(25343.3809, grad_fn=<AddBackward0>)\n",
            "tensor(25861.5918, grad_fn=<AddBackward0>)\n",
            "tensor(25226.4082, grad_fn=<AddBackward0>)\n",
            "tensor(25742.5117, grad_fn=<AddBackward0>)\n",
            "tensor(24983.9258, grad_fn=<AddBackward0>)\n",
            "tensor(24735.1836, grad_fn=<AddBackward0>)\n",
            "tensor(25202.8926, grad_fn=<AddBackward0>)\n",
            "tensor(25783.1250, grad_fn=<AddBackward0>)\n",
            "tensor(26347.2617, grad_fn=<AddBackward0>)\n",
            "tensor(24953.5664, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [20/469], Reconst Loss: 166.8989, KL Div: 1.8701\n",
            "tensor(25713.9492, grad_fn=<AddBackward0>)\n",
            "tensor(25708.1211, grad_fn=<AddBackward0>)\n",
            "tensor(25763.4434, grad_fn=<AddBackward0>)\n",
            "tensor(25223.5879, grad_fn=<AddBackward0>)\n",
            "tensor(24798.5977, grad_fn=<AddBackward0>)\n",
            "tensor(25071.3340, grad_fn=<AddBackward0>)\n",
            "tensor(24589.6934, grad_fn=<AddBackward0>)\n",
            "tensor(25196.0977, grad_fn=<AddBackward0>)\n",
            "tensor(25259.0684, grad_fn=<AddBackward0>)\n",
            "tensor(25268.2109, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [30/469], Reconst Loss: 168.8849, KL Div: 1.9015\n",
            "tensor(24785.6270, grad_fn=<AddBackward0>)\n",
            "tensor(25634.5332, grad_fn=<AddBackward0>)\n",
            "tensor(24483.0371, grad_fn=<AddBackward0>)\n",
            "tensor(25574.3223, grad_fn=<AddBackward0>)\n",
            "tensor(26060.7363, grad_fn=<AddBackward0>)\n",
            "tensor(24965.2441, grad_fn=<AddBackward0>)\n",
            "tensor(25034.0781, grad_fn=<AddBackward0>)\n",
            "tensor(24620.7148, grad_fn=<AddBackward0>)\n",
            "tensor(24856.1035, grad_fn=<AddBackward0>)\n",
            "tensor(24635.4844, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [40/469], Reconst Loss: 164.2187, KL Div: 1.8831\n",
            "tensor(25382.5996, grad_fn=<AddBackward0>)\n",
            "tensor(25270.3320, grad_fn=<AddBackward0>)\n",
            "tensor(24273.2168, grad_fn=<AddBackward0>)\n",
            "tensor(24659.9238, grad_fn=<AddBackward0>)\n",
            "tensor(25103.6719, grad_fn=<AddBackward0>)\n",
            "tensor(24805.2871, grad_fn=<AddBackward0>)\n",
            "tensor(26243.2539, grad_fn=<AddBackward0>)\n",
            "tensor(25431.7734, grad_fn=<AddBackward0>)\n",
            "tensor(24541.6406, grad_fn=<AddBackward0>)\n",
            "tensor(24772.1211, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [50/469], Reconst Loss: 165.8078, KL Div: 1.8483\n",
            "tensor(25140.0098, grad_fn=<AddBackward0>)\n",
            "tensor(25472.2676, grad_fn=<AddBackward0>)\n",
            "tensor(25181.5449, grad_fn=<AddBackward0>)\n",
            "tensor(26173.0801, grad_fn=<AddBackward0>)\n",
            "tensor(25016.7656, grad_fn=<AddBackward0>)\n",
            "tensor(24839.0996, grad_fn=<AddBackward0>)\n",
            "tensor(25837.5508, grad_fn=<AddBackward0>)\n",
            "tensor(25617.9062, grad_fn=<AddBackward0>)\n",
            "tensor(24973.1562, grad_fn=<AddBackward0>)\n",
            "tensor(24574.9062, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [60/469], Reconst Loss: 164.1858, KL Div: 1.8537\n",
            "tensor(25614.6172, grad_fn=<AddBackward0>)\n",
            "tensor(24846.6738, grad_fn=<AddBackward0>)\n",
            "tensor(25473.1680, grad_fn=<AddBackward0>)\n",
            "tensor(25240.9863, grad_fn=<AddBackward0>)\n",
            "tensor(24823.8750, grad_fn=<AddBackward0>)\n",
            "tensor(27068.8535, grad_fn=<AddBackward0>)\n",
            "tensor(24282.8184, grad_fn=<AddBackward0>)\n",
            "tensor(25840.5684, grad_fn=<AddBackward0>)\n",
            "tensor(25377.3301, grad_fn=<AddBackward0>)\n",
            "tensor(25586.8555, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [70/469], Reconst Loss: 170.6675, KL Div: 1.9487\n",
            "tensor(23825.1504, grad_fn=<AddBackward0>)\n",
            "tensor(25551.7383, grad_fn=<AddBackward0>)\n",
            "tensor(25195.9512, grad_fn=<AddBackward0>)\n",
            "tensor(24832.8965, grad_fn=<AddBackward0>)\n",
            "tensor(25546.7695, grad_fn=<AddBackward0>)\n",
            "tensor(24409.6367, grad_fn=<AddBackward0>)\n",
            "tensor(25637.4258, grad_fn=<AddBackward0>)\n",
            "tensor(25708.7812, grad_fn=<AddBackward0>)\n",
            "tensor(24771., grad_fn=<AddBackward0>)\n",
            "tensor(25431.7422, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [80/469], Reconst Loss: 168.8669, KL Div: 1.9879\n",
            "tensor(25167.4512, grad_fn=<AddBackward0>)\n",
            "tensor(24720.2949, grad_fn=<AddBackward0>)\n",
            "tensor(25428.3125, grad_fn=<AddBackward0>)\n",
            "tensor(26097.2676, grad_fn=<AddBackward0>)\n",
            "tensor(24931.0234, grad_fn=<AddBackward0>)\n",
            "tensor(25163.8984, grad_fn=<AddBackward0>)\n",
            "tensor(25116.2930, grad_fn=<AddBackward0>)\n",
            "tensor(25964.1094, grad_fn=<AddBackward0>)\n",
            "tensor(24917.2324, grad_fn=<AddBackward0>)\n",
            "tensor(25360.1992, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [90/469], Reconst Loss: 171.5854, KL Div: 1.7694\n",
            "tensor(25018.8281, grad_fn=<AddBackward0>)\n",
            "tensor(24526.8203, grad_fn=<AddBackward0>)\n",
            "tensor(25191.4785, grad_fn=<AddBackward0>)\n",
            "tensor(24863.1035, grad_fn=<AddBackward0>)\n",
            "tensor(24660.8535, grad_fn=<AddBackward0>)\n",
            "tensor(26117.8867, grad_fn=<AddBackward0>)\n",
            "tensor(24998.6504, grad_fn=<AddBackward0>)\n",
            "tensor(25921.3711, grad_fn=<AddBackward0>)\n",
            "tensor(25835.4453, grad_fn=<AddBackward0>)\n",
            "tensor(24373.8203, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [100/469], Reconst Loss: 160.1423, KL Div: 2.0185\n",
            "tensor(25174.7441, grad_fn=<AddBackward0>)\n",
            "tensor(24883.0527, grad_fn=<AddBackward0>)\n",
            "tensor(26326.0996, grad_fn=<AddBackward0>)\n",
            "tensor(25398.1992, grad_fn=<AddBackward0>)\n",
            "tensor(25517.5566, grad_fn=<AddBackward0>)\n",
            "tensor(24922.8047, grad_fn=<AddBackward0>)\n",
            "tensor(26423.4512, grad_fn=<AddBackward0>)\n",
            "tensor(24807.3047, grad_fn=<AddBackward0>)\n",
            "tensor(25791.7969, grad_fn=<AddBackward0>)\n",
            "tensor(26181.4707, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [110/469], Reconst Loss: 174.1286, KL Div: 2.0276\n",
            "tensor(25303.3164, grad_fn=<AddBackward0>)\n",
            "tensor(25774.9766, grad_fn=<AddBackward0>)\n",
            "tensor(24574.4746, grad_fn=<AddBackward0>)\n",
            "tensor(24793.2852, grad_fn=<AddBackward0>)\n",
            "tensor(25178.3496, grad_fn=<AddBackward0>)\n",
            "tensor(24860.1270, grad_fn=<AddBackward0>)\n",
            "tensor(25176.3047, grad_fn=<AddBackward0>)\n",
            "tensor(24896.1270, grad_fn=<AddBackward0>)\n",
            "tensor(25292.4062, grad_fn=<AddBackward0>)\n",
            "tensor(24759.6465, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [120/469], Reconst Loss: 166.4570, KL Div: 1.7985\n",
            "tensor(24660.8516, grad_fn=<AddBackward0>)\n",
            "tensor(24594.6602, grad_fn=<AddBackward0>)\n",
            "tensor(23981.2852, grad_fn=<AddBackward0>)\n",
            "tensor(24894.6484, grad_fn=<AddBackward0>)\n",
            "tensor(25810.3477, grad_fn=<AddBackward0>)\n",
            "tensor(24873.7090, grad_fn=<AddBackward0>)\n",
            "tensor(24765.1836, grad_fn=<AddBackward0>)\n",
            "tensor(25064.9629, grad_fn=<AddBackward0>)\n",
            "tensor(26108.9043, grad_fn=<AddBackward0>)\n",
            "tensor(24503.3594, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [130/469], Reconst Loss: 164.6160, KL Div: 1.7878\n",
            "tensor(25389.1562, grad_fn=<AddBackward0>)\n",
            "tensor(26254.0020, grad_fn=<AddBackward0>)\n",
            "tensor(25381.2090, grad_fn=<AddBackward0>)\n",
            "tensor(23800.6406, grad_fn=<AddBackward0>)\n",
            "tensor(25335.7480, grad_fn=<AddBackward0>)\n",
            "tensor(25977.6914, grad_fn=<AddBackward0>)\n",
            "tensor(25254.6875, grad_fn=<AddBackward0>)\n",
            "tensor(25029.5469, grad_fn=<AddBackward0>)\n",
            "tensor(25097.3633, grad_fn=<AddBackward0>)\n",
            "tensor(25429.2500, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [140/469], Reconst Loss: 170.4810, KL Div: 1.8790\n",
            "tensor(25097.7793, grad_fn=<AddBackward0>)\n",
            "tensor(25445.1523, grad_fn=<AddBackward0>)\n",
            "tensor(26104.4180, grad_fn=<AddBackward0>)\n",
            "tensor(26015.6406, grad_fn=<AddBackward0>)\n",
            "tensor(24413.9121, grad_fn=<AddBackward0>)\n",
            "tensor(24828.8906, grad_fn=<AddBackward0>)\n",
            "tensor(24923.1504, grad_fn=<AddBackward0>)\n",
            "tensor(24516.5742, grad_fn=<AddBackward0>)\n",
            "tensor(24942.8320, grad_fn=<AddBackward0>)\n",
            "tensor(25500.0156, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [150/469], Reconst Loss: 171.6686, KL Div: 1.8367\n",
            "tensor(24304.5781, grad_fn=<AddBackward0>)\n",
            "tensor(25899.2051, grad_fn=<AddBackward0>)\n",
            "tensor(25871.5898, grad_fn=<AddBackward0>)\n",
            "tensor(24866.0137, grad_fn=<AddBackward0>)\n",
            "tensor(25074.6953, grad_fn=<AddBackward0>)\n",
            "tensor(25509.3672, grad_fn=<AddBackward0>)\n",
            "tensor(24427.3203, grad_fn=<AddBackward0>)\n",
            "tensor(24574.2227, grad_fn=<AddBackward0>)\n",
            "tensor(25643.7949, grad_fn=<AddBackward0>)\n",
            "tensor(26039.9609, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [160/469], Reconst Loss: 172.6468, KL Div: 2.0527\n",
            "tensor(25497.9062, grad_fn=<AddBackward0>)\n",
            "tensor(25490.4961, grad_fn=<AddBackward0>)\n",
            "tensor(24172.3535, grad_fn=<AddBackward0>)\n",
            "tensor(25511.9590, grad_fn=<AddBackward0>)\n",
            "tensor(24872.2812, grad_fn=<AddBackward0>)\n",
            "tensor(24789.6504, grad_fn=<AddBackward0>)\n",
            "tensor(25984.5312, grad_fn=<AddBackward0>)\n",
            "tensor(24662.9023, grad_fn=<AddBackward0>)\n",
            "tensor(24759.4199, grad_fn=<AddBackward0>)\n",
            "tensor(25930.8672, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [170/469], Reconst Loss: 173.9799, KL Div: 1.9070\n",
            "tensor(25363.3477, grad_fn=<AddBackward0>)\n",
            "tensor(24553.3828, grad_fn=<AddBackward0>)\n",
            "tensor(24570.9512, grad_fn=<AddBackward0>)\n",
            "tensor(25160.1484, grad_fn=<AddBackward0>)\n",
            "tensor(24577.2402, grad_fn=<AddBackward0>)\n",
            "tensor(24709.5977, grad_fn=<AddBackward0>)\n",
            "tensor(24704.5137, grad_fn=<AddBackward0>)\n",
            "tensor(24316.6562, grad_fn=<AddBackward0>)\n",
            "tensor(25222.3457, grad_fn=<AddBackward0>)\n",
            "tensor(25908.4688, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [180/469], Reconst Loss: 173.9948, KL Div: 1.8943\n",
            "tensor(25093.2207, grad_fn=<AddBackward0>)\n",
            "tensor(25296.6738, grad_fn=<AddBackward0>)\n",
            "tensor(24370.7559, grad_fn=<AddBackward0>)\n",
            "tensor(25629.5391, grad_fn=<AddBackward0>)\n",
            "tensor(25236.9199, grad_fn=<AddBackward0>)\n",
            "tensor(25083.4727, grad_fn=<AddBackward0>)\n",
            "tensor(25153.2383, grad_fn=<AddBackward0>)\n",
            "tensor(25176.2715, grad_fn=<AddBackward0>)\n",
            "tensor(25058.9727, grad_fn=<AddBackward0>)\n",
            "tensor(24743.1641, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [190/469], Reconst Loss: 164.4725, KL Div: 1.9222\n",
            "tensor(24303.5840, grad_fn=<AddBackward0>)\n",
            "tensor(25620.2832, grad_fn=<AddBackward0>)\n",
            "tensor(25035.4648, grad_fn=<AddBackward0>)\n",
            "tensor(25818.8828, grad_fn=<AddBackward0>)\n",
            "tensor(25480.7344, grad_fn=<AddBackward0>)\n",
            "tensor(25221.7500, grad_fn=<AddBackward0>)\n",
            "tensor(24989.0332, grad_fn=<AddBackward0>)\n",
            "tensor(24391.0938, grad_fn=<AddBackward0>)\n",
            "tensor(24945.8770, grad_fn=<AddBackward0>)\n",
            "tensor(24983.8438, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [200/469], Reconst Loss: 168.4927, KL Div: 1.7796\n",
            "tensor(25829.6367, grad_fn=<AddBackward0>)\n",
            "tensor(25934.6855, grad_fn=<AddBackward0>)\n",
            "tensor(25090.1074, grad_fn=<AddBackward0>)\n",
            "tensor(24833.2402, grad_fn=<AddBackward0>)\n",
            "tensor(25702.0332, grad_fn=<AddBackward0>)\n",
            "tensor(24501.0020, grad_fn=<AddBackward0>)\n",
            "tensor(24775.2715, grad_fn=<AddBackward0>)\n",
            "tensor(24772.6406, grad_fn=<AddBackward0>)\n",
            "tensor(25025.5684, grad_fn=<AddBackward0>)\n",
            "tensor(25578.9551, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [210/469], Reconst Loss: 172.8528, KL Div: 1.7988\n",
            "tensor(25149.4551, grad_fn=<AddBackward0>)\n",
            "tensor(24916.6191, grad_fn=<AddBackward0>)\n",
            "tensor(24616.1055, grad_fn=<AddBackward0>)\n",
            "tensor(25304.9199, grad_fn=<AddBackward0>)\n",
            "tensor(25516.8770, grad_fn=<AddBackward0>)\n",
            "tensor(25504.1055, grad_fn=<AddBackward0>)\n",
            "tensor(26052.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25344.3418, grad_fn=<AddBackward0>)\n",
            "tensor(24861.9766, grad_fn=<AddBackward0>)\n",
            "tensor(24730.9180, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [220/469], Reconst Loss: 163.8839, KL Div: 1.9551\n",
            "tensor(24883.1211, grad_fn=<AddBackward0>)\n",
            "tensor(25133.2930, grad_fn=<AddBackward0>)\n",
            "tensor(24971.4727, grad_fn=<AddBackward0>)\n",
            "tensor(25518.8086, grad_fn=<AddBackward0>)\n",
            "tensor(25929.7793, grad_fn=<AddBackward0>)\n",
            "tensor(25332.0742, grad_fn=<AddBackward0>)\n",
            "tensor(25191.2480, grad_fn=<AddBackward0>)\n",
            "tensor(25643.4668, grad_fn=<AddBackward0>)\n",
            "tensor(25121.0215, grad_fn=<AddBackward0>)\n",
            "tensor(25137.4727, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [230/469], Reconst Loss: 167.7836, KL Div: 1.9069\n",
            "tensor(24903.2520, grad_fn=<AddBackward0>)\n",
            "tensor(24886.3633, grad_fn=<AddBackward0>)\n",
            "tensor(24935.2520, grad_fn=<AddBackward0>)\n",
            "tensor(25047.9434, grad_fn=<AddBackward0>)\n",
            "tensor(24646.3574, grad_fn=<AddBackward0>)\n",
            "tensor(25529.0195, grad_fn=<AddBackward0>)\n",
            "tensor(25519.7715, grad_fn=<AddBackward0>)\n",
            "tensor(24695.1504, grad_fn=<AddBackward0>)\n",
            "tensor(26002.6426, grad_fn=<AddBackward0>)\n",
            "tensor(24417.7031, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [240/469], Reconst Loss: 162.0398, KL Div: 1.9149\n",
            "tensor(24812.5781, grad_fn=<AddBackward0>)\n",
            "tensor(25163.0820, grad_fn=<AddBackward0>)\n",
            "tensor(24805.5547, grad_fn=<AddBackward0>)\n",
            "tensor(25388.3984, grad_fn=<AddBackward0>)\n",
            "tensor(25294.3809, grad_fn=<AddBackward0>)\n",
            "tensor(25092.7852, grad_fn=<AddBackward0>)\n",
            "tensor(25648.4512, grad_fn=<AddBackward0>)\n",
            "tensor(25922.2168, grad_fn=<AddBackward0>)\n",
            "tensor(25555.3164, grad_fn=<AddBackward0>)\n",
            "tensor(25626.4531, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [250/469], Reconst Loss: 174.8652, KL Div: 1.6894\n",
            "tensor(24936.2285, grad_fn=<AddBackward0>)\n",
            "tensor(25197.7148, grad_fn=<AddBackward0>)\n",
            "tensor(25277.4531, grad_fn=<AddBackward0>)\n",
            "tensor(24364.6738, grad_fn=<AddBackward0>)\n",
            "tensor(25804.8438, grad_fn=<AddBackward0>)\n",
            "tensor(24980.8008, grad_fn=<AddBackward0>)\n",
            "tensor(25513.9668, grad_fn=<AddBackward0>)\n",
            "tensor(25356.0781, grad_fn=<AddBackward0>)\n",
            "tensor(25393.5684, grad_fn=<AddBackward0>)\n",
            "tensor(24730.4082, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [260/469], Reconst Loss: 165.3622, KL Div: 1.8563\n",
            "tensor(24740.7031, grad_fn=<AddBackward0>)\n",
            "tensor(24507.7441, grad_fn=<AddBackward0>)\n",
            "tensor(26011.7812, grad_fn=<AddBackward0>)\n",
            "tensor(25218.9727, grad_fn=<AddBackward0>)\n",
            "tensor(25081.2402, grad_fn=<AddBackward0>)\n",
            "tensor(25746.2910, grad_fn=<AddBackward0>)\n",
            "tensor(25962.4453, grad_fn=<AddBackward0>)\n",
            "tensor(25536.6602, grad_fn=<AddBackward0>)\n",
            "tensor(25696.8145, grad_fn=<AddBackward0>)\n",
            "tensor(25530.7031, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [270/469], Reconst Loss: 171.7165, KL Div: 1.8495\n",
            "tensor(24995.6348, grad_fn=<AddBackward0>)\n",
            "tensor(24922.9023, grad_fn=<AddBackward0>)\n",
            "tensor(25358.4727, grad_fn=<AddBackward0>)\n",
            "tensor(25267.5547, grad_fn=<AddBackward0>)\n",
            "tensor(25798.7969, grad_fn=<AddBackward0>)\n",
            "tensor(25292.9180, grad_fn=<AddBackward0>)\n",
            "tensor(26255.3848, grad_fn=<AddBackward0>)\n",
            "tensor(24338.6699, grad_fn=<AddBackward0>)\n",
            "tensor(25990.9062, grad_fn=<AddBackward0>)\n",
            "tensor(25293.3320, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [280/469], Reconst Loss: 168.5051, KL Div: 1.9399\n",
            "tensor(24813.6777, grad_fn=<AddBackward0>)\n",
            "tensor(25495.7773, grad_fn=<AddBackward0>)\n",
            "tensor(24894.7578, grad_fn=<AddBackward0>)\n",
            "tensor(24848.9180, grad_fn=<AddBackward0>)\n",
            "tensor(23962.8867, grad_fn=<AddBackward0>)\n",
            "tensor(24864.6719, grad_fn=<AddBackward0>)\n",
            "tensor(24675.5527, grad_fn=<AddBackward0>)\n",
            "tensor(25055.8926, grad_fn=<AddBackward0>)\n",
            "tensor(25399.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25841.1445, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [290/469], Reconst Loss: 175.5847, KL Div: 1.7533\n",
            "tensor(25483.0137, grad_fn=<AddBackward0>)\n",
            "tensor(24266.9688, grad_fn=<AddBackward0>)\n",
            "tensor(25722.7402, grad_fn=<AddBackward0>)\n",
            "tensor(25479.9785, grad_fn=<AddBackward0>)\n",
            "tensor(25187.1152, grad_fn=<AddBackward0>)\n",
            "tensor(25618.0352, grad_fn=<AddBackward0>)\n",
            "tensor(26334.0625, grad_fn=<AddBackward0>)\n",
            "tensor(24956.5938, grad_fn=<AddBackward0>)\n",
            "tensor(25073.6738, grad_fn=<AddBackward0>)\n",
            "tensor(24529.2148, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [300/469], Reconst Loss: 163.8445, KL Div: 1.8527\n",
            "tensor(25055.2266, grad_fn=<AddBackward0>)\n",
            "tensor(25156.8672, grad_fn=<AddBackward0>)\n",
            "tensor(24735.3359, grad_fn=<AddBackward0>)\n",
            "tensor(25515.5098, grad_fn=<AddBackward0>)\n",
            "tensor(26008.3535, grad_fn=<AddBackward0>)\n",
            "tensor(25043.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25467.2812, grad_fn=<AddBackward0>)\n",
            "tensor(24540.5664, grad_fn=<AddBackward0>)\n",
            "tensor(24490.1055, grad_fn=<AddBackward0>)\n",
            "tensor(25715.6016, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [310/469], Reconst Loss: 172.6600, KL Div: 1.8829\n",
            "tensor(24399.8105, grad_fn=<AddBackward0>)\n",
            "tensor(25069.5312, grad_fn=<AddBackward0>)\n",
            "tensor(25357.8477, grad_fn=<AddBackward0>)\n",
            "tensor(25345.3086, grad_fn=<AddBackward0>)\n",
            "tensor(26076.0820, grad_fn=<AddBackward0>)\n",
            "tensor(24717.4590, grad_fn=<AddBackward0>)\n",
            "tensor(25483.1797, grad_fn=<AddBackward0>)\n",
            "tensor(25561.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25110.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25300.0664, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [320/469], Reconst Loss: 169.1951, KL Div: 1.8974\n",
            "tensor(24674.4648, grad_fn=<AddBackward0>)\n",
            "tensor(24689.3281, grad_fn=<AddBackward0>)\n",
            "tensor(24632.8027, grad_fn=<AddBackward0>)\n",
            "tensor(24917.3027, grad_fn=<AddBackward0>)\n",
            "tensor(24485.4023, grad_fn=<AddBackward0>)\n",
            "tensor(24436.4316, grad_fn=<AddBackward0>)\n",
            "tensor(24508.2754, grad_fn=<AddBackward0>)\n",
            "tensor(25447.6035, grad_fn=<AddBackward0>)\n",
            "tensor(25234.3809, grad_fn=<AddBackward0>)\n",
            "tensor(24644.8789, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [330/469], Reconst Loss: 165.0543, KL Div: 1.8323\n",
            "tensor(25896.7246, grad_fn=<AddBackward0>)\n",
            "tensor(24707.3438, grad_fn=<AddBackward0>)\n",
            "tensor(26184.0312, grad_fn=<AddBackward0>)\n",
            "tensor(25485.1035, grad_fn=<AddBackward0>)\n",
            "tensor(24546.2285, grad_fn=<AddBackward0>)\n",
            "tensor(25623.4219, grad_fn=<AddBackward0>)\n",
            "tensor(24429.0352, grad_fn=<AddBackward0>)\n",
            "tensor(25785.3359, grad_fn=<AddBackward0>)\n",
            "tensor(24915.1172, grad_fn=<AddBackward0>)\n",
            "tensor(25503.5078, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [340/469], Reconst Loss: 170.3606, KL Div: 1.9257\n",
            "tensor(25823.1895, grad_fn=<AddBackward0>)\n",
            "tensor(24261.2324, grad_fn=<AddBackward0>)\n",
            "tensor(24563.9414, grad_fn=<AddBackward0>)\n",
            "tensor(24377.7891, grad_fn=<AddBackward0>)\n",
            "tensor(24827.4688, grad_fn=<AddBackward0>)\n",
            "tensor(25080.7285, grad_fn=<AddBackward0>)\n",
            "tensor(24953.8496, grad_fn=<AddBackward0>)\n",
            "tensor(26370.6367, grad_fn=<AddBackward0>)\n",
            "tensor(26138.8906, grad_fn=<AddBackward0>)\n",
            "tensor(24700.7910, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [350/469], Reconst Loss: 165.5292, KL Div: 1.8297\n",
            "tensor(24558.4062, grad_fn=<AddBackward0>)\n",
            "tensor(25228.0762, grad_fn=<AddBackward0>)\n",
            "tensor(25745.3047, grad_fn=<AddBackward0>)\n",
            "tensor(24602.7363, grad_fn=<AddBackward0>)\n",
            "tensor(25535.7559, grad_fn=<AddBackward0>)\n",
            "tensor(24405.0664, grad_fn=<AddBackward0>)\n",
            "tensor(24361.8281, grad_fn=<AddBackward0>)\n",
            "tensor(25246.1992, grad_fn=<AddBackward0>)\n",
            "tensor(25738.7500, grad_fn=<AddBackward0>)\n",
            "tensor(26164.4746, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [360/469], Reconst Loss: 176.2771, KL Div: 1.8755\n",
            "tensor(24202.7227, grad_fn=<AddBackward0>)\n",
            "tensor(25452.2832, grad_fn=<AddBackward0>)\n",
            "tensor(24658.2246, grad_fn=<AddBackward0>)\n",
            "tensor(24737.1074, grad_fn=<AddBackward0>)\n",
            "tensor(25227.3594, grad_fn=<AddBackward0>)\n",
            "tensor(26855.8086, grad_fn=<AddBackward0>)\n",
            "tensor(25852.4941, grad_fn=<AddBackward0>)\n",
            "tensor(25549.5645, grad_fn=<AddBackward0>)\n",
            "tensor(25302.5547, grad_fn=<AddBackward0>)\n",
            "tensor(25133.8164, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [370/469], Reconst Loss: 167.7392, KL Div: 1.9079\n",
            "tensor(25415.6074, grad_fn=<AddBackward0>)\n",
            "tensor(25087.4492, grad_fn=<AddBackward0>)\n",
            "tensor(26114.7148, grad_fn=<AddBackward0>)\n",
            "tensor(24116.6855, grad_fn=<AddBackward0>)\n",
            "tensor(24678.9023, grad_fn=<AddBackward0>)\n",
            "tensor(24594.9688, grad_fn=<AddBackward0>)\n",
            "tensor(25753.9238, grad_fn=<AddBackward0>)\n",
            "tensor(25298.3750, grad_fn=<AddBackward0>)\n",
            "tensor(25058.6992, grad_fn=<AddBackward0>)\n",
            "tensor(25532.9062, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [380/469], Reconst Loss: 170.6005, KL Div: 1.9250\n",
            "tensor(25590.0527, grad_fn=<AddBackward0>)\n",
            "tensor(25270.7520, grad_fn=<AddBackward0>)\n",
            "tensor(24858.9844, grad_fn=<AddBackward0>)\n",
            "tensor(24968.5156, grad_fn=<AddBackward0>)\n",
            "tensor(24749.3418, grad_fn=<AddBackward0>)\n",
            "tensor(26250.1094, grad_fn=<AddBackward0>)\n",
            "tensor(25829.9688, grad_fn=<AddBackward0>)\n",
            "tensor(25020.7949, grad_fn=<AddBackward0>)\n",
            "tensor(25886.9238, grad_fn=<AddBackward0>)\n",
            "tensor(25295.0781, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [390/469], Reconst Loss: 167.9017, KL Div: 1.9811\n",
            "tensor(25359.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25341.6836, grad_fn=<AddBackward0>)\n",
            "tensor(25283.5195, grad_fn=<AddBackward0>)\n",
            "tensor(25178.6719, grad_fn=<AddBackward0>)\n",
            "tensor(24618.3359, grad_fn=<AddBackward0>)\n",
            "tensor(25439.5918, grad_fn=<AddBackward0>)\n",
            "tensor(24631.0098, grad_fn=<AddBackward0>)\n",
            "tensor(26150.4727, grad_fn=<AddBackward0>)\n",
            "tensor(26100.6582, grad_fn=<AddBackward0>)\n",
            "tensor(25138.3906, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [400/469], Reconst Loss: 169.0910, KL Div: 1.8202\n",
            "tensor(26159.7207, grad_fn=<AddBackward0>)\n",
            "tensor(24596.9512, grad_fn=<AddBackward0>)\n",
            "tensor(24756.4395, grad_fn=<AddBackward0>)\n",
            "tensor(24566.4844, grad_fn=<AddBackward0>)\n",
            "tensor(26541.8633, grad_fn=<AddBackward0>)\n",
            "tensor(25327.6680, grad_fn=<AddBackward0>)\n",
            "tensor(24269.6270, grad_fn=<AddBackward0>)\n",
            "tensor(24930.3770, grad_fn=<AddBackward0>)\n",
            "tensor(25229.4043, grad_fn=<AddBackward0>)\n",
            "tensor(25531.5391, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [410/469], Reconst Loss: 169.9455, KL Div: 1.9680\n",
            "tensor(25432.6445, grad_fn=<AddBackward0>)\n",
            "tensor(25537.0820, grad_fn=<AddBackward0>)\n",
            "tensor(24349.1016, grad_fn=<AddBackward0>)\n",
            "tensor(24747.0215, grad_fn=<AddBackward0>)\n",
            "tensor(25197.7656, grad_fn=<AddBackward0>)\n",
            "tensor(24383.9141, grad_fn=<AddBackward0>)\n",
            "tensor(25459.0840, grad_fn=<AddBackward0>)\n",
            "tensor(25653.0312, grad_fn=<AddBackward0>)\n",
            "tensor(24587.5840, grad_fn=<AddBackward0>)\n",
            "tensor(25183.9531, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [420/469], Reconst Loss: 166.9206, KL Div: 1.9886\n",
            "tensor(25318.9297, grad_fn=<AddBackward0>)\n",
            "tensor(25025.0449, grad_fn=<AddBackward0>)\n",
            "tensor(24521.7266, grad_fn=<AddBackward0>)\n",
            "tensor(24564.9453, grad_fn=<AddBackward0>)\n",
            "tensor(25834.4316, grad_fn=<AddBackward0>)\n",
            "tensor(25550.4980, grad_fn=<AddBackward0>)\n",
            "tensor(25465.4062, grad_fn=<AddBackward0>)\n",
            "tensor(24847.3262, grad_fn=<AddBackward0>)\n",
            "tensor(25776.5078, grad_fn=<AddBackward0>)\n",
            "tensor(25048.5742, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [430/469], Reconst Loss: 167.6802, KL Div: 1.8675\n",
            "tensor(24627.1797, grad_fn=<AddBackward0>)\n",
            "tensor(24309.1875, grad_fn=<AddBackward0>)\n",
            "tensor(26460.2910, grad_fn=<AddBackward0>)\n",
            "tensor(26615.7207, grad_fn=<AddBackward0>)\n",
            "tensor(25654.1953, grad_fn=<AddBackward0>)\n",
            "tensor(25855.5352, grad_fn=<AddBackward0>)\n",
            "tensor(26014.5371, grad_fn=<AddBackward0>)\n",
            "tensor(25854.8926, grad_fn=<AddBackward0>)\n",
            "tensor(25695.8477, grad_fn=<AddBackward0>)\n",
            "tensor(25038.5215, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [440/469], Reconst Loss: 166.1216, KL Div: 1.9661\n",
            "tensor(25788.0312, grad_fn=<AddBackward0>)\n",
            "tensor(25255.0781, grad_fn=<AddBackward0>)\n",
            "tensor(25507.0117, grad_fn=<AddBackward0>)\n",
            "tensor(25185.8574, grad_fn=<AddBackward0>)\n",
            "tensor(25901.5977, grad_fn=<AddBackward0>)\n",
            "tensor(24761.2109, grad_fn=<AddBackward0>)\n",
            "tensor(24991.9199, grad_fn=<AddBackward0>)\n",
            "tensor(25329.7715, grad_fn=<AddBackward0>)\n",
            "tensor(24343.7832, grad_fn=<AddBackward0>)\n",
            "tensor(26196.7695, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [450/469], Reconst Loss: 177.0302, KL Div: 1.8421\n",
            "tensor(25549.6953, grad_fn=<AddBackward0>)\n",
            "tensor(24329.7500, grad_fn=<AddBackward0>)\n",
            "tensor(26564.7930, grad_fn=<AddBackward0>)\n",
            "tensor(24102.4980, grad_fn=<AddBackward0>)\n",
            "tensor(25188.1289, grad_fn=<AddBackward0>)\n",
            "tensor(25138.6211, grad_fn=<AddBackward0>)\n",
            "tensor(24988.1172, grad_fn=<AddBackward0>)\n",
            "tensor(24820.3594, grad_fn=<AddBackward0>)\n",
            "tensor(25054.7285, grad_fn=<AddBackward0>)\n",
            "tensor(26467.2246, grad_fn=<AddBackward0>)\n",
            "Epoch[21/25], Step [460/469], Reconst Loss: 177.9539, KL Div: 1.9214\n",
            "tensor(26130.3555, grad_fn=<AddBackward0>)\n",
            "tensor(24301.2930, grad_fn=<AddBackward0>)\n",
            "tensor(24753.8691, grad_fn=<AddBackward0>)\n",
            "tensor(25446.4922, grad_fn=<AddBackward0>)\n",
            "tensor(24924.7383, grad_fn=<AddBackward0>)\n",
            "tensor(25406.1836, grad_fn=<AddBackward0>)\n",
            "tensor(25934.4219, grad_fn=<AddBackward0>)\n",
            "tensor(26040.1074, grad_fn=<AddBackward0>)\n",
            "tensor(19502.5508, grad_fn=<AddBackward0>)\n",
            "tensor(24630.5664, grad_fn=<AddBackward0>)\n",
            "tensor(25314.5234, grad_fn=<AddBackward0>)\n",
            "tensor(25059.7070, grad_fn=<AddBackward0>)\n",
            "tensor(24005.7129, grad_fn=<AddBackward0>)\n",
            "tensor(25834.8555, grad_fn=<AddBackward0>)\n",
            "tensor(25843.3691, grad_fn=<AddBackward0>)\n",
            "tensor(25487.5859, grad_fn=<AddBackward0>)\n",
            "tensor(25213.4082, grad_fn=<AddBackward0>)\n",
            "tensor(24708.2383, grad_fn=<AddBackward0>)\n",
            "tensor(25106.5098, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [10/469], Reconst Loss: 168.2525, KL Div: 1.8595\n",
            "tensor(24921.2930, grad_fn=<AddBackward0>)\n",
            "tensor(26155.6797, grad_fn=<AddBackward0>)\n",
            "tensor(25527.9629, grad_fn=<AddBackward0>)\n",
            "tensor(24984.1855, grad_fn=<AddBackward0>)\n",
            "tensor(25256.5488, grad_fn=<AddBackward0>)\n",
            "tensor(25268.2402, grad_fn=<AddBackward0>)\n",
            "tensor(24613.5195, grad_fn=<AddBackward0>)\n",
            "tensor(24951.3145, grad_fn=<AddBackward0>)\n",
            "tensor(25201.9316, grad_fn=<AddBackward0>)\n",
            "tensor(25145.9023, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [20/469], Reconst Loss: 167.9607, KL Div: 1.8994\n",
            "tensor(25546.2988, grad_fn=<AddBackward0>)\n",
            "tensor(24769.0020, grad_fn=<AddBackward0>)\n",
            "tensor(25449.0156, grad_fn=<AddBackward0>)\n",
            "tensor(25559.4082, grad_fn=<AddBackward0>)\n",
            "tensor(24874.0449, grad_fn=<AddBackward0>)\n",
            "tensor(24907.6191, grad_fn=<AddBackward0>)\n",
            "tensor(24897.8125, grad_fn=<AddBackward0>)\n",
            "tensor(25718.2305, grad_fn=<AddBackward0>)\n",
            "tensor(27026.3770, grad_fn=<AddBackward0>)\n",
            "tensor(25781.7285, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [30/469], Reconst Loss: 173.3672, KL Div: 1.8702\n",
            "tensor(24374.9727, grad_fn=<AddBackward0>)\n",
            "tensor(23864.4590, grad_fn=<AddBackward0>)\n",
            "tensor(25329.6562, grad_fn=<AddBackward0>)\n",
            "tensor(25302.9062, grad_fn=<AddBackward0>)\n",
            "tensor(25000.4492, grad_fn=<AddBackward0>)\n",
            "tensor(24586.6875, grad_fn=<AddBackward0>)\n",
            "tensor(24722.3320, grad_fn=<AddBackward0>)\n",
            "tensor(24672.1016, grad_fn=<AddBackward0>)\n",
            "tensor(24823.1113, grad_fn=<AddBackward0>)\n",
            "tensor(25518.5586, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [40/469], Reconst Loss: 171.5266, KL Div: 1.8558\n",
            "tensor(26154.9297, grad_fn=<AddBackward0>)\n",
            "tensor(25525.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25850.8965, grad_fn=<AddBackward0>)\n",
            "tensor(24345.7012, grad_fn=<AddBackward0>)\n",
            "tensor(24790.3184, grad_fn=<AddBackward0>)\n",
            "tensor(24697.5977, grad_fn=<AddBackward0>)\n",
            "tensor(25266.1230, grad_fn=<AddBackward0>)\n",
            "tensor(25109.3555, grad_fn=<AddBackward0>)\n",
            "tensor(25589.8633, grad_fn=<AddBackward0>)\n",
            "tensor(25173.3086, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [50/469], Reconst Loss: 169.0487, KL Div: 1.8412\n",
            "tensor(25232.9121, grad_fn=<AddBackward0>)\n",
            "tensor(25028.0820, grad_fn=<AddBackward0>)\n",
            "tensor(25216.4492, grad_fn=<AddBackward0>)\n",
            "tensor(25115.4883, grad_fn=<AddBackward0>)\n",
            "tensor(25294.4375, grad_fn=<AddBackward0>)\n",
            "tensor(25397.8906, grad_fn=<AddBackward0>)\n",
            "tensor(25090.8984, grad_fn=<AddBackward0>)\n",
            "tensor(25141.5566, grad_fn=<AddBackward0>)\n",
            "tensor(24496.1406, grad_fn=<AddBackward0>)\n",
            "tensor(25435.5430, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [60/469], Reconst Loss: 170.4920, KL Div: 1.8815\n",
            "tensor(25015.2031, grad_fn=<AddBackward0>)\n",
            "tensor(24519.5898, grad_fn=<AddBackward0>)\n",
            "tensor(25526.8086, grad_fn=<AddBackward0>)\n",
            "tensor(25131.3789, grad_fn=<AddBackward0>)\n",
            "tensor(24450.5938, grad_fn=<AddBackward0>)\n",
            "tensor(25212.7539, grad_fn=<AddBackward0>)\n",
            "tensor(25495.8125, grad_fn=<AddBackward0>)\n",
            "tensor(25536.9707, grad_fn=<AddBackward0>)\n",
            "tensor(25960.0566, grad_fn=<AddBackward0>)\n",
            "tensor(25838.1348, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [70/469], Reconst Loss: 174.4665, KL Div: 1.8263\n",
            "tensor(24145.3066, grad_fn=<AddBackward0>)\n",
            "tensor(24720.0156, grad_fn=<AddBackward0>)\n",
            "tensor(26119.2227, grad_fn=<AddBackward0>)\n",
            "tensor(24870.7773, grad_fn=<AddBackward0>)\n",
            "tensor(25425.4258, grad_fn=<AddBackward0>)\n",
            "tensor(26077.7480, grad_fn=<AddBackward0>)\n",
            "tensor(24945.8672, grad_fn=<AddBackward0>)\n",
            "tensor(25544.8262, grad_fn=<AddBackward0>)\n",
            "tensor(25762.2285, grad_fn=<AddBackward0>)\n",
            "tensor(25091.0977, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [80/469], Reconst Loss: 168.5397, KL Div: 1.8323\n",
            "tensor(24704.3320, grad_fn=<AddBackward0>)\n",
            "tensor(25341.9238, grad_fn=<AddBackward0>)\n",
            "tensor(25394.4375, grad_fn=<AddBackward0>)\n",
            "tensor(25463.0137, grad_fn=<AddBackward0>)\n",
            "tensor(24874.4414, grad_fn=<AddBackward0>)\n",
            "tensor(24964.5977, grad_fn=<AddBackward0>)\n",
            "tensor(24586.9805, grad_fn=<AddBackward0>)\n",
            "tensor(24024.0469, grad_fn=<AddBackward0>)\n",
            "tensor(26376.4277, grad_fn=<AddBackward0>)\n",
            "tensor(24891.7598, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [90/469], Reconst Loss: 169.3126, KL Div: 1.6770\n",
            "tensor(25118.5371, grad_fn=<AddBackward0>)\n",
            "tensor(26180.0801, grad_fn=<AddBackward0>)\n",
            "tensor(25280.4355, grad_fn=<AddBackward0>)\n",
            "tensor(25661.9395, grad_fn=<AddBackward0>)\n",
            "tensor(24460.8496, grad_fn=<AddBackward0>)\n",
            "tensor(24575.2637, grad_fn=<AddBackward0>)\n",
            "tensor(24825.9355, grad_fn=<AddBackward0>)\n",
            "tensor(24967.5391, grad_fn=<AddBackward0>)\n",
            "tensor(25464.5312, grad_fn=<AddBackward0>)\n",
            "tensor(24986.2285, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [100/469], Reconst Loss: 164.6573, KL Div: 2.0365\n",
            "tensor(25170.1875, grad_fn=<AddBackward0>)\n",
            "tensor(25760.3379, grad_fn=<AddBackward0>)\n",
            "tensor(24723.1055, grad_fn=<AddBackward0>)\n",
            "tensor(25072.7129, grad_fn=<AddBackward0>)\n",
            "tensor(25511.5000, grad_fn=<AddBackward0>)\n",
            "tensor(25460.8379, grad_fn=<AddBackward0>)\n",
            "tensor(25400.6641, grad_fn=<AddBackward0>)\n",
            "tensor(25191.3125, grad_fn=<AddBackward0>)\n",
            "tensor(24098.7109, grad_fn=<AddBackward0>)\n",
            "tensor(24907.1543, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [110/469], Reconst Loss: 165.9765, KL Div: 1.9074\n",
            "tensor(24956.4570, grad_fn=<AddBackward0>)\n",
            "tensor(25358.6836, grad_fn=<AddBackward0>)\n",
            "tensor(25699.9004, grad_fn=<AddBackward0>)\n",
            "tensor(25662.9238, grad_fn=<AddBackward0>)\n",
            "tensor(25992.1621, grad_fn=<AddBackward0>)\n",
            "tensor(25463.7051, grad_fn=<AddBackward0>)\n",
            "tensor(24882.2910, grad_fn=<AddBackward0>)\n",
            "tensor(24640.7129, grad_fn=<AddBackward0>)\n",
            "tensor(25467.3555, grad_fn=<AddBackward0>)\n",
            "tensor(25530.6582, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [120/469], Reconst Loss: 172.3721, KL Div: 1.8057\n",
            "tensor(25166.8086, grad_fn=<AddBackward0>)\n",
            "tensor(25391.8203, grad_fn=<AddBackward0>)\n",
            "tensor(25462.2188, grad_fn=<AddBackward0>)\n",
            "tensor(25888.3652, grad_fn=<AddBackward0>)\n",
            "tensor(24443.5820, grad_fn=<AddBackward0>)\n",
            "tensor(25438.2090, grad_fn=<AddBackward0>)\n",
            "tensor(24789.5195, grad_fn=<AddBackward0>)\n",
            "tensor(25102.9922, grad_fn=<AddBackward0>)\n",
            "tensor(25250.1055, grad_fn=<AddBackward0>)\n",
            "tensor(24885.6602, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [130/469], Reconst Loss: 165.9938, KL Div: 1.8950\n",
            "tensor(24943.1777, grad_fn=<AddBackward0>)\n",
            "tensor(25059.6289, grad_fn=<AddBackward0>)\n",
            "tensor(25708.7148, grad_fn=<AddBackward0>)\n",
            "tensor(25470.2598, grad_fn=<AddBackward0>)\n",
            "tensor(26097.1641, grad_fn=<AddBackward0>)\n",
            "tensor(25710.9512, grad_fn=<AddBackward0>)\n",
            "tensor(25713.8809, grad_fn=<AddBackward0>)\n",
            "tensor(25266.5527, grad_fn=<AddBackward0>)\n",
            "tensor(25677.7188, grad_fn=<AddBackward0>)\n",
            "tensor(26117.7363, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [140/469], Reconst Loss: 177.0984, KL Div: 1.7964\n",
            "tensor(24661.7637, grad_fn=<AddBackward0>)\n",
            "tensor(25111.6152, grad_fn=<AddBackward0>)\n",
            "tensor(24761.8301, grad_fn=<AddBackward0>)\n",
            "tensor(25247.1582, grad_fn=<AddBackward0>)\n",
            "tensor(25308.1816, grad_fn=<AddBackward0>)\n",
            "tensor(25140.0664, grad_fn=<AddBackward0>)\n",
            "tensor(25349.0488, grad_fn=<AddBackward0>)\n",
            "tensor(24674.4648, grad_fn=<AddBackward0>)\n",
            "tensor(25391.7949, grad_fn=<AddBackward0>)\n",
            "tensor(24184.6660, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [150/469], Reconst Loss: 161.4265, KL Div: 1.8344\n",
            "tensor(26091.7812, grad_fn=<AddBackward0>)\n",
            "tensor(24721.3262, grad_fn=<AddBackward0>)\n",
            "tensor(24724.9180, grad_fn=<AddBackward0>)\n",
            "tensor(24469.6777, grad_fn=<AddBackward0>)\n",
            "tensor(23838.6797, grad_fn=<AddBackward0>)\n",
            "tensor(25043.0430, grad_fn=<AddBackward0>)\n",
            "tensor(25429.0078, grad_fn=<AddBackward0>)\n",
            "tensor(25316.9707, grad_fn=<AddBackward0>)\n",
            "tensor(24827.6934, grad_fn=<AddBackward0>)\n",
            "tensor(25432.6621, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [160/469], Reconst Loss: 170.9862, KL Div: 1.8471\n",
            "tensor(25029.9434, grad_fn=<AddBackward0>)\n",
            "tensor(26281.1465, grad_fn=<AddBackward0>)\n",
            "tensor(24660.9707, grad_fn=<AddBackward0>)\n",
            "tensor(24495.0605, grad_fn=<AddBackward0>)\n",
            "tensor(25479.8359, grad_fn=<AddBackward0>)\n",
            "tensor(25692.3320, grad_fn=<AddBackward0>)\n",
            "tensor(26203.0410, grad_fn=<AddBackward0>)\n",
            "tensor(25020.5234, grad_fn=<AddBackward0>)\n",
            "tensor(26644.5020, grad_fn=<AddBackward0>)\n",
            "tensor(25213.4746, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [170/469], Reconst Loss: 167.3112, KL Div: 1.9779\n",
            "tensor(24070.0938, grad_fn=<AddBackward0>)\n",
            "tensor(25315.6484, grad_fn=<AddBackward0>)\n",
            "tensor(25320.1836, grad_fn=<AddBackward0>)\n",
            "tensor(26210.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25646.5527, grad_fn=<AddBackward0>)\n",
            "tensor(24892.1016, grad_fn=<AddBackward0>)\n",
            "tensor(25063.5938, grad_fn=<AddBackward0>)\n",
            "tensor(25032.1953, grad_fn=<AddBackward0>)\n",
            "tensor(25355.0488, grad_fn=<AddBackward0>)\n",
            "tensor(25362.1914, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [180/469], Reconst Loss: 170.0606, KL Div: 1.8721\n",
            "tensor(25068.9434, grad_fn=<AddBackward0>)\n",
            "tensor(25706.7969, grad_fn=<AddBackward0>)\n",
            "tensor(25913.9551, grad_fn=<AddBackward0>)\n",
            "tensor(25493.3379, grad_fn=<AddBackward0>)\n",
            "tensor(24350.2598, grad_fn=<AddBackward0>)\n",
            "tensor(25434.0801, grad_fn=<AddBackward0>)\n",
            "tensor(24746.0098, grad_fn=<AddBackward0>)\n",
            "tensor(25898.8301, grad_fn=<AddBackward0>)\n",
            "tensor(25101.0762, grad_fn=<AddBackward0>)\n",
            "tensor(26363.2617, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [190/469], Reconst Loss: 178.1793, KL Div: 1.8522\n",
            "tensor(25333.1035, grad_fn=<AddBackward0>)\n",
            "tensor(25533.8027, grad_fn=<AddBackward0>)\n",
            "tensor(27162.4297, grad_fn=<AddBackward0>)\n",
            "tensor(25251.1875, grad_fn=<AddBackward0>)\n",
            "tensor(25510.5605, grad_fn=<AddBackward0>)\n",
            "tensor(25293.7578, grad_fn=<AddBackward0>)\n",
            "tensor(25362.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25192.0742, grad_fn=<AddBackward0>)\n",
            "tensor(26311.4668, grad_fn=<AddBackward0>)\n",
            "tensor(25029.5117, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [200/469], Reconst Loss: 166.6794, KL Div: 1.9242\n",
            "tensor(24083.3379, grad_fn=<AddBackward0>)\n",
            "tensor(25048.7363, grad_fn=<AddBackward0>)\n",
            "tensor(25845.7090, grad_fn=<AddBackward0>)\n",
            "tensor(25466.3633, grad_fn=<AddBackward0>)\n",
            "tensor(25419.6562, grad_fn=<AddBackward0>)\n",
            "tensor(24745.2305, grad_fn=<AddBackward0>)\n",
            "tensor(24346.3145, grad_fn=<AddBackward0>)\n",
            "tensor(24699.8555, grad_fn=<AddBackward0>)\n",
            "tensor(24373.7461, grad_fn=<AddBackward0>)\n",
            "tensor(26325.2344, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [210/469], Reconst Loss: 178.1909, KL Div: 1.8317\n",
            "tensor(24967.6094, grad_fn=<AddBackward0>)\n",
            "tensor(25509.2227, grad_fn=<AddBackward0>)\n",
            "tensor(25319.1914, grad_fn=<AddBackward0>)\n",
            "tensor(24714.1836, grad_fn=<AddBackward0>)\n",
            "tensor(25114.3242, grad_fn=<AddBackward0>)\n",
            "tensor(25603.8418, grad_fn=<AddBackward0>)\n",
            "tensor(26032.4941, grad_fn=<AddBackward0>)\n",
            "tensor(25143.2090, grad_fn=<AddBackward0>)\n",
            "tensor(25688.4297, grad_fn=<AddBackward0>)\n",
            "tensor(26134.6523, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [220/469], Reconst Loss: 176.4478, KL Div: 1.8486\n",
            "tensor(25431.5859, grad_fn=<AddBackward0>)\n",
            "tensor(25072.5391, grad_fn=<AddBackward0>)\n",
            "tensor(24049.0098, grad_fn=<AddBackward0>)\n",
            "tensor(24936.5859, grad_fn=<AddBackward0>)\n",
            "tensor(24815.4180, grad_fn=<AddBackward0>)\n",
            "tensor(24567.1367, grad_fn=<AddBackward0>)\n",
            "tensor(25775.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25996.9414, grad_fn=<AddBackward0>)\n",
            "tensor(25733.2266, grad_fn=<AddBackward0>)\n",
            "tensor(24952.4297, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [230/469], Reconst Loss: 166.4366, KL Div: 1.9003\n",
            "tensor(25105.9336, grad_fn=<AddBackward0>)\n",
            "tensor(25533.9805, grad_fn=<AddBackward0>)\n",
            "tensor(24293.7637, grad_fn=<AddBackward0>)\n",
            "tensor(24160.2188, grad_fn=<AddBackward0>)\n",
            "tensor(25069.8203, grad_fn=<AddBackward0>)\n",
            "tensor(24476.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25192.1816, grad_fn=<AddBackward0>)\n",
            "tensor(25599.7051, grad_fn=<AddBackward0>)\n",
            "tensor(24709.7109, grad_fn=<AddBackward0>)\n",
            "tensor(25158.2617, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [240/469], Reconst Loss: 167.3846, KL Div: 1.9443\n",
            "tensor(24991.8594, grad_fn=<AddBackward0>)\n",
            "tensor(25648.7246, grad_fn=<AddBackward0>)\n",
            "tensor(25338.1621, grad_fn=<AddBackward0>)\n",
            "tensor(25243.3867, grad_fn=<AddBackward0>)\n",
            "tensor(25063.2969, grad_fn=<AddBackward0>)\n",
            "tensor(25459.4531, grad_fn=<AddBackward0>)\n",
            "tensor(25721.5391, grad_fn=<AddBackward0>)\n",
            "tensor(24895.2051, grad_fn=<AddBackward0>)\n",
            "tensor(25563.1836, grad_fn=<AddBackward0>)\n",
            "tensor(24661.2441, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [250/469], Reconst Loss: 166.1608, KL Div: 1.7670\n",
            "tensor(25366.8945, grad_fn=<AddBackward0>)\n",
            "tensor(25289., grad_fn=<AddBackward0>)\n",
            "tensor(24675.1445, grad_fn=<AddBackward0>)\n",
            "tensor(24961.5859, grad_fn=<AddBackward0>)\n",
            "tensor(25575.5410, grad_fn=<AddBackward0>)\n",
            "tensor(25402.2617, grad_fn=<AddBackward0>)\n",
            "tensor(25796.8809, grad_fn=<AddBackward0>)\n",
            "tensor(25120.0840, grad_fn=<AddBackward0>)\n",
            "tensor(24954.1895, grad_fn=<AddBackward0>)\n",
            "tensor(24361.3535, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [260/469], Reconst Loss: 160.7212, KL Div: 1.9735\n",
            "tensor(25517.5020, grad_fn=<AddBackward0>)\n",
            "tensor(25583.6250, grad_fn=<AddBackward0>)\n",
            "tensor(24845.9375, grad_fn=<AddBackward0>)\n",
            "tensor(24403.1367, grad_fn=<AddBackward0>)\n",
            "tensor(24828.4375, grad_fn=<AddBackward0>)\n",
            "tensor(24558.5391, grad_fn=<AddBackward0>)\n",
            "tensor(25506.6094, grad_fn=<AddBackward0>)\n",
            "tensor(24955.8320, grad_fn=<AddBackward0>)\n",
            "tensor(24854.8652, grad_fn=<AddBackward0>)\n",
            "tensor(25709.1699, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [270/469], Reconst Loss: 173.9194, KL Div: 1.7956\n",
            "tensor(25839.8496, grad_fn=<AddBackward0>)\n",
            "tensor(25229.9043, grad_fn=<AddBackward0>)\n",
            "tensor(25268.2930, grad_fn=<AddBackward0>)\n",
            "tensor(25176.6719, grad_fn=<AddBackward0>)\n",
            "tensor(23970.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25210.1875, grad_fn=<AddBackward0>)\n",
            "tensor(25058.3574, grad_fn=<AddBackward0>)\n",
            "tensor(24552.0977, grad_fn=<AddBackward0>)\n",
            "tensor(24620.9531, grad_fn=<AddBackward0>)\n",
            "tensor(25548.2129, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [280/469], Reconst Loss: 171.9749, KL Div: 1.8414\n",
            "tensor(25576.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25787.7988, grad_fn=<AddBackward0>)\n",
            "tensor(25393.0742, grad_fn=<AddBackward0>)\n",
            "tensor(24926.0566, grad_fn=<AddBackward0>)\n",
            "tensor(25382.1289, grad_fn=<AddBackward0>)\n",
            "tensor(25602.4980, grad_fn=<AddBackward0>)\n",
            "tensor(26329.0977, grad_fn=<AddBackward0>)\n",
            "tensor(24859.2266, grad_fn=<AddBackward0>)\n",
            "tensor(24256.4590, grad_fn=<AddBackward0>)\n",
            "tensor(26015.0059, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [290/469], Reconst Loss: 174.4648, KL Div: 1.9185\n",
            "tensor(25196.2988, grad_fn=<AddBackward0>)\n",
            "tensor(24976.3691, grad_fn=<AddBackward0>)\n",
            "tensor(24939.3359, grad_fn=<AddBackward0>)\n",
            "tensor(24775.5176, grad_fn=<AddBackward0>)\n",
            "tensor(25230.4277, grad_fn=<AddBackward0>)\n",
            "tensor(25913.9570, grad_fn=<AddBackward0>)\n",
            "tensor(24173.1758, grad_fn=<AddBackward0>)\n",
            "tensor(26002.4941, grad_fn=<AddBackward0>)\n",
            "tensor(25081.2539, grad_fn=<AddBackward0>)\n",
            "tensor(25266.7188, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [300/469], Reconst Loss: 167.6581, KL Div: 1.9825\n",
            "tensor(24693.5625, grad_fn=<AddBackward0>)\n",
            "tensor(24623.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25538.7695, grad_fn=<AddBackward0>)\n",
            "tensor(25576.2168, grad_fn=<AddBackward0>)\n",
            "tensor(24044.8281, grad_fn=<AddBackward0>)\n",
            "tensor(25222.0566, grad_fn=<AddBackward0>)\n",
            "tensor(24452.8926, grad_fn=<AddBackward0>)\n",
            "tensor(24794.3789, grad_fn=<AddBackward0>)\n",
            "tensor(24844.4492, grad_fn=<AddBackward0>)\n",
            "tensor(25011.3047, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [310/469], Reconst Loss: 169.1295, KL Div: 1.7514\n",
            "tensor(25409.5000, grad_fn=<AddBackward0>)\n",
            "tensor(25182.0039, grad_fn=<AddBackward0>)\n",
            "tensor(24965.9004, grad_fn=<AddBackward0>)\n",
            "tensor(25980.4434, grad_fn=<AddBackward0>)\n",
            "tensor(25132.9941, grad_fn=<AddBackward0>)\n",
            "tensor(25528.7148, grad_fn=<AddBackward0>)\n",
            "tensor(24839.2891, grad_fn=<AddBackward0>)\n",
            "tensor(25523.9883, grad_fn=<AddBackward0>)\n",
            "tensor(25234.4102, grad_fn=<AddBackward0>)\n",
            "tensor(25374.4570, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [320/469], Reconst Loss: 169.6804, KL Div: 1.9038\n",
            "tensor(24826.0488, grad_fn=<AddBackward0>)\n",
            "tensor(24296.2148, grad_fn=<AddBackward0>)\n",
            "tensor(25811.6562, grad_fn=<AddBackward0>)\n",
            "tensor(25265.9551, grad_fn=<AddBackward0>)\n",
            "tensor(24899.4805, grad_fn=<AddBackward0>)\n",
            "tensor(25812.0059, grad_fn=<AddBackward0>)\n",
            "tensor(25464.4121, grad_fn=<AddBackward0>)\n",
            "tensor(24708.3457, grad_fn=<AddBackward0>)\n",
            "tensor(25039.7090, grad_fn=<AddBackward0>)\n",
            "tensor(25301.3887, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [330/469], Reconst Loss: 169.4051, KL Div: 1.8841\n",
            "tensor(25300.5449, grad_fn=<AddBackward0>)\n",
            "tensor(25985.6211, grad_fn=<AddBackward0>)\n",
            "tensor(25017.1797, grad_fn=<AddBackward0>)\n",
            "tensor(24527.1133, grad_fn=<AddBackward0>)\n",
            "tensor(25555.0625, grad_fn=<AddBackward0>)\n",
            "tensor(24999.0625, grad_fn=<AddBackward0>)\n",
            "tensor(24955.1992, grad_fn=<AddBackward0>)\n",
            "tensor(25678.0684, grad_fn=<AddBackward0>)\n",
            "tensor(24616.3945, grad_fn=<AddBackward0>)\n",
            "tensor(24533.9023, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [340/469], Reconst Loss: 164.1820, KL Div: 1.8326\n",
            "tensor(25552.6738, grad_fn=<AddBackward0>)\n",
            "tensor(25349.4531, grad_fn=<AddBackward0>)\n",
            "tensor(26405.2539, grad_fn=<AddBackward0>)\n",
            "tensor(25211.1914, grad_fn=<AddBackward0>)\n",
            "tensor(26224.7402, grad_fn=<AddBackward0>)\n",
            "tensor(25164.0996, grad_fn=<AddBackward0>)\n",
            "tensor(24849.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25438.9805, grad_fn=<AddBackward0>)\n",
            "tensor(25515.8477, grad_fn=<AddBackward0>)\n",
            "tensor(24998.4961, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [350/469], Reconst Loss: 168.7186, KL Div: 1.7721\n",
            "tensor(25716.6016, grad_fn=<AddBackward0>)\n",
            "tensor(26015.6504, grad_fn=<AddBackward0>)\n",
            "tensor(25194.7598, grad_fn=<AddBackward0>)\n",
            "tensor(25824.5762, grad_fn=<AddBackward0>)\n",
            "tensor(25321.1777, grad_fn=<AddBackward0>)\n",
            "tensor(25002.1934, grad_fn=<AddBackward0>)\n",
            "tensor(25380.7910, grad_fn=<AddBackward0>)\n",
            "tensor(24783.4844, grad_fn=<AddBackward0>)\n",
            "tensor(24887.4688, grad_fn=<AddBackward0>)\n",
            "tensor(24765.5156, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [360/469], Reconst Loss: 164.0554, KL Div: 1.9617\n",
            "tensor(25067.5371, grad_fn=<AddBackward0>)\n",
            "tensor(25283.3594, grad_fn=<AddBackward0>)\n",
            "tensor(23429.3203, grad_fn=<AddBackward0>)\n",
            "tensor(25813.4492, grad_fn=<AddBackward0>)\n",
            "tensor(24718.7754, grad_fn=<AddBackward0>)\n",
            "tensor(25230.3398, grad_fn=<AddBackward0>)\n",
            "tensor(25719.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25298.5820, grad_fn=<AddBackward0>)\n",
            "tensor(25338.3086, grad_fn=<AddBackward0>)\n",
            "tensor(25197.7891, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [370/469], Reconst Loss: 167.9754, KL Div: 1.9255\n",
            "tensor(25092.7480, grad_fn=<AddBackward0>)\n",
            "tensor(25227.6406, grad_fn=<AddBackward0>)\n",
            "tensor(26267.9492, grad_fn=<AddBackward0>)\n",
            "tensor(25249.4961, grad_fn=<AddBackward0>)\n",
            "tensor(24819.9766, grad_fn=<AddBackward0>)\n",
            "tensor(25116.9414, grad_fn=<AddBackward0>)\n",
            "tensor(26003.4590, grad_fn=<AddBackward0>)\n",
            "tensor(25718.9512, grad_fn=<AddBackward0>)\n",
            "tensor(25178.6602, grad_fn=<AddBackward0>)\n",
            "tensor(25510.9961, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [380/469], Reconst Loss: 170.3793, KL Div: 1.9284\n",
            "tensor(25374.8633, grad_fn=<AddBackward0>)\n",
            "tensor(24460.3340, grad_fn=<AddBackward0>)\n",
            "tensor(25316.8164, grad_fn=<AddBackward0>)\n",
            "tensor(24577.3613, grad_fn=<AddBackward0>)\n",
            "tensor(25245.4160, grad_fn=<AddBackward0>)\n",
            "tensor(24639.6836, grad_fn=<AddBackward0>)\n",
            "tensor(25257.0020, grad_fn=<AddBackward0>)\n",
            "tensor(25091.3672, grad_fn=<AddBackward0>)\n",
            "tensor(25409.9082, grad_fn=<AddBackward0>)\n",
            "tensor(25270.9141, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [390/469], Reconst Loss: 167.6624, KL Div: 1.9844\n",
            "tensor(24721.9199, grad_fn=<AddBackward0>)\n",
            "tensor(25009.0977, grad_fn=<AddBackward0>)\n",
            "tensor(24874.4434, grad_fn=<AddBackward0>)\n",
            "tensor(25073.3164, grad_fn=<AddBackward0>)\n",
            "tensor(25392.8789, grad_fn=<AddBackward0>)\n",
            "tensor(25037.3613, grad_fn=<AddBackward0>)\n",
            "tensor(25701.5801, grad_fn=<AddBackward0>)\n",
            "tensor(24856.7148, grad_fn=<AddBackward0>)\n",
            "tensor(25244.4707, grad_fn=<AddBackward0>)\n",
            "tensor(24473.0645, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [400/469], Reconst Loss: 165.6379, KL Div: 1.7039\n",
            "tensor(24790.2324, grad_fn=<AddBackward0>)\n",
            "tensor(25311.1230, grad_fn=<AddBackward0>)\n",
            "tensor(25678.9004, grad_fn=<AddBackward0>)\n",
            "tensor(25784.5332, grad_fn=<AddBackward0>)\n",
            "tensor(25689.3438, grad_fn=<AddBackward0>)\n",
            "tensor(25190.9961, grad_fn=<AddBackward0>)\n",
            "tensor(24880.7754, grad_fn=<AddBackward0>)\n",
            "tensor(24739.4844, grad_fn=<AddBackward0>)\n",
            "tensor(24638.1211, grad_fn=<AddBackward0>)\n",
            "tensor(25539.4316, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [410/469], Reconst Loss: 172.7799, KL Div: 1.7831\n",
            "tensor(25081.9922, grad_fn=<AddBackward0>)\n",
            "tensor(24064.4277, grad_fn=<AddBackward0>)\n",
            "tensor(24470.2188, grad_fn=<AddBackward0>)\n",
            "tensor(25242.6895, grad_fn=<AddBackward0>)\n",
            "tensor(25048.9570, grad_fn=<AddBackward0>)\n",
            "tensor(24744.7500, grad_fn=<AddBackward0>)\n",
            "tensor(25300.2637, grad_fn=<AddBackward0>)\n",
            "tensor(25982.5840, grad_fn=<AddBackward0>)\n",
            "tensor(25176.7598, grad_fn=<AddBackward0>)\n",
            "tensor(25285.5820, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [420/469], Reconst Loss: 167.9665, KL Div: 1.9718\n",
            "tensor(25678.6836, grad_fn=<AddBackward0>)\n",
            "tensor(25666.1875, grad_fn=<AddBackward0>)\n",
            "tensor(25492.9219, grad_fn=<AddBackward0>)\n",
            "tensor(26088.7266, grad_fn=<AddBackward0>)\n",
            "tensor(25209.2656, grad_fn=<AddBackward0>)\n",
            "tensor(25179.8926, grad_fn=<AddBackward0>)\n",
            "tensor(24378.8516, grad_fn=<AddBackward0>)\n",
            "tensor(25209.6875, grad_fn=<AddBackward0>)\n",
            "tensor(25316.3984, grad_fn=<AddBackward0>)\n",
            "tensor(24621.6719, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [430/469], Reconst Loss: 164.8864, KL Div: 1.8314\n",
            "tensor(25056.9336, grad_fn=<AddBackward0>)\n",
            "tensor(26211.1875, grad_fn=<AddBackward0>)\n",
            "tensor(24985.1992, grad_fn=<AddBackward0>)\n",
            "tensor(24324.4004, grad_fn=<AddBackward0>)\n",
            "tensor(24806.9160, grad_fn=<AddBackward0>)\n",
            "tensor(25205.9727, grad_fn=<AddBackward0>)\n",
            "tensor(24641.5566, grad_fn=<AddBackward0>)\n",
            "tensor(25149.9082, grad_fn=<AddBackward0>)\n",
            "tensor(25877.2812, grad_fn=<AddBackward0>)\n",
            "tensor(24809.8730, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [440/469], Reconst Loss: 166.9414, KL Div: 1.7924\n",
            "tensor(25176.6797, grad_fn=<AddBackward0>)\n",
            "tensor(25454.8926, grad_fn=<AddBackward0>)\n",
            "tensor(24494.7441, grad_fn=<AddBackward0>)\n",
            "tensor(25485.8496, grad_fn=<AddBackward0>)\n",
            "tensor(24688.2559, grad_fn=<AddBackward0>)\n",
            "tensor(24020.4570, grad_fn=<AddBackward0>)\n",
            "tensor(24983.5566, grad_fn=<AddBackward0>)\n",
            "tensor(25299.2109, grad_fn=<AddBackward0>)\n",
            "tensor(25434.1738, grad_fn=<AddBackward0>)\n",
            "tensor(25095.1934, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [450/469], Reconst Loss: 166.0248, KL Div: 2.0021\n",
            "tensor(25393.3184, grad_fn=<AddBackward0>)\n",
            "tensor(24594.5332, grad_fn=<AddBackward0>)\n",
            "tensor(26076.9668, grad_fn=<AddBackward0>)\n",
            "tensor(25621.0527, grad_fn=<AddBackward0>)\n",
            "tensor(24705.0156, grad_fn=<AddBackward0>)\n",
            "tensor(25474.7441, grad_fn=<AddBackward0>)\n",
            "tensor(25184.0801, grad_fn=<AddBackward0>)\n",
            "tensor(25881.3320, grad_fn=<AddBackward0>)\n",
            "tensor(25597.6523, grad_fn=<AddBackward0>)\n",
            "tensor(24263.6426, grad_fn=<AddBackward0>)\n",
            "Epoch[22/25], Step [460/469], Reconst Loss: 162.8061, KL Div: 1.7836\n",
            "tensor(25027.6387, grad_fn=<AddBackward0>)\n",
            "tensor(25388.9219, grad_fn=<AddBackward0>)\n",
            "tensor(25782.8203, grad_fn=<AddBackward0>)\n",
            "tensor(24857.2617, grad_fn=<AddBackward0>)\n",
            "tensor(24340.3262, grad_fn=<AddBackward0>)\n",
            "tensor(26216.5449, grad_fn=<AddBackward0>)\n",
            "tensor(25775.8867, grad_fn=<AddBackward0>)\n",
            "tensor(25869.1211, grad_fn=<AddBackward0>)\n",
            "tensor(18168.6328, grad_fn=<AddBackward0>)\n",
            "tensor(25319.7500, grad_fn=<AddBackward0>)\n",
            "tensor(24938.0781, grad_fn=<AddBackward0>)\n",
            "tensor(25963.9121, grad_fn=<AddBackward0>)\n",
            "tensor(23928.4727, grad_fn=<AddBackward0>)\n",
            "tensor(24757.3633, grad_fn=<AddBackward0>)\n",
            "tensor(25053.0273, grad_fn=<AddBackward0>)\n",
            "tensor(24912.0762, grad_fn=<AddBackward0>)\n",
            "tensor(25613.4766, grad_fn=<AddBackward0>)\n",
            "tensor(25072.5938, grad_fn=<AddBackward0>)\n",
            "tensor(25040.1133, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [10/469], Reconst Loss: 170.3838, KL Div: 1.6828\n",
            "tensor(25857.5703, grad_fn=<AddBackward0>)\n",
            "tensor(25877.0254, grad_fn=<AddBackward0>)\n",
            "tensor(24867.3457, grad_fn=<AddBackward0>)\n",
            "tensor(24670.4004, grad_fn=<AddBackward0>)\n",
            "tensor(25890.9883, grad_fn=<AddBackward0>)\n",
            "tensor(25893.1855, grad_fn=<AddBackward0>)\n",
            "tensor(25738.5781, grad_fn=<AddBackward0>)\n",
            "tensor(26155.3398, grad_fn=<AddBackward0>)\n",
            "tensor(25312.9102, grad_fn=<AddBackward0>)\n",
            "tensor(24965.8320, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [20/469], Reconst Loss: 163.6951, KL Div: 2.0900\n",
            "tensor(25248.9922, grad_fn=<AddBackward0>)\n",
            "tensor(24365.0566, grad_fn=<AddBackward0>)\n",
            "tensor(25741.6914, grad_fn=<AddBackward0>)\n",
            "tensor(25177.4746, grad_fn=<AddBackward0>)\n",
            "tensor(25788.1777, grad_fn=<AddBackward0>)\n",
            "tensor(25197.6543, grad_fn=<AddBackward0>)\n",
            "tensor(25476.3750, grad_fn=<AddBackward0>)\n",
            "tensor(25062.4453, grad_fn=<AddBackward0>)\n",
            "tensor(25270.3926, grad_fn=<AddBackward0>)\n",
            "tensor(25835.2266, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [30/469], Reconst Loss: 174.3061, KL Div: 1.8354\n",
            "tensor(25793.2559, grad_fn=<AddBackward0>)\n",
            "tensor(25706.1016, grad_fn=<AddBackward0>)\n",
            "tensor(25315.2500, grad_fn=<AddBackward0>)\n",
            "tensor(25622.4297, grad_fn=<AddBackward0>)\n",
            "tensor(25865.7168, grad_fn=<AddBackward0>)\n",
            "tensor(24617.6777, grad_fn=<AddBackward0>)\n",
            "tensor(25809.1387, grad_fn=<AddBackward0>)\n",
            "tensor(24928.7207, grad_fn=<AddBackward0>)\n",
            "tensor(26414.9199, grad_fn=<AddBackward0>)\n",
            "tensor(26045.9141, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [40/469], Reconst Loss: 172.3714, KL Div: 2.0742\n",
            "tensor(24603.7148, grad_fn=<AddBackward0>)\n",
            "tensor(24769.9883, grad_fn=<AddBackward0>)\n",
            "tensor(25315.6992, grad_fn=<AddBackward0>)\n",
            "tensor(25168.8301, grad_fn=<AddBackward0>)\n",
            "tensor(26146.9609, grad_fn=<AddBackward0>)\n",
            "tensor(25526.7168, grad_fn=<AddBackward0>)\n",
            "tensor(24625.3340, grad_fn=<AddBackward0>)\n",
            "tensor(24973.2305, grad_fn=<AddBackward0>)\n",
            "tensor(24413.1094, grad_fn=<AddBackward0>)\n",
            "tensor(25492.8945, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [50/469], Reconst Loss: 172.8951, KL Div: 1.7512\n",
            "tensor(25553.5117, grad_fn=<AddBackward0>)\n",
            "tensor(25272.8301, grad_fn=<AddBackward0>)\n",
            "tensor(25746.7148, grad_fn=<AddBackward0>)\n",
            "tensor(24530.8418, grad_fn=<AddBackward0>)\n",
            "tensor(25861.8047, grad_fn=<AddBackward0>)\n",
            "tensor(24432.6582, grad_fn=<AddBackward0>)\n",
            "tensor(25570.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25216.1367, grad_fn=<AddBackward0>)\n",
            "tensor(24815.6172, grad_fn=<AddBackward0>)\n",
            "tensor(24677.5547, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [60/469], Reconst Loss: 164.4888, KL Div: 1.8870\n",
            "tensor(25419.4805, grad_fn=<AddBackward0>)\n",
            "tensor(24735.9043, grad_fn=<AddBackward0>)\n",
            "tensor(25427.4648, grad_fn=<AddBackward0>)\n",
            "tensor(26513.1641, grad_fn=<AddBackward0>)\n",
            "tensor(25783.7988, grad_fn=<AddBackward0>)\n",
            "tensor(24828.8457, grad_fn=<AddBackward0>)\n",
            "tensor(24839.6602, grad_fn=<AddBackward0>)\n",
            "tensor(24364.0469, grad_fn=<AddBackward0>)\n",
            "tensor(25610.6504, grad_fn=<AddBackward0>)\n",
            "tensor(24298.1816, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [70/469], Reconst Loss: 161.0441, KL Div: 1.9190\n",
            "tensor(25255.4609, grad_fn=<AddBackward0>)\n",
            "tensor(25911.5137, grad_fn=<AddBackward0>)\n",
            "tensor(24587., grad_fn=<AddBackward0>)\n",
            "tensor(25688.0469, grad_fn=<AddBackward0>)\n",
            "tensor(24653.2422, grad_fn=<AddBackward0>)\n",
            "tensor(24827.4688, grad_fn=<AddBackward0>)\n",
            "tensor(24738.0859, grad_fn=<AddBackward0>)\n",
            "tensor(25654.8789, grad_fn=<AddBackward0>)\n",
            "tensor(25381.6797, grad_fn=<AddBackward0>)\n",
            "tensor(25110.8828, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [80/469], Reconst Loss: 167.6837, KL Div: 1.8997\n",
            "tensor(25291.5215, grad_fn=<AddBackward0>)\n",
            "tensor(25295.6992, grad_fn=<AddBackward0>)\n",
            "tensor(25362.9492, grad_fn=<AddBackward0>)\n",
            "tensor(25160.1230, grad_fn=<AddBackward0>)\n",
            "tensor(25411.5605, grad_fn=<AddBackward0>)\n",
            "tensor(25434.9297, grad_fn=<AddBackward0>)\n",
            "tensor(25159.7988, grad_fn=<AddBackward0>)\n",
            "tensor(25292.4883, grad_fn=<AddBackward0>)\n",
            "tensor(25314.3242, grad_fn=<AddBackward0>)\n",
            "tensor(24888.5098, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [90/469], Reconst Loss: 166.0107, KL Div: 1.8954\n",
            "tensor(25646.7988, grad_fn=<AddBackward0>)\n",
            "tensor(24645.1621, grad_fn=<AddBackward0>)\n",
            "tensor(25429.1992, grad_fn=<AddBackward0>)\n",
            "tensor(25839.4805, grad_fn=<AddBackward0>)\n",
            "tensor(25691.5703, grad_fn=<AddBackward0>)\n",
            "tensor(24649.7090, grad_fn=<AddBackward0>)\n",
            "tensor(25769.5918, grad_fn=<AddBackward0>)\n",
            "tensor(25039.2012, grad_fn=<AddBackward0>)\n",
            "tensor(25744.0469, grad_fn=<AddBackward0>)\n",
            "tensor(25057.2148, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [100/469], Reconst Loss: 167.1920, KL Div: 1.9045\n",
            "tensor(24421.8711, grad_fn=<AddBackward0>)\n",
            "tensor(24496.6738, grad_fn=<AddBackward0>)\n",
            "tensor(24917.3164, grad_fn=<AddBackward0>)\n",
            "tensor(25116.5508, grad_fn=<AddBackward0>)\n",
            "tensor(25805.1289, grad_fn=<AddBackward0>)\n",
            "tensor(25219.1699, grad_fn=<AddBackward0>)\n",
            "tensor(24932.3516, grad_fn=<AddBackward0>)\n",
            "tensor(24367.1621, grad_fn=<AddBackward0>)\n",
            "tensor(25947.1621, grad_fn=<AddBackward0>)\n",
            "tensor(25290.1758, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [110/469], Reconst Loss: 168.2642, KL Div: 1.9544\n",
            "tensor(25254.6758, grad_fn=<AddBackward0>)\n",
            "tensor(25896.0156, grad_fn=<AddBackward0>)\n",
            "tensor(25310.4902, grad_fn=<AddBackward0>)\n",
            "tensor(25779.2031, grad_fn=<AddBackward0>)\n",
            "tensor(25088.1992, grad_fn=<AddBackward0>)\n",
            "tensor(25481.9199, grad_fn=<AddBackward0>)\n",
            "tensor(24818.7148, grad_fn=<AddBackward0>)\n",
            "tensor(25731.6445, grad_fn=<AddBackward0>)\n",
            "tensor(25293.4688, grad_fn=<AddBackward0>)\n",
            "tensor(24930.6387, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [120/469], Reconst Loss: 167.9688, KL Div: 1.7868\n",
            "tensor(24493.0703, grad_fn=<AddBackward0>)\n",
            "tensor(24564.7930, grad_fn=<AddBackward0>)\n",
            "tensor(25645.6035, grad_fn=<AddBackward0>)\n",
            "tensor(25571.2109, grad_fn=<AddBackward0>)\n",
            "tensor(25341.1816, grad_fn=<AddBackward0>)\n",
            "tensor(25325.5000, grad_fn=<AddBackward0>)\n",
            "tensor(25512.1523, grad_fn=<AddBackward0>)\n",
            "tensor(25196.7930, grad_fn=<AddBackward0>)\n",
            "tensor(25291.7656, grad_fn=<AddBackward0>)\n",
            "tensor(25174.6504, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [130/469], Reconst Loss: 168.6917, KL Div: 1.8657\n",
            "tensor(25212.0938, grad_fn=<AddBackward0>)\n",
            "tensor(24973.3066, grad_fn=<AddBackward0>)\n",
            "tensor(25122.6777, grad_fn=<AddBackward0>)\n",
            "tensor(25421.3906, grad_fn=<AddBackward0>)\n",
            "tensor(25638.7012, grad_fn=<AddBackward0>)\n",
            "tensor(25205.8867, grad_fn=<AddBackward0>)\n",
            "tensor(26068.2422, grad_fn=<AddBackward0>)\n",
            "tensor(25744.6328, grad_fn=<AddBackward0>)\n",
            "tensor(24826.7969, grad_fn=<AddBackward0>)\n",
            "tensor(24635.3203, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [140/469], Reconst Loss: 164.5930, KL Div: 1.8580\n",
            "tensor(25462.5859, grad_fn=<AddBackward0>)\n",
            "tensor(23898.3555, grad_fn=<AddBackward0>)\n",
            "tensor(24732.8789, grad_fn=<AddBackward0>)\n",
            "tensor(25276.1387, grad_fn=<AddBackward0>)\n",
            "tensor(24237.6387, grad_fn=<AddBackward0>)\n",
            "tensor(26050.0234, grad_fn=<AddBackward0>)\n",
            "tensor(25932.5078, grad_fn=<AddBackward0>)\n",
            "tensor(25264.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25385.8594, grad_fn=<AddBackward0>)\n",
            "tensor(25167.0234, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [150/469], Reconst Loss: 169.5998, KL Div: 1.8012\n",
            "tensor(23905.3516, grad_fn=<AddBackward0>)\n",
            "tensor(25689.8789, grad_fn=<AddBackward0>)\n",
            "tensor(24208.5176, grad_fn=<AddBackward0>)\n",
            "tensor(26089.7891, grad_fn=<AddBackward0>)\n",
            "tensor(24254.7461, grad_fn=<AddBackward0>)\n",
            "tensor(25779.7500, grad_fn=<AddBackward0>)\n",
            "tensor(25117.9355, grad_fn=<AddBackward0>)\n",
            "tensor(25331.4785, grad_fn=<AddBackward0>)\n",
            "tensor(24747.2246, grad_fn=<AddBackward0>)\n",
            "tensor(26007.4160, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [160/469], Reconst Loss: 175.3709, KL Div: 1.8541\n",
            "tensor(25688.6250, grad_fn=<AddBackward0>)\n",
            "tensor(24633.5410, grad_fn=<AddBackward0>)\n",
            "tensor(25083.4590, grad_fn=<AddBackward0>)\n",
            "tensor(25124.1445, grad_fn=<AddBackward0>)\n",
            "tensor(25623.9922, grad_fn=<AddBackward0>)\n",
            "tensor(24734.2676, grad_fn=<AddBackward0>)\n",
            "tensor(25436.0938, grad_fn=<AddBackward0>)\n",
            "tensor(24663.8281, grad_fn=<AddBackward0>)\n",
            "tensor(25237.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25333.8145, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [170/469], Reconst Loss: 167.7540, KL Div: 2.0111\n",
            "tensor(24828.9375, grad_fn=<AddBackward0>)\n",
            "tensor(25665.4746, grad_fn=<AddBackward0>)\n",
            "tensor(24848.3086, grad_fn=<AddBackward0>)\n",
            "tensor(24940.2754, grad_fn=<AddBackward0>)\n",
            "tensor(24604.0137, grad_fn=<AddBackward0>)\n",
            "tensor(24733.6797, grad_fn=<AddBackward0>)\n",
            "tensor(24416.0859, grad_fn=<AddBackward0>)\n",
            "tensor(25283.1836, grad_fn=<AddBackward0>)\n",
            "tensor(25382.8828, grad_fn=<AddBackward0>)\n",
            "tensor(26120.1523, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [180/469], Reconst Loss: 174.7015, KL Div: 1.9575\n",
            "tensor(24628.8203, grad_fn=<AddBackward0>)\n",
            "tensor(24607.9297, grad_fn=<AddBackward0>)\n",
            "tensor(25180.3789, grad_fn=<AddBackward0>)\n",
            "tensor(24514.3984, grad_fn=<AddBackward0>)\n",
            "tensor(25606.5156, grad_fn=<AddBackward0>)\n",
            "tensor(25764.6699, grad_fn=<AddBackward0>)\n",
            "tensor(25462.2578, grad_fn=<AddBackward0>)\n",
            "tensor(25024.7227, grad_fn=<AddBackward0>)\n",
            "tensor(25106.8145, grad_fn=<AddBackward0>)\n",
            "tensor(24732.7500, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [190/469], Reconst Loss: 166.8598, KL Div: 1.7577\n",
            "tensor(24954.3535, grad_fn=<AddBackward0>)\n",
            "tensor(26079.7305, grad_fn=<AddBackward0>)\n",
            "tensor(24556.6719, grad_fn=<AddBackward0>)\n",
            "tensor(25570.1680, grad_fn=<AddBackward0>)\n",
            "tensor(26392.5039, grad_fn=<AddBackward0>)\n",
            "tensor(24796.1387, grad_fn=<AddBackward0>)\n",
            "tensor(24396.1836, grad_fn=<AddBackward0>)\n",
            "tensor(24778.6953, grad_fn=<AddBackward0>)\n",
            "tensor(25314.4590, grad_fn=<AddBackward0>)\n",
            "tensor(26010.8398, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [200/469], Reconst Loss: 174.7397, KL Div: 1.8980\n",
            "tensor(26000.5039, grad_fn=<AddBackward0>)\n",
            "tensor(25959.7207, grad_fn=<AddBackward0>)\n",
            "tensor(25207.3125, grad_fn=<AddBackward0>)\n",
            "tensor(24332.3926, grad_fn=<AddBackward0>)\n",
            "tensor(25000.3867, grad_fn=<AddBackward0>)\n",
            "tensor(25129.3672, grad_fn=<AddBackward0>)\n",
            "tensor(24955.2383, grad_fn=<AddBackward0>)\n",
            "tensor(24810.8066, grad_fn=<AddBackward0>)\n",
            "tensor(24816.3359, grad_fn=<AddBackward0>)\n",
            "tensor(25841.2695, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [210/469], Reconst Loss: 175.6847, KL Div: 1.7467\n",
            "tensor(25221.1172, grad_fn=<AddBackward0>)\n",
            "tensor(24681.4512, grad_fn=<AddBackward0>)\n",
            "tensor(24813.7910, grad_fn=<AddBackward0>)\n",
            "tensor(24222.2793, grad_fn=<AddBackward0>)\n",
            "tensor(25916.7012, grad_fn=<AddBackward0>)\n",
            "tensor(25804.1953, grad_fn=<AddBackward0>)\n",
            "tensor(25425.2305, grad_fn=<AddBackward0>)\n",
            "tensor(24828.9805, grad_fn=<AddBackward0>)\n",
            "tensor(25175.1758, grad_fn=<AddBackward0>)\n",
            "tensor(24272.8203, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [220/469], Reconst Loss: 161.5521, KL Div: 1.8720\n",
            "tensor(24861.2188, grad_fn=<AddBackward0>)\n",
            "tensor(25683.9961, grad_fn=<AddBackward0>)\n",
            "tensor(25964.1660, grad_fn=<AddBackward0>)\n",
            "tensor(25629.5488, grad_fn=<AddBackward0>)\n",
            "tensor(24639.3340, grad_fn=<AddBackward0>)\n",
            "tensor(24771.1953, grad_fn=<AddBackward0>)\n",
            "tensor(25129.6191, grad_fn=<AddBackward0>)\n",
            "tensor(26127.4238, grad_fn=<AddBackward0>)\n",
            "tensor(24544.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25495.2168, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [230/469], Reconst Loss: 169.8517, KL Div: 1.9553\n",
            "tensor(24935.5840, grad_fn=<AddBackward0>)\n",
            "tensor(26319.5352, grad_fn=<AddBackward0>)\n",
            "tensor(26384.9902, grad_fn=<AddBackward0>)\n",
            "tensor(25226.7031, grad_fn=<AddBackward0>)\n",
            "tensor(25505.2539, grad_fn=<AddBackward0>)\n",
            "tensor(25467.7363, grad_fn=<AddBackward0>)\n",
            "tensor(25378.4395, grad_fn=<AddBackward0>)\n",
            "tensor(25418.8926, grad_fn=<AddBackward0>)\n",
            "tensor(24263.5137, grad_fn=<AddBackward0>)\n",
            "tensor(25559.4688, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [240/469], Reconst Loss: 167.7544, KL Div: 2.1286\n",
            "tensor(25246.9531, grad_fn=<AddBackward0>)\n",
            "tensor(24926.5527, grad_fn=<AddBackward0>)\n",
            "tensor(25306.1465, grad_fn=<AddBackward0>)\n",
            "tensor(24937.8418, grad_fn=<AddBackward0>)\n",
            "tensor(25149.0156, grad_fn=<AddBackward0>)\n",
            "tensor(25495.3926, grad_fn=<AddBackward0>)\n",
            "tensor(25090.1836, grad_fn=<AddBackward0>)\n",
            "tensor(25768.3223, grad_fn=<AddBackward0>)\n",
            "tensor(25062.3750, grad_fn=<AddBackward0>)\n",
            "tensor(25693.8672, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [250/469], Reconst Loss: 171.4886, KL Div: 1.9496\n",
            "tensor(24793.6641, grad_fn=<AddBackward0>)\n",
            "tensor(24933.0547, grad_fn=<AddBackward0>)\n",
            "tensor(26139.1562, grad_fn=<AddBackward0>)\n",
            "tensor(24816.0801, grad_fn=<AddBackward0>)\n",
            "tensor(25416.1465, grad_fn=<AddBackward0>)\n",
            "tensor(25274.5488, grad_fn=<AddBackward0>)\n",
            "tensor(24978.9141, grad_fn=<AddBackward0>)\n",
            "tensor(24949.3711, grad_fn=<AddBackward0>)\n",
            "tensor(24396.7676, grad_fn=<AddBackward0>)\n",
            "tensor(25401.8828, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [260/469], Reconst Loss: 171.7314, KL Div: 1.7814\n",
            "tensor(25213.6816, grad_fn=<AddBackward0>)\n",
            "tensor(26601.0918, grad_fn=<AddBackward0>)\n",
            "tensor(25410.6836, grad_fn=<AddBackward0>)\n",
            "tensor(25660.7324, grad_fn=<AddBackward0>)\n",
            "tensor(26186.4355, grad_fn=<AddBackward0>)\n",
            "tensor(24724.6738, grad_fn=<AddBackward0>)\n",
            "tensor(25157.0215, grad_fn=<AddBackward0>)\n",
            "tensor(24869.3164, grad_fn=<AddBackward0>)\n",
            "tensor(25390.4434, grad_fn=<AddBackward0>)\n",
            "tensor(24805.4355, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [270/469], Reconst Loss: 165.4154, KL Div: 1.8918\n",
            "tensor(24177.8086, grad_fn=<AddBackward0>)\n",
            "tensor(25458.6719, grad_fn=<AddBackward0>)\n",
            "tensor(24819.9805, grad_fn=<AddBackward0>)\n",
            "tensor(24380.7383, grad_fn=<AddBackward0>)\n",
            "tensor(25500.5918, grad_fn=<AddBackward0>)\n",
            "tensor(25405.5156, grad_fn=<AddBackward0>)\n",
            "tensor(25518.5898, grad_fn=<AddBackward0>)\n",
            "tensor(26248.5547, grad_fn=<AddBackward0>)\n",
            "tensor(25217.7832, grad_fn=<AddBackward0>)\n",
            "tensor(24097.3418, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [280/469], Reconst Loss: 161.0798, KL Div: 1.8120\n",
            "tensor(24808.6523, grad_fn=<AddBackward0>)\n",
            "tensor(24802.5039, grad_fn=<AddBackward0>)\n",
            "tensor(24964.0039, grad_fn=<AddBackward0>)\n",
            "tensor(25493.3574, grad_fn=<AddBackward0>)\n",
            "tensor(25354.2051, grad_fn=<AddBackward0>)\n",
            "tensor(24572.9160, grad_fn=<AddBackward0>)\n",
            "tensor(25572.3535, grad_fn=<AddBackward0>)\n",
            "tensor(25012.2891, grad_fn=<AddBackward0>)\n",
            "tensor(24860.3340, grad_fn=<AddBackward0>)\n",
            "tensor(25589.5176, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [290/469], Reconst Loss: 172.0518, KL Div: 1.8578\n",
            "tensor(25355.1445, grad_fn=<AddBackward0>)\n",
            "tensor(25675.5938, grad_fn=<AddBackward0>)\n",
            "tensor(25660.5625, grad_fn=<AddBackward0>)\n",
            "tensor(24416.5039, grad_fn=<AddBackward0>)\n",
            "tensor(25634.9375, grad_fn=<AddBackward0>)\n",
            "tensor(25461.8262, grad_fn=<AddBackward0>)\n",
            "tensor(24682.1758, grad_fn=<AddBackward0>)\n",
            "tensor(25579.0469, grad_fn=<AddBackward0>)\n",
            "tensor(25556.4570, grad_fn=<AddBackward0>)\n",
            "tensor(25321.8477, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [300/469], Reconst Loss: 168.4287, KL Div: 1.9599\n",
            "tensor(25110.5859, grad_fn=<AddBackward0>)\n",
            "tensor(24908.6172, grad_fn=<AddBackward0>)\n",
            "tensor(24534.9824, grad_fn=<AddBackward0>)\n",
            "tensor(24560.3750, grad_fn=<AddBackward0>)\n",
            "tensor(26005.3574, grad_fn=<AddBackward0>)\n",
            "tensor(24286.8281, grad_fn=<AddBackward0>)\n",
            "tensor(25955.5234, grad_fn=<AddBackward0>)\n",
            "tensor(24450.4531, grad_fn=<AddBackward0>)\n",
            "tensor(25109.2637, grad_fn=<AddBackward0>)\n",
            "tensor(24415.8223, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [310/469], Reconst Loss: 164.9940, KL Div: 1.7170\n",
            "tensor(25412.1113, grad_fn=<AddBackward0>)\n",
            "tensor(25424.6738, grad_fn=<AddBackward0>)\n",
            "tensor(25047.4121, grad_fn=<AddBackward0>)\n",
            "tensor(26299.3125, grad_fn=<AddBackward0>)\n",
            "tensor(25402.1250, grad_fn=<AddBackward0>)\n",
            "tensor(24110.2852, grad_fn=<AddBackward0>)\n",
            "tensor(25395.1133, grad_fn=<AddBackward0>)\n",
            "tensor(24161.0234, grad_fn=<AddBackward0>)\n",
            "tensor(25437.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25338.5723, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [320/469], Reconst Loss: 172.1596, KL Div: 1.7199\n",
            "tensor(25727.5156, grad_fn=<AddBackward0>)\n",
            "tensor(23905.4453, grad_fn=<AddBackward0>)\n",
            "tensor(25461.2363, grad_fn=<AddBackward0>)\n",
            "tensor(25142.2969, grad_fn=<AddBackward0>)\n",
            "tensor(24941.6895, grad_fn=<AddBackward0>)\n",
            "tensor(24543.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25368.8906, grad_fn=<AddBackward0>)\n",
            "tensor(25941.1016, grad_fn=<AddBackward0>)\n",
            "tensor(25579.0352, grad_fn=<AddBackward0>)\n",
            "tensor(25171.9609, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [330/469], Reconst Loss: 167.2767, KL Div: 1.9586\n",
            "tensor(25912.6172, grad_fn=<AddBackward0>)\n",
            "tensor(25791.2656, grad_fn=<AddBackward0>)\n",
            "tensor(24162.7285, grad_fn=<AddBackward0>)\n",
            "tensor(25833.8164, grad_fn=<AddBackward0>)\n",
            "tensor(24967.7422, grad_fn=<AddBackward0>)\n",
            "tensor(25087.2773, grad_fn=<AddBackward0>)\n",
            "tensor(26352.8711, grad_fn=<AddBackward0>)\n",
            "tensor(25443.8398, grad_fn=<AddBackward0>)\n",
            "tensor(25046.3477, grad_fn=<AddBackward0>)\n",
            "tensor(24689.6348, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [340/469], Reconst Loss: 165.0949, KL Div: 1.8529\n",
            "tensor(25777.8242, grad_fn=<AddBackward0>)\n",
            "tensor(24985.5391, grad_fn=<AddBackward0>)\n",
            "tensor(25179.9961, grad_fn=<AddBackward0>)\n",
            "tensor(24613.4121, grad_fn=<AddBackward0>)\n",
            "tensor(24974.7090, grad_fn=<AddBackward0>)\n",
            "tensor(25499.6621, grad_fn=<AddBackward0>)\n",
            "tensor(24510.2559, grad_fn=<AddBackward0>)\n",
            "tensor(25305.2891, grad_fn=<AddBackward0>)\n",
            "tensor(24796.5176, grad_fn=<AddBackward0>)\n",
            "tensor(24153.9199, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [350/469], Reconst Loss: 162.6811, KL Div: 1.7348\n",
            "tensor(24733.4805, grad_fn=<AddBackward0>)\n",
            "tensor(24750.5312, grad_fn=<AddBackward0>)\n",
            "tensor(25364.7695, grad_fn=<AddBackward0>)\n",
            "tensor(24514.7422, grad_fn=<AddBackward0>)\n",
            "tensor(25880.9473, grad_fn=<AddBackward0>)\n",
            "tensor(25045.3887, grad_fn=<AddBackward0>)\n",
            "tensor(25928.0977, grad_fn=<AddBackward0>)\n",
            "tensor(24481.7031, grad_fn=<AddBackward0>)\n",
            "tensor(26334.7285, grad_fn=<AddBackward0>)\n",
            "tensor(25936.4707, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [360/469], Reconst Loss: 174.4449, KL Div: 1.8789\n",
            "tensor(25174.8516, grad_fn=<AddBackward0>)\n",
            "tensor(24879.9688, grad_fn=<AddBackward0>)\n",
            "tensor(25585.5078, grad_fn=<AddBackward0>)\n",
            "tensor(25397.7246, grad_fn=<AddBackward0>)\n",
            "tensor(25920.9980, grad_fn=<AddBackward0>)\n",
            "tensor(25266.4785, grad_fn=<AddBackward0>)\n",
            "tensor(25680.2852, grad_fn=<AddBackward0>)\n",
            "tensor(24400.4375, grad_fn=<AddBackward0>)\n",
            "tensor(24994.1641, grad_fn=<AddBackward0>)\n",
            "tensor(24903.3945, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [370/469], Reconst Loss: 167.2970, KL Div: 1.8174\n",
            "tensor(26063.6992, grad_fn=<AddBackward0>)\n",
            "tensor(25164.3203, grad_fn=<AddBackward0>)\n",
            "tensor(25400.3125, grad_fn=<AddBackward0>)\n",
            "tensor(24803.2871, grad_fn=<AddBackward0>)\n",
            "tensor(25591.6836, grad_fn=<AddBackward0>)\n",
            "tensor(24407.8242, grad_fn=<AddBackward0>)\n",
            "tensor(24883.9238, grad_fn=<AddBackward0>)\n",
            "tensor(25588.3594, grad_fn=<AddBackward0>)\n",
            "tensor(25149.1621, grad_fn=<AddBackward0>)\n",
            "tensor(24575.6484, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [380/469], Reconst Loss: 164.9343, KL Div: 1.8042\n",
            "tensor(25048.4160, grad_fn=<AddBackward0>)\n",
            "tensor(24871.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25136.8848, grad_fn=<AddBackward0>)\n",
            "tensor(24427.4688, grad_fn=<AddBackward0>)\n",
            "tensor(24657.9727, grad_fn=<AddBackward0>)\n",
            "tensor(25195.9805, grad_fn=<AddBackward0>)\n",
            "tensor(26085.5801, grad_fn=<AddBackward0>)\n",
            "tensor(24928.4102, grad_fn=<AddBackward0>)\n",
            "tensor(25304.3145, grad_fn=<AddBackward0>)\n",
            "tensor(24565.4941, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [390/469], Reconst Loss: 165.2630, KL Div: 1.7770\n",
            "tensor(24181.0039, grad_fn=<AddBackward0>)\n",
            "tensor(24794.4551, grad_fn=<AddBackward0>)\n",
            "tensor(24195.3613, grad_fn=<AddBackward0>)\n",
            "tensor(25452.0273, grad_fn=<AddBackward0>)\n",
            "tensor(25734.5781, grad_fn=<AddBackward0>)\n",
            "tensor(24670.7090, grad_fn=<AddBackward0>)\n",
            "tensor(25347.5938, grad_fn=<AddBackward0>)\n",
            "tensor(26489.1387, grad_fn=<AddBackward0>)\n",
            "tensor(24578.4062, grad_fn=<AddBackward0>)\n",
            "tensor(24902.0859, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [400/469], Reconst Loss: 165.3021, KL Div: 1.9497\n",
            "tensor(24646.9004, grad_fn=<AddBackward0>)\n",
            "tensor(25698.3320, grad_fn=<AddBackward0>)\n",
            "tensor(25522.5527, grad_fn=<AddBackward0>)\n",
            "tensor(26266.6289, grad_fn=<AddBackward0>)\n",
            "tensor(25923.6328, grad_fn=<AddBackward0>)\n",
            "tensor(25294.6973, grad_fn=<AddBackward0>)\n",
            "tensor(24696.7227, grad_fn=<AddBackward0>)\n",
            "tensor(25007.9004, grad_fn=<AddBackward0>)\n",
            "tensor(24342.4668, grad_fn=<AddBackward0>)\n",
            "tensor(25030.4004, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [410/469], Reconst Loss: 165.7083, KL Div: 1.9894\n",
            "tensor(25502.5605, grad_fn=<AddBackward0>)\n",
            "tensor(25863.7598, grad_fn=<AddBackward0>)\n",
            "tensor(24157.8457, grad_fn=<AddBackward0>)\n",
            "tensor(24054.2383, grad_fn=<AddBackward0>)\n",
            "tensor(25100.3418, grad_fn=<AddBackward0>)\n",
            "tensor(25454.4414, grad_fn=<AddBackward0>)\n",
            "tensor(25759.1445, grad_fn=<AddBackward0>)\n",
            "tensor(26383.7168, grad_fn=<AddBackward0>)\n",
            "tensor(25308.8633, grad_fn=<AddBackward0>)\n",
            "tensor(25154.1875, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [420/469], Reconst Loss: 169.5485, KL Div: 1.7979\n",
            "tensor(25680.4805, grad_fn=<AddBackward0>)\n",
            "tensor(25206.8867, grad_fn=<AddBackward0>)\n",
            "tensor(25095.5449, grad_fn=<AddBackward0>)\n",
            "tensor(24893.3691, grad_fn=<AddBackward0>)\n",
            "tensor(26071.0625, grad_fn=<AddBackward0>)\n",
            "tensor(25198.7754, grad_fn=<AddBackward0>)\n",
            "tensor(24692.1465, grad_fn=<AddBackward0>)\n",
            "tensor(25483.3535, grad_fn=<AddBackward0>)\n",
            "tensor(25065.8379, grad_fn=<AddBackward0>)\n",
            "tensor(26045.6680, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [430/469], Reconst Loss: 175.0655, KL Div: 1.8944\n",
            "tensor(25998.3887, grad_fn=<AddBackward0>)\n",
            "tensor(25190.6289, grad_fn=<AddBackward0>)\n",
            "tensor(24846.7793, grad_fn=<AddBackward0>)\n",
            "tensor(24830.2539, grad_fn=<AddBackward0>)\n",
            "tensor(25053.9375, grad_fn=<AddBackward0>)\n",
            "tensor(25550.7930, grad_fn=<AddBackward0>)\n",
            "tensor(25258.1523, grad_fn=<AddBackward0>)\n",
            "tensor(25471.3359, grad_fn=<AddBackward0>)\n",
            "tensor(25043.8398, grad_fn=<AddBackward0>)\n",
            "tensor(25883.8926, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [440/469], Reconst Loss: 170.8181, KL Div: 2.0933\n",
            "tensor(24870.9492, grad_fn=<AddBackward0>)\n",
            "tensor(24813.2188, grad_fn=<AddBackward0>)\n",
            "tensor(25591.5703, grad_fn=<AddBackward0>)\n",
            "tensor(24614.8242, grad_fn=<AddBackward0>)\n",
            "tensor(25087.5488, grad_fn=<AddBackward0>)\n",
            "tensor(24328.1621, grad_fn=<AddBackward0>)\n",
            "tensor(25056.6738, grad_fn=<AddBackward0>)\n",
            "tensor(25200.7207, grad_fn=<AddBackward0>)\n",
            "tensor(25742.6777, grad_fn=<AddBackward0>)\n",
            "tensor(24483.4805, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [450/469], Reconst Loss: 163.5711, KL Div: 1.8471\n",
            "tensor(25863.4805, grad_fn=<AddBackward0>)\n",
            "tensor(25281.5723, grad_fn=<AddBackward0>)\n",
            "tensor(25632.4062, grad_fn=<AddBackward0>)\n",
            "tensor(25498.6602, grad_fn=<AddBackward0>)\n",
            "tensor(24219.6602, grad_fn=<AddBackward0>)\n",
            "tensor(25573.8438, grad_fn=<AddBackward0>)\n",
            "tensor(24448.0703, grad_fn=<AddBackward0>)\n",
            "tensor(25703.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25308.9805, grad_fn=<AddBackward0>)\n",
            "tensor(24968.0020, grad_fn=<AddBackward0>)\n",
            "Epoch[23/25], Step [460/469], Reconst Loss: 167.4164, KL Div: 1.8431\n",
            "tensor(23874.6309, grad_fn=<AddBackward0>)\n",
            "tensor(24754.5117, grad_fn=<AddBackward0>)\n",
            "tensor(25277.8477, grad_fn=<AddBackward0>)\n",
            "tensor(25320.6367, grad_fn=<AddBackward0>)\n",
            "tensor(25312.1738, grad_fn=<AddBackward0>)\n",
            "tensor(25004.1465, grad_fn=<AddBackward0>)\n",
            "tensor(24667.8828, grad_fn=<AddBackward0>)\n",
            "tensor(25197.6152, grad_fn=<AddBackward0>)\n",
            "tensor(17968.7188, grad_fn=<AddBackward0>)\n",
            "tensor(25034.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25274.6953, grad_fn=<AddBackward0>)\n",
            "tensor(24704.3242, grad_fn=<AddBackward0>)\n",
            "tensor(25377.8086, grad_fn=<AddBackward0>)\n",
            "tensor(24873.3516, grad_fn=<AddBackward0>)\n",
            "tensor(25240.8340, grad_fn=<AddBackward0>)\n",
            "tensor(24816.8652, grad_fn=<AddBackward0>)\n",
            "tensor(25778.9453, grad_fn=<AddBackward0>)\n",
            "tensor(26746.5566, grad_fn=<AddBackward0>)\n",
            "tensor(26797.8516, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [10/469], Reconst Loss: 179.7914, KL Div: 1.9711\n",
            "tensor(25676.6230, grad_fn=<AddBackward0>)\n",
            "tensor(24945.1230, grad_fn=<AddBackward0>)\n",
            "tensor(25985.9297, grad_fn=<AddBackward0>)\n",
            "tensor(25604.2520, grad_fn=<AddBackward0>)\n",
            "tensor(24227.0234, grad_fn=<AddBackward0>)\n",
            "tensor(24470.4395, grad_fn=<AddBackward0>)\n",
            "tensor(26038.6641, grad_fn=<AddBackward0>)\n",
            "tensor(24802.4688, grad_fn=<AddBackward0>)\n",
            "tensor(24615.1602, grad_fn=<AddBackward0>)\n",
            "tensor(25697.4941, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [20/469], Reconst Loss: 172.6656, KL Div: 1.8731\n",
            "tensor(25067.8711, grad_fn=<AddBackward0>)\n",
            "tensor(25111.5371, grad_fn=<AddBackward0>)\n",
            "tensor(25307.5508, grad_fn=<AddBackward0>)\n",
            "tensor(25748.3945, grad_fn=<AddBackward0>)\n",
            "tensor(25745.9375, grad_fn=<AddBackward0>)\n",
            "tensor(23620.8652, grad_fn=<AddBackward0>)\n",
            "tensor(25106.2598, grad_fn=<AddBackward0>)\n",
            "tensor(24351.8164, grad_fn=<AddBackward0>)\n",
            "tensor(25256.5547, grad_fn=<AddBackward0>)\n",
            "tensor(25174.0527, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [30/469], Reconst Loss: 167.2873, KL Div: 1.9590\n",
            "tensor(24483.8066, grad_fn=<AddBackward0>)\n",
            "tensor(25712.5176, grad_fn=<AddBackward0>)\n",
            "tensor(25428.7812, grad_fn=<AddBackward0>)\n",
            "tensor(26425.5430, grad_fn=<AddBackward0>)\n",
            "tensor(25669.4512, grad_fn=<AddBackward0>)\n",
            "tensor(25097.4473, grad_fn=<AddBackward0>)\n",
            "tensor(24986.8516, grad_fn=<AddBackward0>)\n",
            "tensor(25081.5449, grad_fn=<AddBackward0>)\n",
            "tensor(25529.5176, grad_fn=<AddBackward0>)\n",
            "tensor(25356.2949, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [40/469], Reconst Loss: 168.4489, KL Div: 1.9765\n",
            "tensor(24609.1680, grad_fn=<AddBackward0>)\n",
            "tensor(26105.2637, grad_fn=<AddBackward0>)\n",
            "tensor(25936.8945, grad_fn=<AddBackward0>)\n",
            "tensor(25675.4824, grad_fn=<AddBackward0>)\n",
            "tensor(25354.1777, grad_fn=<AddBackward0>)\n",
            "tensor(26066.2168, grad_fn=<AddBackward0>)\n",
            "tensor(26094.4043, grad_fn=<AddBackward0>)\n",
            "tensor(24943.5801, grad_fn=<AddBackward0>)\n",
            "tensor(24354.6133, grad_fn=<AddBackward0>)\n",
            "tensor(25826.5430, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [50/469], Reconst Loss: 174.6326, KL Div: 1.8092\n",
            "tensor(25735.8320, grad_fn=<AddBackward0>)\n",
            "tensor(24676.6348, grad_fn=<AddBackward0>)\n",
            "tensor(24678.3379, grad_fn=<AddBackward0>)\n",
            "tensor(24960.7480, grad_fn=<AddBackward0>)\n",
            "tensor(24754.9395, grad_fn=<AddBackward0>)\n",
            "tensor(25463.8828, grad_fn=<AddBackward0>)\n",
            "tensor(24930.9785, grad_fn=<AddBackward0>)\n",
            "tensor(25042.9512, grad_fn=<AddBackward0>)\n",
            "tensor(24406.2285, grad_fn=<AddBackward0>)\n",
            "tensor(25700.1621, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [60/469], Reconst Loss: 172.3289, KL Div: 1.8969\n",
            "tensor(25600.1211, grad_fn=<AddBackward0>)\n",
            "tensor(25908.4434, grad_fn=<AddBackward0>)\n",
            "tensor(24896.7324, grad_fn=<AddBackward0>)\n",
            "tensor(25384.4219, grad_fn=<AddBackward0>)\n",
            "tensor(24605.5840, grad_fn=<AddBackward0>)\n",
            "tensor(24816.6406, grad_fn=<AddBackward0>)\n",
            "tensor(24407.7383, grad_fn=<AddBackward0>)\n",
            "tensor(25705.5273, grad_fn=<AddBackward0>)\n",
            "tensor(25104.0527, grad_fn=<AddBackward0>)\n",
            "tensor(25334.6426, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [70/469], Reconst Loss: 169.9902, KL Div: 1.8624\n",
            "tensor(25021.7441, grad_fn=<AddBackward0>)\n",
            "tensor(24632.2676, grad_fn=<AddBackward0>)\n",
            "tensor(24947.3164, grad_fn=<AddBackward0>)\n",
            "tensor(24936.6250, grad_fn=<AddBackward0>)\n",
            "tensor(25714.4375, grad_fn=<AddBackward0>)\n",
            "tensor(25388.5547, grad_fn=<AddBackward0>)\n",
            "tensor(25326.3926, grad_fn=<AddBackward0>)\n",
            "tensor(25577.6602, grad_fn=<AddBackward0>)\n",
            "tensor(24811.9219, grad_fn=<AddBackward0>)\n",
            "tensor(25703.3633, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [80/469], Reconst Loss: 173.2642, KL Div: 1.8362\n",
            "tensor(25516.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25429.3535, grad_fn=<AddBackward0>)\n",
            "tensor(25694.6465, grad_fn=<AddBackward0>)\n",
            "tensor(25206.2598, grad_fn=<AddBackward0>)\n",
            "tensor(25523.8926, grad_fn=<AddBackward0>)\n",
            "tensor(25106.8398, grad_fn=<AddBackward0>)\n",
            "tensor(25503.6758, grad_fn=<AddBackward0>)\n",
            "tensor(24981.4941, grad_fn=<AddBackward0>)\n",
            "tensor(24778.9453, grad_fn=<AddBackward0>)\n",
            "tensor(24849.5820, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [90/469], Reconst Loss: 166.9783, KL Div: 1.8106\n",
            "tensor(24852.9180, grad_fn=<AddBackward0>)\n",
            "tensor(25478.7305, grad_fn=<AddBackward0>)\n",
            "tensor(24807.4473, grad_fn=<AddBackward0>)\n",
            "tensor(24421.6328, grad_fn=<AddBackward0>)\n",
            "tensor(25541.7148, grad_fn=<AddBackward0>)\n",
            "tensor(24993.6367, grad_fn=<AddBackward0>)\n",
            "tensor(24730.0273, grad_fn=<AddBackward0>)\n",
            "tensor(25782.4316, grad_fn=<AddBackward0>)\n",
            "tensor(25795.1035, grad_fn=<AddBackward0>)\n",
            "tensor(25411.8145, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [100/469], Reconst Loss: 169.0515, KL Div: 1.9652\n",
            "tensor(25455.0527, grad_fn=<AddBackward0>)\n",
            "tensor(25136.6211, grad_fn=<AddBackward0>)\n",
            "tensor(25344.7480, grad_fn=<AddBackward0>)\n",
            "tensor(25808.7891, grad_fn=<AddBackward0>)\n",
            "tensor(25915.8359, grad_fn=<AddBackward0>)\n",
            "tensor(25651.8594, grad_fn=<AddBackward0>)\n",
            "tensor(25452.7832, grad_fn=<AddBackward0>)\n",
            "tensor(25733.8711, grad_fn=<AddBackward0>)\n",
            "tensor(25837.8340, grad_fn=<AddBackward0>)\n",
            "tensor(26183.3789, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [110/469], Reconst Loss: 174.9117, KL Div: 1.9764\n",
            "tensor(24796.3867, grad_fn=<AddBackward0>)\n",
            "tensor(24911.6406, grad_fn=<AddBackward0>)\n",
            "tensor(26094.5859, grad_fn=<AddBackward0>)\n",
            "tensor(24867.7188, grad_fn=<AddBackward0>)\n",
            "tensor(25727.9492, grad_fn=<AddBackward0>)\n",
            "tensor(25390.5703, grad_fn=<AddBackward0>)\n",
            "tensor(25481.3867, grad_fn=<AddBackward0>)\n",
            "tensor(24471.5098, grad_fn=<AddBackward0>)\n",
            "tensor(25220.2266, grad_fn=<AddBackward0>)\n",
            "tensor(25598.9922, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [120/469], Reconst Loss: 172.6748, KL Div: 1.8212\n",
            "tensor(24271.7109, grad_fn=<AddBackward0>)\n",
            "tensor(25637.2598, grad_fn=<AddBackward0>)\n",
            "tensor(25317.4980, grad_fn=<AddBackward0>)\n",
            "tensor(25165.2676, grad_fn=<AddBackward0>)\n",
            "tensor(24522.4297, grad_fn=<AddBackward0>)\n",
            "tensor(25754.4102, grad_fn=<AddBackward0>)\n",
            "tensor(25314.3887, grad_fn=<AddBackward0>)\n",
            "tensor(24676.6113, grad_fn=<AddBackward0>)\n",
            "tensor(25654.0039, grad_fn=<AddBackward0>)\n",
            "tensor(24853.6621, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [130/469], Reconst Loss: 164.0493, KL Div: 2.0080\n",
            "tensor(25981.0801, grad_fn=<AddBackward0>)\n",
            "tensor(25638.7422, grad_fn=<AddBackward0>)\n",
            "tensor(24727.5645, grad_fn=<AddBackward0>)\n",
            "tensor(24758.0234, grad_fn=<AddBackward0>)\n",
            "tensor(25484.3652, grad_fn=<AddBackward0>)\n",
            "tensor(25423.1172, grad_fn=<AddBackward0>)\n",
            "tensor(25853.5664, grad_fn=<AddBackward0>)\n",
            "tensor(25852.9453, grad_fn=<AddBackward0>)\n",
            "tensor(25515.3789, grad_fn=<AddBackward0>)\n",
            "tensor(24965.0781, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [140/469], Reconst Loss: 167.8910, KL Div: 1.8099\n",
            "tensor(24996.4316, grad_fn=<AddBackward0>)\n",
            "tensor(25597.0039, grad_fn=<AddBackward0>)\n",
            "tensor(24585.1016, grad_fn=<AddBackward0>)\n",
            "tensor(24707.3086, grad_fn=<AddBackward0>)\n",
            "tensor(25020.4219, grad_fn=<AddBackward0>)\n",
            "tensor(25164.0938, grad_fn=<AddBackward0>)\n",
            "tensor(23643.1914, grad_fn=<AddBackward0>)\n",
            "tensor(26024.8242, grad_fn=<AddBackward0>)\n",
            "tensor(24622.9922, grad_fn=<AddBackward0>)\n",
            "tensor(25006.6934, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [150/469], Reconst Loss: 166.4277, KL Div: 1.9291\n",
            "tensor(25337.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25438.3145, grad_fn=<AddBackward0>)\n",
            "tensor(24720.5391, grad_fn=<AddBackward0>)\n",
            "tensor(25113.2422, grad_fn=<AddBackward0>)\n",
            "tensor(24775.6172, grad_fn=<AddBackward0>)\n",
            "tensor(26047.1426, grad_fn=<AddBackward0>)\n",
            "tensor(24747.1797, grad_fn=<AddBackward0>)\n",
            "tensor(25646.5273, grad_fn=<AddBackward0>)\n",
            "tensor(25051.3906, grad_fn=<AddBackward0>)\n",
            "tensor(24390.7559, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [160/469], Reconst Loss: 162.1165, KL Div: 1.8958\n",
            "tensor(25363.8477, grad_fn=<AddBackward0>)\n",
            "tensor(24974.9492, grad_fn=<AddBackward0>)\n",
            "tensor(24406.4023, grad_fn=<AddBackward0>)\n",
            "tensor(24681.6660, grad_fn=<AddBackward0>)\n",
            "tensor(25406.4551, grad_fn=<AddBackward0>)\n",
            "tensor(24391.9414, grad_fn=<AddBackward0>)\n",
            "tensor(25680.0312, grad_fn=<AddBackward0>)\n",
            "tensor(25226.6328, grad_fn=<AddBackward0>)\n",
            "tensor(24461.5195, grad_fn=<AddBackward0>)\n",
            "tensor(25463.4121, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [170/469], Reconst Loss: 171.6941, KL Div: 1.8159\n",
            "tensor(25179.0918, grad_fn=<AddBackward0>)\n",
            "tensor(23739.9727, grad_fn=<AddBackward0>)\n",
            "tensor(24782.3262, grad_fn=<AddBackward0>)\n",
            "tensor(25882.5664, grad_fn=<AddBackward0>)\n",
            "tensor(25219.2227, grad_fn=<AddBackward0>)\n",
            "tensor(24590.9941, grad_fn=<AddBackward0>)\n",
            "tensor(25321.9922, grad_fn=<AddBackward0>)\n",
            "tensor(24419.4648, grad_fn=<AddBackward0>)\n",
            "tensor(25327.7773, grad_fn=<AddBackward0>)\n",
            "tensor(25010.4434, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [180/469], Reconst Loss: 165.2888, KL Div: 2.0070\n",
            "tensor(25035.9707, grad_fn=<AddBackward0>)\n",
            "tensor(25068.7246, grad_fn=<AddBackward0>)\n",
            "tensor(25730.2207, grad_fn=<AddBackward0>)\n",
            "tensor(25948.1660, grad_fn=<AddBackward0>)\n",
            "tensor(24960.6660, grad_fn=<AddBackward0>)\n",
            "tensor(24528.9219, grad_fn=<AddBackward0>)\n",
            "tensor(25115.6211, grad_fn=<AddBackward0>)\n",
            "tensor(24867.1562, grad_fn=<AddBackward0>)\n",
            "tensor(25406.6484, grad_fn=<AddBackward0>)\n",
            "tensor(26119.5352, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [190/469], Reconst Loss: 175.6616, KL Div: 1.8932\n",
            "tensor(25085.8086, grad_fn=<AddBackward0>)\n",
            "tensor(25669.9785, grad_fn=<AddBackward0>)\n",
            "tensor(25176.8516, grad_fn=<AddBackward0>)\n",
            "tensor(25471.2188, grad_fn=<AddBackward0>)\n",
            "tensor(24864.0977, grad_fn=<AddBackward0>)\n",
            "tensor(25793.5645, grad_fn=<AddBackward0>)\n",
            "tensor(25064.9766, grad_fn=<AddBackward0>)\n",
            "tensor(25401.4766, grad_fn=<AddBackward0>)\n",
            "tensor(25068.7578, grad_fn=<AddBackward0>)\n",
            "tensor(25030.5586, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [200/469], Reconst Loss: 167.3297, KL Div: 1.8814\n",
            "tensor(24629.2578, grad_fn=<AddBackward0>)\n",
            "tensor(25548.3867, grad_fn=<AddBackward0>)\n",
            "tensor(25048.9453, grad_fn=<AddBackward0>)\n",
            "tensor(24717.7148, grad_fn=<AddBackward0>)\n",
            "tensor(25556.4707, grad_fn=<AddBackward0>)\n",
            "tensor(24060.4277, grad_fn=<AddBackward0>)\n",
            "tensor(24889.8477, grad_fn=<AddBackward0>)\n",
            "tensor(23526.0801, grad_fn=<AddBackward0>)\n",
            "tensor(25437.6641, grad_fn=<AddBackward0>)\n",
            "tensor(25332.7383, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [210/469], Reconst Loss: 172.2809, KL Div: 1.7087\n",
            "tensor(25922.4141, grad_fn=<AddBackward0>)\n",
            "tensor(24439.1367, grad_fn=<AddBackward0>)\n",
            "tensor(26183.5273, grad_fn=<AddBackward0>)\n",
            "tensor(24526.7305, grad_fn=<AddBackward0>)\n",
            "tensor(24296.3203, grad_fn=<AddBackward0>)\n",
            "tensor(24417.9883, grad_fn=<AddBackward0>)\n",
            "tensor(25120.8984, grad_fn=<AddBackward0>)\n",
            "tensor(25938.2441, grad_fn=<AddBackward0>)\n",
            "tensor(24123.5254, grad_fn=<AddBackward0>)\n",
            "tensor(24920.3809, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [220/469], Reconst Loss: 167.2426, KL Div: 1.8299\n",
            "tensor(25325.8086, grad_fn=<AddBackward0>)\n",
            "tensor(25471.6367, grad_fn=<AddBackward0>)\n",
            "tensor(24744.1035, grad_fn=<AddBackward0>)\n",
            "tensor(24351.0586, grad_fn=<AddBackward0>)\n",
            "tensor(24356.0039, grad_fn=<AddBackward0>)\n",
            "tensor(26189.4863, grad_fn=<AddBackward0>)\n",
            "tensor(25248.9238, grad_fn=<AddBackward0>)\n",
            "tensor(25557.3555, grad_fn=<AddBackward0>)\n",
            "tensor(25458.6699, grad_fn=<AddBackward0>)\n",
            "tensor(25174.2051, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [230/469], Reconst Loss: 166.8396, KL Div: 1.9889\n",
            "tensor(24371.4707, grad_fn=<AddBackward0>)\n",
            "tensor(25052.1758, grad_fn=<AddBackward0>)\n",
            "tensor(25783.1699, grad_fn=<AddBackward0>)\n",
            "tensor(24660.1074, grad_fn=<AddBackward0>)\n",
            "tensor(24348.6973, grad_fn=<AddBackward0>)\n",
            "tensor(24939.2871, grad_fn=<AddBackward0>)\n",
            "tensor(25031.5781, grad_fn=<AddBackward0>)\n",
            "tensor(24774.6426, grad_fn=<AddBackward0>)\n",
            "tensor(25171.3984, grad_fn=<AddBackward0>)\n",
            "tensor(25015.6250, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [240/469], Reconst Loss: 166.2029, KL Div: 1.9488\n",
            "tensor(24474.8164, grad_fn=<AddBackward0>)\n",
            "tensor(25488.8105, grad_fn=<AddBackward0>)\n",
            "tensor(25243.6641, grad_fn=<AddBackward0>)\n",
            "tensor(24538.8398, grad_fn=<AddBackward0>)\n",
            "tensor(25121.4121, grad_fn=<AddBackward0>)\n",
            "tensor(25055.3105, grad_fn=<AddBackward0>)\n",
            "tensor(26243.3906, grad_fn=<AddBackward0>)\n",
            "tensor(25573.8477, grad_fn=<AddBackward0>)\n",
            "tensor(24328.8457, grad_fn=<AddBackward0>)\n",
            "tensor(24683.5996, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [250/469], Reconst Loss: 165.7843, KL Div: 1.8038\n",
            "tensor(26139.9023, grad_fn=<AddBackward0>)\n",
            "tensor(24664.2441, grad_fn=<AddBackward0>)\n",
            "tensor(25005.2910, grad_fn=<AddBackward0>)\n",
            "tensor(26024.7754, grad_fn=<AddBackward0>)\n",
            "tensor(24788.9844, grad_fn=<AddBackward0>)\n",
            "tensor(24386.8809, grad_fn=<AddBackward0>)\n",
            "tensor(25401.8125, grad_fn=<AddBackward0>)\n",
            "tensor(25003.3223, grad_fn=<AddBackward0>)\n",
            "tensor(25388.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25070.6797, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [260/469], Reconst Loss: 167.8015, KL Div: 1.8709\n",
            "tensor(25218.0977, grad_fn=<AddBackward0>)\n",
            "tensor(24671.4844, grad_fn=<AddBackward0>)\n",
            "tensor(24994.9648, grad_fn=<AddBackward0>)\n",
            "tensor(25345.4180, grad_fn=<AddBackward0>)\n",
            "tensor(24917.1211, grad_fn=<AddBackward0>)\n",
            "tensor(25540.2812, grad_fn=<AddBackward0>)\n",
            "tensor(24602.7539, grad_fn=<AddBackward0>)\n",
            "tensor(26161.1367, grad_fn=<AddBackward0>)\n",
            "tensor(25272.7598, grad_fn=<AddBackward0>)\n",
            "tensor(25541.1367, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [270/469], Reconst Loss: 169.1482, KL Div: 2.0261\n",
            "tensor(25333.2422, grad_fn=<AddBackward0>)\n",
            "tensor(25226.8008, grad_fn=<AddBackward0>)\n",
            "tensor(25626.0508, grad_fn=<AddBackward0>)\n",
            "tensor(25240.4082, grad_fn=<AddBackward0>)\n",
            "tensor(24812.7578, grad_fn=<AddBackward0>)\n",
            "tensor(24709.8223, grad_fn=<AddBackward0>)\n",
            "tensor(25379.3164, grad_fn=<AddBackward0>)\n",
            "tensor(25622.6191, grad_fn=<AddBackward0>)\n",
            "tensor(25564.6328, grad_fn=<AddBackward0>)\n",
            "tensor(26009.8379, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [280/469], Reconst Loss: 175.4267, KL Div: 1.8517\n",
            "tensor(25782.5234, grad_fn=<AddBackward0>)\n",
            "tensor(24577.4688, grad_fn=<AddBackward0>)\n",
            "tensor(24408.6758, grad_fn=<AddBackward0>)\n",
            "tensor(25071.4688, grad_fn=<AddBackward0>)\n",
            "tensor(25354.7266, grad_fn=<AddBackward0>)\n",
            "tensor(25854.6621, grad_fn=<AddBackward0>)\n",
            "tensor(25417.4336, grad_fn=<AddBackward0>)\n",
            "tensor(25449.1699, grad_fn=<AddBackward0>)\n",
            "tensor(24542.8320, grad_fn=<AddBackward0>)\n",
            "tensor(26102.0938, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [290/469], Reconst Loss: 174.3322, KL Div: 1.9727\n",
            "tensor(24798.0684, grad_fn=<AddBackward0>)\n",
            "tensor(25514.4004, grad_fn=<AddBackward0>)\n",
            "tensor(25261., grad_fn=<AddBackward0>)\n",
            "tensor(24963.4961, grad_fn=<AddBackward0>)\n",
            "tensor(24923.3359, grad_fn=<AddBackward0>)\n",
            "tensor(25429.1055, grad_fn=<AddBackward0>)\n",
            "tensor(24925.0664, grad_fn=<AddBackward0>)\n",
            "tensor(25477.9062, grad_fn=<AddBackward0>)\n",
            "tensor(25206.0547, grad_fn=<AddBackward0>)\n",
            "tensor(25679.0410, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [300/469], Reconst Loss: 172.9016, KL Div: 1.8477\n",
            "tensor(25205.6777, grad_fn=<AddBackward0>)\n",
            "tensor(24503.6523, grad_fn=<AddBackward0>)\n",
            "tensor(24559.4180, grad_fn=<AddBackward0>)\n",
            "tensor(24665.1035, grad_fn=<AddBackward0>)\n",
            "tensor(25162.5684, grad_fn=<AddBackward0>)\n",
            "tensor(24917.8027, grad_fn=<AddBackward0>)\n",
            "tensor(26025.7812, grad_fn=<AddBackward0>)\n",
            "tensor(26006.6133, grad_fn=<AddBackward0>)\n",
            "tensor(25107.1211, grad_fn=<AddBackward0>)\n",
            "tensor(24508.9414, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [310/469], Reconst Loss: 164.3228, KL Div: 1.8102\n",
            "tensor(25259.8711, grad_fn=<AddBackward0>)\n",
            "tensor(25486.6680, grad_fn=<AddBackward0>)\n",
            "tensor(26158.5938, grad_fn=<AddBackward0>)\n",
            "tensor(24281.6211, grad_fn=<AddBackward0>)\n",
            "tensor(25561.7598, grad_fn=<AddBackward0>)\n",
            "tensor(26099.7266, grad_fn=<AddBackward0>)\n",
            "tensor(24431.5508, grad_fn=<AddBackward0>)\n",
            "tensor(24327.0879, grad_fn=<AddBackward0>)\n",
            "tensor(24698.1172, grad_fn=<AddBackward0>)\n",
            "tensor(25472.3730, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [320/469], Reconst Loss: 172.2374, KL Div: 1.7844\n",
            "tensor(24204.4785, grad_fn=<AddBackward0>)\n",
            "tensor(26580.0234, grad_fn=<AddBackward0>)\n",
            "tensor(25546.8867, grad_fn=<AddBackward0>)\n",
            "tensor(25317.6191, grad_fn=<AddBackward0>)\n",
            "tensor(24522.1309, grad_fn=<AddBackward0>)\n",
            "tensor(25607.4062, grad_fn=<AddBackward0>)\n",
            "tensor(25657.5664, grad_fn=<AddBackward0>)\n",
            "tensor(24351.7109, grad_fn=<AddBackward0>)\n",
            "tensor(25432.8281, grad_fn=<AddBackward0>)\n",
            "tensor(25028.4746, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [330/469], Reconst Loss: 167.1572, KL Div: 1.8918\n",
            "tensor(25182.7383, grad_fn=<AddBackward0>)\n",
            "tensor(25481.4414, grad_fn=<AddBackward0>)\n",
            "tensor(24607.8359, grad_fn=<AddBackward0>)\n",
            "tensor(25636.6250, grad_fn=<AddBackward0>)\n",
            "tensor(24919.8535, grad_fn=<AddBackward0>)\n",
            "tensor(25071.0898, grad_fn=<AddBackward0>)\n",
            "tensor(23750.0938, grad_fn=<AddBackward0>)\n",
            "tensor(25921.3906, grad_fn=<AddBackward0>)\n",
            "tensor(25425.7871, grad_fn=<AddBackward0>)\n",
            "tensor(25279.7461, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [340/469], Reconst Loss: 168.1981, KL Div: 1.9533\n",
            "tensor(25686.4551, grad_fn=<AddBackward0>)\n",
            "tensor(24369.1992, grad_fn=<AddBackward0>)\n",
            "tensor(24686.0332, grad_fn=<AddBackward0>)\n",
            "tensor(24804.1660, grad_fn=<AddBackward0>)\n",
            "tensor(24920.7305, grad_fn=<AddBackward0>)\n",
            "tensor(25304.9551, grad_fn=<AddBackward0>)\n",
            "tensor(25163.3613, grad_fn=<AddBackward0>)\n",
            "tensor(25345.5234, grad_fn=<AddBackward0>)\n",
            "tensor(25359.0488, grad_fn=<AddBackward0>)\n",
            "tensor(25030.1016, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [350/469], Reconst Loss: 168.4834, KL Div: 1.8043\n",
            "tensor(24852.9219, grad_fn=<AddBackward0>)\n",
            "tensor(25130.3438, grad_fn=<AddBackward0>)\n",
            "tensor(25276.3789, grad_fn=<AddBackward0>)\n",
            "tensor(24610.3672, grad_fn=<AddBackward0>)\n",
            "tensor(25977.8066, grad_fn=<AddBackward0>)\n",
            "tensor(24597.5254, grad_fn=<AddBackward0>)\n",
            "tensor(25741.9414, grad_fn=<AddBackward0>)\n",
            "tensor(25817.8242, grad_fn=<AddBackward0>)\n",
            "tensor(25906.2852, grad_fn=<AddBackward0>)\n",
            "tensor(25252.9688, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [360/469], Reconst Loss: 169.8199, KL Div: 1.8313\n",
            "tensor(24976.0918, grad_fn=<AddBackward0>)\n",
            "tensor(25800.7168, grad_fn=<AddBackward0>)\n",
            "tensor(25011.0391, grad_fn=<AddBackward0>)\n",
            "tensor(24974.5078, grad_fn=<AddBackward0>)\n",
            "tensor(24809.8281, grad_fn=<AddBackward0>)\n",
            "tensor(25109.1582, grad_fn=<AddBackward0>)\n",
            "tensor(25565.1055, grad_fn=<AddBackward0>)\n",
            "tensor(25778.6641, grad_fn=<AddBackward0>)\n",
            "tensor(24373.7031, grad_fn=<AddBackward0>)\n",
            "tensor(24994.2793, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [370/469], Reconst Loss: 163.6309, KL Div: 2.1091\n",
            "tensor(25375.2207, grad_fn=<AddBackward0>)\n",
            "tensor(25295.3457, grad_fn=<AddBackward0>)\n",
            "tensor(26127.0234, grad_fn=<AddBackward0>)\n",
            "tensor(25749.2246, grad_fn=<AddBackward0>)\n",
            "tensor(25661.6523, grad_fn=<AddBackward0>)\n",
            "tensor(24560.9473, grad_fn=<AddBackward0>)\n",
            "tensor(24963.0508, grad_fn=<AddBackward0>)\n",
            "tensor(24835.5625, grad_fn=<AddBackward0>)\n",
            "tensor(24425.2539, grad_fn=<AddBackward0>)\n",
            "tensor(25319.3203, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [380/469], Reconst Loss: 168.7467, KL Div: 1.9374\n",
            "tensor(25303.0430, grad_fn=<AddBackward0>)\n",
            "tensor(24522.5059, grad_fn=<AddBackward0>)\n",
            "tensor(25155.3965, grad_fn=<AddBackward0>)\n",
            "tensor(24489.6719, grad_fn=<AddBackward0>)\n",
            "tensor(25063.2266, grad_fn=<AddBackward0>)\n",
            "tensor(25657.0176, grad_fn=<AddBackward0>)\n",
            "tensor(25172.2031, grad_fn=<AddBackward0>)\n",
            "tensor(24853.0352, grad_fn=<AddBackward0>)\n",
            "tensor(24977.5098, grad_fn=<AddBackward0>)\n",
            "tensor(24858.4629, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [390/469], Reconst Loss: 165.7043, KL Div: 1.9002\n",
            "tensor(24662.7207, grad_fn=<AddBackward0>)\n",
            "tensor(25290.7109, grad_fn=<AddBackward0>)\n",
            "tensor(24975.9883, grad_fn=<AddBackward0>)\n",
            "tensor(25629.1133, grad_fn=<AddBackward0>)\n",
            "tensor(25137.0312, grad_fn=<AddBackward0>)\n",
            "tensor(25505.1250, grad_fn=<AddBackward0>)\n",
            "tensor(24997.9316, grad_fn=<AddBackward0>)\n",
            "tensor(25280.9258, grad_fn=<AddBackward0>)\n",
            "tensor(24937.1738, grad_fn=<AddBackward0>)\n",
            "tensor(25401.6914, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [400/469], Reconst Loss: 168.6231, KL Div: 1.9885\n",
            "tensor(25103.3359, grad_fn=<AddBackward0>)\n",
            "tensor(25795.9766, grad_fn=<AddBackward0>)\n",
            "tensor(24919.1172, grad_fn=<AddBackward0>)\n",
            "tensor(24897.9570, grad_fn=<AddBackward0>)\n",
            "tensor(25544.8750, grad_fn=<AddBackward0>)\n",
            "tensor(24583.8379, grad_fn=<AddBackward0>)\n",
            "tensor(25043.6484, grad_fn=<AddBackward0>)\n",
            "tensor(24627.0508, grad_fn=<AddBackward0>)\n",
            "tensor(24834.6016, grad_fn=<AddBackward0>)\n",
            "tensor(26210.1914, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [410/469], Reconst Loss: 174.3317, KL Div: 2.0290\n",
            "tensor(25130.1074, grad_fn=<AddBackward0>)\n",
            "tensor(24516.4199, grad_fn=<AddBackward0>)\n",
            "tensor(25611.3379, grad_fn=<AddBackward0>)\n",
            "tensor(25307.5801, grad_fn=<AddBackward0>)\n",
            "tensor(25967.2324, grad_fn=<AddBackward0>)\n",
            "tensor(25607.3555, grad_fn=<AddBackward0>)\n",
            "tensor(24561.6016, grad_fn=<AddBackward0>)\n",
            "tensor(24595.3926, grad_fn=<AddBackward0>)\n",
            "tensor(24608.2500, grad_fn=<AddBackward0>)\n",
            "tensor(26565.6348, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [420/469], Reconst Loss: 179.4317, KL Div: 1.8742\n",
            "tensor(25147.4707, grad_fn=<AddBackward0>)\n",
            "tensor(24482.9648, grad_fn=<AddBackward0>)\n",
            "tensor(24985.6855, grad_fn=<AddBackward0>)\n",
            "tensor(25541.1973, grad_fn=<AddBackward0>)\n",
            "tensor(25164.5781, grad_fn=<AddBackward0>)\n",
            "tensor(26360.3984, grad_fn=<AddBackward0>)\n",
            "tensor(25713.6035, grad_fn=<AddBackward0>)\n",
            "tensor(24812.3066, grad_fn=<AddBackward0>)\n",
            "tensor(25040.6895, grad_fn=<AddBackward0>)\n",
            "tensor(24810.6172, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [430/469], Reconst Loss: 166.7784, KL Div: 1.8036\n",
            "tensor(25644.6484, grad_fn=<AddBackward0>)\n",
            "tensor(25605.1719, grad_fn=<AddBackward0>)\n",
            "tensor(25229.7715, grad_fn=<AddBackward0>)\n",
            "tensor(25672.3789, grad_fn=<AddBackward0>)\n",
            "tensor(24736.0469, grad_fn=<AddBackward0>)\n",
            "tensor(24414.2461, grad_fn=<AddBackward0>)\n",
            "tensor(24594.2617, grad_fn=<AddBackward0>)\n",
            "tensor(25671.3105, grad_fn=<AddBackward0>)\n",
            "tensor(26257.5527, grad_fn=<AddBackward0>)\n",
            "tensor(24845.1934, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [440/469], Reconst Loss: 168.7743, KL Div: 1.6886\n",
            "tensor(24585.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25026.9414, grad_fn=<AddBackward0>)\n",
            "tensor(25568.8125, grad_fn=<AddBackward0>)\n",
            "tensor(25095.3379, grad_fn=<AddBackward0>)\n",
            "tensor(25598.8750, grad_fn=<AddBackward0>)\n",
            "tensor(25217.2051, grad_fn=<AddBackward0>)\n",
            "tensor(26091.4297, grad_fn=<AddBackward0>)\n",
            "tensor(26252.0391, grad_fn=<AddBackward0>)\n",
            "tensor(25603.1113, grad_fn=<AddBackward0>)\n",
            "tensor(24499.2520, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [450/469], Reconst Loss: 164.2123, KL Div: 1.8125\n",
            "tensor(25079.3086, grad_fn=<AddBackward0>)\n",
            "tensor(25084.9180, grad_fn=<AddBackward0>)\n",
            "tensor(25260.8105, grad_fn=<AddBackward0>)\n",
            "tensor(24994.5566, grad_fn=<AddBackward0>)\n",
            "tensor(26003.9121, grad_fn=<AddBackward0>)\n",
            "tensor(24953.9668, grad_fn=<AddBackward0>)\n",
            "tensor(25094.5918, grad_fn=<AddBackward0>)\n",
            "tensor(23922.1270, grad_fn=<AddBackward0>)\n",
            "tensor(25757.6543, grad_fn=<AddBackward0>)\n",
            "tensor(24941.6406, grad_fn=<AddBackward0>)\n",
            "Epoch[24/25], Step [460/469], Reconst Loss: 167.4433, KL Div: 1.8276\n",
            "tensor(25030.8242, grad_fn=<AddBackward0>)\n",
            "tensor(25503.3164, grad_fn=<AddBackward0>)\n",
            "tensor(24404.2734, grad_fn=<AddBackward0>)\n",
            "tensor(25137.8184, grad_fn=<AddBackward0>)\n",
            "tensor(24497.5098, grad_fn=<AddBackward0>)\n",
            "tensor(25792.8594, grad_fn=<AddBackward0>)\n",
            "tensor(25487.8945, grad_fn=<AddBackward0>)\n",
            "tensor(25328.9375, grad_fn=<AddBackward0>)\n",
            "tensor(19043.5176, grad_fn=<AddBackward0>)\n",
            "tensor(24348.3730, grad_fn=<AddBackward0>)\n",
            "tensor(24358.2051, grad_fn=<AddBackward0>)\n",
            "tensor(24918.8867, grad_fn=<AddBackward0>)\n",
            "tensor(24730.8691, grad_fn=<AddBackward0>)\n",
            "tensor(24463.3125, grad_fn=<AddBackward0>)\n",
            "tensor(25253.9785, grad_fn=<AddBackward0>)\n",
            "tensor(26162.0273, grad_fn=<AddBackward0>)\n",
            "tensor(25744.2305, grad_fn=<AddBackward0>)\n",
            "tensor(24655.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25550.3945, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [10/469], Reconst Loss: 173.5465, KL Div: 1.7377\n",
            "tensor(24561.0254, grad_fn=<AddBackward0>)\n",
            "tensor(24637.6895, grad_fn=<AddBackward0>)\n",
            "tensor(24736.9648, grad_fn=<AddBackward0>)\n",
            "tensor(24931.6738, grad_fn=<AddBackward0>)\n",
            "tensor(25718.1641, grad_fn=<AddBackward0>)\n",
            "tensor(25569.2910, grad_fn=<AddBackward0>)\n",
            "tensor(24838.1367, grad_fn=<AddBackward0>)\n",
            "tensor(25192.4668, grad_fn=<AddBackward0>)\n",
            "tensor(25816.1211, grad_fn=<AddBackward0>)\n",
            "tensor(24990.7363, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [20/469], Reconst Loss: 166.2150, KL Div: 1.9350\n",
            "tensor(25296.3555, grad_fn=<AddBackward0>)\n",
            "tensor(25983.0039, grad_fn=<AddBackward0>)\n",
            "tensor(24824.6973, grad_fn=<AddBackward0>)\n",
            "tensor(24720.1602, grad_fn=<AddBackward0>)\n",
            "tensor(25319.8242, grad_fn=<AddBackward0>)\n",
            "tensor(25013.9414, grad_fn=<AddBackward0>)\n",
            "tensor(25649.9961, grad_fn=<AddBackward0>)\n",
            "tensor(24932.9805, grad_fn=<AddBackward0>)\n",
            "tensor(25232.0527, grad_fn=<AddBackward0>)\n",
            "tensor(26486.7168, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [30/469], Reconst Loss: 180.1058, KL Div: 1.7881\n",
            "tensor(24972.8457, grad_fn=<AddBackward0>)\n",
            "tensor(25149.6641, grad_fn=<AddBackward0>)\n",
            "tensor(26063.7969, grad_fn=<AddBackward0>)\n",
            "tensor(25533.7832, grad_fn=<AddBackward0>)\n",
            "tensor(25368.4102, grad_fn=<AddBackward0>)\n",
            "tensor(24238.1387, grad_fn=<AddBackward0>)\n",
            "tensor(24211.3438, grad_fn=<AddBackward0>)\n",
            "tensor(25968.8359, grad_fn=<AddBackward0>)\n",
            "tensor(24975.3496, grad_fn=<AddBackward0>)\n",
            "tensor(24909.1816, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [40/469], Reconst Loss: 166.1039, KL Div: 1.8999\n",
            "tensor(24944., grad_fn=<AddBackward0>)\n",
            "tensor(24480.7246, grad_fn=<AddBackward0>)\n",
            "tensor(24981.5742, grad_fn=<AddBackward0>)\n",
            "tensor(24687.0781, grad_fn=<AddBackward0>)\n",
            "tensor(25022.9434, grad_fn=<AddBackward0>)\n",
            "tensor(25831.5879, grad_fn=<AddBackward0>)\n",
            "tensor(25177.4668, grad_fn=<AddBackward0>)\n",
            "tensor(24562.9297, grad_fn=<AddBackward0>)\n",
            "tensor(24914.0371, grad_fn=<AddBackward0>)\n",
            "tensor(25525.5859, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [50/469], Reconst Loss: 169.8015, KL Div: 1.9745\n",
            "tensor(24132.0977, grad_fn=<AddBackward0>)\n",
            "tensor(24748.6543, grad_fn=<AddBackward0>)\n",
            "tensor(25936.7480, grad_fn=<AddBackward0>)\n",
            "tensor(26248.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25556.5156, grad_fn=<AddBackward0>)\n",
            "tensor(25193.3613, grad_fn=<AddBackward0>)\n",
            "tensor(25317.2891, grad_fn=<AddBackward0>)\n",
            "tensor(25373.0059, grad_fn=<AddBackward0>)\n",
            "tensor(23831.2734, grad_fn=<AddBackward0>)\n",
            "tensor(25035.9297, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [60/469], Reconst Loss: 166.4747, KL Div: 1.9412\n",
            "tensor(24858.9180, grad_fn=<AddBackward0>)\n",
            "tensor(24988.0371, grad_fn=<AddBackward0>)\n",
            "tensor(24790.9297, grad_fn=<AddBackward0>)\n",
            "tensor(25201.2266, grad_fn=<AddBackward0>)\n",
            "tensor(24761.3320, grad_fn=<AddBackward0>)\n",
            "tensor(25320.8438, grad_fn=<AddBackward0>)\n",
            "tensor(25406.0586, grad_fn=<AddBackward0>)\n",
            "tensor(24625.5938, grad_fn=<AddBackward0>)\n",
            "tensor(26093.4258, grad_fn=<AddBackward0>)\n",
            "tensor(27048.3672, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [70/469], Reconst Loss: 182.4894, KL Div: 1.9217\n",
            "tensor(24999.6875, grad_fn=<AddBackward0>)\n",
            "tensor(25340.1738, grad_fn=<AddBackward0>)\n",
            "tensor(24759.0078, grad_fn=<AddBackward0>)\n",
            "tensor(24834.2168, grad_fn=<AddBackward0>)\n",
            "tensor(25426.9512, grad_fn=<AddBackward0>)\n",
            "tensor(25436.1445, grad_fn=<AddBackward0>)\n",
            "tensor(24843.7676, grad_fn=<AddBackward0>)\n",
            "tensor(26053.9082, grad_fn=<AddBackward0>)\n",
            "tensor(24258.1328, grad_fn=<AddBackward0>)\n",
            "tensor(24700.7910, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [80/469], Reconst Loss: 166.1970, KL Div: 1.7852\n",
            "tensor(24703.5996, grad_fn=<AddBackward0>)\n",
            "tensor(25041.6504, grad_fn=<AddBackward0>)\n",
            "tensor(24948.9160, grad_fn=<AddBackward0>)\n",
            "tensor(25101.9551, grad_fn=<AddBackward0>)\n",
            "tensor(25305.2090, grad_fn=<AddBackward0>)\n",
            "tensor(25953.7168, grad_fn=<AddBackward0>)\n",
            "tensor(24565.1484, grad_fn=<AddBackward0>)\n",
            "tensor(25031.0918, grad_fn=<AddBackward0>)\n",
            "tensor(24939.1738, grad_fn=<AddBackward0>)\n",
            "tensor(24642.8203, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [90/469], Reconst Loss: 163.7581, KL Div: 1.9176\n",
            "tensor(24778.7090, grad_fn=<AddBackward0>)\n",
            "tensor(25639.1074, grad_fn=<AddBackward0>)\n",
            "tensor(25086.2520, grad_fn=<AddBackward0>)\n",
            "tensor(25348.6445, grad_fn=<AddBackward0>)\n",
            "tensor(24682.2168, grad_fn=<AddBackward0>)\n",
            "tensor(25039.1465, grad_fn=<AddBackward0>)\n",
            "tensor(25349.3633, grad_fn=<AddBackward0>)\n",
            "tensor(25135.7617, grad_fn=<AddBackward0>)\n",
            "tensor(25079.9492, grad_fn=<AddBackward0>)\n",
            "tensor(25696.5410, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [100/469], Reconst Loss: 172.4951, KL Div: 1.8839\n",
            "tensor(25688.9961, grad_fn=<AddBackward0>)\n",
            "tensor(25104.5039, grad_fn=<AddBackward0>)\n",
            "tensor(24684.3340, grad_fn=<AddBackward0>)\n",
            "tensor(24533.7188, grad_fn=<AddBackward0>)\n",
            "tensor(26852.2207, grad_fn=<AddBackward0>)\n",
            "tensor(24636.7109, grad_fn=<AddBackward0>)\n",
            "tensor(25407.3223, grad_fn=<AddBackward0>)\n",
            "tensor(25719.8047, grad_fn=<AddBackward0>)\n",
            "tensor(24369.1230, grad_fn=<AddBackward0>)\n",
            "tensor(25823.8047, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [110/469], Reconst Loss: 171.5397, KL Div: 2.0139\n",
            "tensor(25380.6074, grad_fn=<AddBackward0>)\n",
            "tensor(25982.4492, grad_fn=<AddBackward0>)\n",
            "tensor(24737.4434, grad_fn=<AddBackward0>)\n",
            "tensor(25179.9883, grad_fn=<AddBackward0>)\n",
            "tensor(26274.1445, grad_fn=<AddBackward0>)\n",
            "tensor(25617.1875, grad_fn=<AddBackward0>)\n",
            "tensor(24830.8105, grad_fn=<AddBackward0>)\n",
            "tensor(25856.2402, grad_fn=<AddBackward0>)\n",
            "tensor(25057.2930, grad_fn=<AddBackward0>)\n",
            "tensor(25957.1992, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [120/469], Reconst Loss: 173.0111, KL Div: 1.9853\n",
            "tensor(25357.2988, grad_fn=<AddBackward0>)\n",
            "tensor(24605.7051, grad_fn=<AddBackward0>)\n",
            "tensor(25596.1504, grad_fn=<AddBackward0>)\n",
            "tensor(24793.0176, grad_fn=<AddBackward0>)\n",
            "tensor(25890.0703, grad_fn=<AddBackward0>)\n",
            "tensor(24978.3418, grad_fn=<AddBackward0>)\n",
            "tensor(25284.6016, grad_fn=<AddBackward0>)\n",
            "tensor(25832.2422, grad_fn=<AddBackward0>)\n",
            "tensor(25935.8770, grad_fn=<AddBackward0>)\n",
            "tensor(24885.6738, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [130/469], Reconst Loss: 166.2467, KL Div: 1.8782\n",
            "tensor(25103.1797, grad_fn=<AddBackward0>)\n",
            "tensor(24632.5117, grad_fn=<AddBackward0>)\n",
            "tensor(24506.4102, grad_fn=<AddBackward0>)\n",
            "tensor(24193.2305, grad_fn=<AddBackward0>)\n",
            "tensor(25328.3535, grad_fn=<AddBackward0>)\n",
            "tensor(24499.6719, grad_fn=<AddBackward0>)\n",
            "tensor(25578.3555, grad_fn=<AddBackward0>)\n",
            "tensor(26433.6504, grad_fn=<AddBackward0>)\n",
            "tensor(25592.7852, grad_fn=<AddBackward0>)\n",
            "tensor(24640.6680, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [140/469], Reconst Loss: 162.2757, KL Div: 2.0153\n",
            "tensor(25046.1895, grad_fn=<AddBackward0>)\n",
            "tensor(25405.3281, grad_fn=<AddBackward0>)\n",
            "tensor(26342.0254, grad_fn=<AddBackward0>)\n",
            "tensor(24880.8418, grad_fn=<AddBackward0>)\n",
            "tensor(24739.4883, grad_fn=<AddBackward0>)\n",
            "tensor(25528.1816, grad_fn=<AddBackward0>)\n",
            "tensor(24886.1758, grad_fn=<AddBackward0>)\n",
            "tensor(26256.6016, grad_fn=<AddBackward0>)\n",
            "tensor(25502.4727, grad_fn=<AddBackward0>)\n",
            "tensor(24550.0156, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [150/469], Reconst Loss: 162.6602, KL Div: 1.9425\n",
            "tensor(25440.8457, grad_fn=<AddBackward0>)\n",
            "tensor(25924.8789, grad_fn=<AddBackward0>)\n",
            "tensor(24841.8086, grad_fn=<AddBackward0>)\n",
            "tensor(25630.6152, grad_fn=<AddBackward0>)\n",
            "tensor(25850.0977, grad_fn=<AddBackward0>)\n",
            "tensor(24752.3730, grad_fn=<AddBackward0>)\n",
            "tensor(24713.2461, grad_fn=<AddBackward0>)\n",
            "tensor(24277.8066, grad_fn=<AddBackward0>)\n",
            "tensor(25445.7188, grad_fn=<AddBackward0>)\n",
            "tensor(25046.9512, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [160/469], Reconst Loss: 166.3725, KL Div: 1.9538\n",
            "tensor(25644.2832, grad_fn=<AddBackward0>)\n",
            "tensor(26022., grad_fn=<AddBackward0>)\n",
            "tensor(25452.1074, grad_fn=<AddBackward0>)\n",
            "tensor(25405.4648, grad_fn=<AddBackward0>)\n",
            "tensor(25237.5449, grad_fn=<AddBackward0>)\n",
            "tensor(25802.3340, grad_fn=<AddBackward0>)\n",
            "tensor(24933.1914, grad_fn=<AddBackward0>)\n",
            "tensor(25436.2969, grad_fn=<AddBackward0>)\n",
            "tensor(24621.7188, grad_fn=<AddBackward0>)\n",
            "tensor(25557.3379, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [170/469], Reconst Loss: 170.9678, KL Div: 1.9133\n",
            "tensor(25042.0469, grad_fn=<AddBackward0>)\n",
            "tensor(23977.7559, grad_fn=<AddBackward0>)\n",
            "tensor(25588.7109, grad_fn=<AddBackward0>)\n",
            "tensor(25283.2031, grad_fn=<AddBackward0>)\n",
            "tensor(25623.2715, grad_fn=<AddBackward0>)\n",
            "tensor(24969.1094, grad_fn=<AddBackward0>)\n",
            "tensor(25295.9668, grad_fn=<AddBackward0>)\n",
            "tensor(25056.8984, grad_fn=<AddBackward0>)\n",
            "tensor(25569.8398, grad_fn=<AddBackward0>)\n",
            "tensor(25322.9863, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [180/469], Reconst Loss: 169.7903, KL Div: 1.8697\n",
            "tensor(25957.4141, grad_fn=<AddBackward0>)\n",
            "tensor(24832.3203, grad_fn=<AddBackward0>)\n",
            "tensor(25133.6953, grad_fn=<AddBackward0>)\n",
            "tensor(25566.5352, grad_fn=<AddBackward0>)\n",
            "tensor(25056.2871, grad_fn=<AddBackward0>)\n",
            "tensor(25773.5488, grad_fn=<AddBackward0>)\n",
            "tensor(26556.7051, grad_fn=<AddBackward0>)\n",
            "tensor(25266.6621, grad_fn=<AddBackward0>)\n",
            "tensor(24419.6172, grad_fn=<AddBackward0>)\n",
            "tensor(25598.9258, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [190/469], Reconst Loss: 171.6216, KL Div: 1.8913\n",
            "tensor(24927.3672, grad_fn=<AddBackward0>)\n",
            "tensor(25284.1230, grad_fn=<AddBackward0>)\n",
            "tensor(25484.4805, grad_fn=<AddBackward0>)\n",
            "tensor(24182.1055, grad_fn=<AddBackward0>)\n",
            "tensor(24872.0254, grad_fn=<AddBackward0>)\n",
            "tensor(25924.8242, grad_fn=<AddBackward0>)\n",
            "tensor(24360.2383, grad_fn=<AddBackward0>)\n",
            "tensor(26478.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25745.0449, grad_fn=<AddBackward0>)\n",
            "tensor(25362.3906, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [200/469], Reconst Loss: 170.1824, KL Div: 1.8641\n",
            "tensor(24873.0605, grad_fn=<AddBackward0>)\n",
            "tensor(25107.5762, grad_fn=<AddBackward0>)\n",
            "tensor(25462.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25209.9355, grad_fn=<AddBackward0>)\n",
            "tensor(24866.7129, grad_fn=<AddBackward0>)\n",
            "tensor(25566.1094, grad_fn=<AddBackward0>)\n",
            "tensor(25088.1211, grad_fn=<AddBackward0>)\n",
            "tensor(24952.1016, grad_fn=<AddBackward0>)\n",
            "tensor(26054.3262, grad_fn=<AddBackward0>)\n",
            "tensor(25355.1074, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [210/469], Reconst Loss: 170.0610, KL Div: 1.8684\n",
            "tensor(24562.6758, grad_fn=<AddBackward0>)\n",
            "tensor(24280.9629, grad_fn=<AddBackward0>)\n",
            "tensor(25671.0547, grad_fn=<AddBackward0>)\n",
            "tensor(25028.9355, grad_fn=<AddBackward0>)\n",
            "tensor(25202.0879, grad_fn=<AddBackward0>)\n",
            "tensor(25509.2676, grad_fn=<AddBackward0>)\n",
            "tensor(25426.6367, grad_fn=<AddBackward0>)\n",
            "tensor(25448.1855, grad_fn=<AddBackward0>)\n",
            "tensor(25028.0605, grad_fn=<AddBackward0>)\n",
            "tensor(24435.7051, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [220/469], Reconst Loss: 164.2885, KL Div: 1.7744\n",
            "tensor(25071.5332, grad_fn=<AddBackward0>)\n",
            "tensor(25235.8027, grad_fn=<AddBackward0>)\n",
            "tensor(25561.4043, grad_fn=<AddBackward0>)\n",
            "tensor(25634.7266, grad_fn=<AddBackward0>)\n",
            "tensor(25286.4375, grad_fn=<AddBackward0>)\n",
            "tensor(24900.5586, grad_fn=<AddBackward0>)\n",
            "tensor(26211.9180, grad_fn=<AddBackward0>)\n",
            "tensor(24924.3848, grad_fn=<AddBackward0>)\n",
            "tensor(24425.6523, grad_fn=<AddBackward0>)\n",
            "tensor(24054.8730, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [230/469], Reconst Loss: 160.6782, KL Div: 1.8167\n",
            "tensor(25496.2109, grad_fn=<AddBackward0>)\n",
            "tensor(25723.8203, grad_fn=<AddBackward0>)\n",
            "tensor(24456.8047, grad_fn=<AddBackward0>)\n",
            "tensor(25276.6094, grad_fn=<AddBackward0>)\n",
            "tensor(25599.5605, grad_fn=<AddBackward0>)\n",
            "tensor(24940.9648, grad_fn=<AddBackward0>)\n",
            "tensor(25056.8223, grad_fn=<AddBackward0>)\n",
            "tensor(24923.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25947.7793, grad_fn=<AddBackward0>)\n",
            "tensor(24651.1953, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [240/469], Reconst Loss: 164.3449, KL Div: 1.8828\n",
            "tensor(25604.0957, grad_fn=<AddBackward0>)\n",
            "tensor(24868.7383, grad_fn=<AddBackward0>)\n",
            "tensor(25510.8730, grad_fn=<AddBackward0>)\n",
            "tensor(25291.2422, grad_fn=<AddBackward0>)\n",
            "tensor(25652.9688, grad_fn=<AddBackward0>)\n",
            "tensor(24830.1035, grad_fn=<AddBackward0>)\n",
            "tensor(25032.3691, grad_fn=<AddBackward0>)\n",
            "tensor(25377.6270, grad_fn=<AddBackward0>)\n",
            "tensor(25775.2070, grad_fn=<AddBackward0>)\n",
            "tensor(25551.7344, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [250/469], Reconst Loss: 172.3162, KL Div: 1.8204\n",
            "tensor(25939.9922, grad_fn=<AddBackward0>)\n",
            "tensor(26628.6875, grad_fn=<AddBackward0>)\n",
            "tensor(24580.4512, grad_fn=<AddBackward0>)\n",
            "tensor(24958.5996, grad_fn=<AddBackward0>)\n",
            "tensor(25035.5801, grad_fn=<AddBackward0>)\n",
            "tensor(25529.7148, grad_fn=<AddBackward0>)\n",
            "tensor(25267.2754, grad_fn=<AddBackward0>)\n",
            "tensor(24928.8516, grad_fn=<AddBackward0>)\n",
            "tensor(25092.4590, grad_fn=<AddBackward0>)\n",
            "tensor(24268.4082, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [260/469], Reconst Loss: 161.1927, KL Div: 1.8936\n",
            "tensor(25453.6055, grad_fn=<AddBackward0>)\n",
            "tensor(24919.2656, grad_fn=<AddBackward0>)\n",
            "tensor(24777.5625, grad_fn=<AddBackward0>)\n",
            "tensor(24979.8066, grad_fn=<AddBackward0>)\n",
            "tensor(25175.8887, grad_fn=<AddBackward0>)\n",
            "tensor(25318.3418, grad_fn=<AddBackward0>)\n",
            "tensor(25123.0352, grad_fn=<AddBackward0>)\n",
            "tensor(24484.5234, grad_fn=<AddBackward0>)\n",
            "tensor(24761.0938, grad_fn=<AddBackward0>)\n",
            "tensor(25284.7441, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [270/469], Reconst Loss: 166.4042, KL Div: 2.0755\n",
            "tensor(25643.7812, grad_fn=<AddBackward0>)\n",
            "tensor(25879.8613, grad_fn=<AddBackward0>)\n",
            "tensor(25413.5410, grad_fn=<AddBackward0>)\n",
            "tensor(25931.6152, grad_fn=<AddBackward0>)\n",
            "tensor(25586.5801, grad_fn=<AddBackward0>)\n",
            "tensor(24562.9062, grad_fn=<AddBackward0>)\n",
            "tensor(25339.7949, grad_fn=<AddBackward0>)\n",
            "tensor(25198.8477, grad_fn=<AddBackward0>)\n",
            "tensor(25650.0898, grad_fn=<AddBackward0>)\n",
            "tensor(24751.5000, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [280/469], Reconst Loss: 163.8507, KL Div: 1.9680\n",
            "tensor(24628.0938, grad_fn=<AddBackward0>)\n",
            "tensor(25345.9570, grad_fn=<AddBackward0>)\n",
            "tensor(24860.9297, grad_fn=<AddBackward0>)\n",
            "tensor(24667.8926, grad_fn=<AddBackward0>)\n",
            "tensor(25971.8164, grad_fn=<AddBackward0>)\n",
            "tensor(25531.7090, grad_fn=<AddBackward0>)\n",
            "tensor(24921.4141, grad_fn=<AddBackward0>)\n",
            "tensor(24960.4766, grad_fn=<AddBackward0>)\n",
            "tensor(25405.2559, grad_fn=<AddBackward0>)\n",
            "tensor(25051.3789, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [290/469], Reconst Loss: 169.0539, KL Div: 1.7773\n",
            "tensor(24630.2422, grad_fn=<AddBackward0>)\n",
            "tensor(25503.6348, grad_fn=<AddBackward0>)\n",
            "tensor(25185.4492, grad_fn=<AddBackward0>)\n",
            "tensor(25751.5820, grad_fn=<AddBackward0>)\n",
            "tensor(25459.3828, grad_fn=<AddBackward0>)\n",
            "tensor(24093.5312, grad_fn=<AddBackward0>)\n",
            "tensor(25690.5098, grad_fn=<AddBackward0>)\n",
            "tensor(25128.0352, grad_fn=<AddBackward0>)\n",
            "tensor(25393.2109, grad_fn=<AddBackward0>)\n",
            "tensor(24751.2871, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [300/469], Reconst Loss: 163.3862, KL Div: 1.9989\n",
            "tensor(25337.2188, grad_fn=<AddBackward0>)\n",
            "tensor(24691.0469, grad_fn=<AddBackward0>)\n",
            "tensor(25228.3027, grad_fn=<AddBackward0>)\n",
            "tensor(24955.8672, grad_fn=<AddBackward0>)\n",
            "tensor(25223.6562, grad_fn=<AddBackward0>)\n",
            "tensor(25083.8574, grad_fn=<AddBackward0>)\n",
            "tensor(25313.1250, grad_fn=<AddBackward0>)\n",
            "tensor(25038.0117, grad_fn=<AddBackward0>)\n",
            "tensor(25609.9375, grad_fn=<AddBackward0>)\n",
            "tensor(25449.5371, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [310/469], Reconst Loss: 172.2538, KL Div: 1.7714\n",
            "tensor(24617.4902, grad_fn=<AddBackward0>)\n",
            "tensor(25602.6953, grad_fn=<AddBackward0>)\n",
            "tensor(24443.5215, grad_fn=<AddBackward0>)\n",
            "tensor(24902.8789, grad_fn=<AddBackward0>)\n",
            "tensor(26594.0469, grad_fn=<AddBackward0>)\n",
            "tensor(25053.0586, grad_fn=<AddBackward0>)\n",
            "tensor(25066.7402, grad_fn=<AddBackward0>)\n",
            "tensor(25431.4805, grad_fn=<AddBackward0>)\n",
            "tensor(24282.4707, grad_fn=<AddBackward0>)\n",
            "tensor(25250.4492, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [320/469], Reconst Loss: 166.9205, KL Div: 2.0232\n",
            "tensor(25511.2207, grad_fn=<AddBackward0>)\n",
            "tensor(25401.5898, grad_fn=<AddBackward0>)\n",
            "tensor(24862.4297, grad_fn=<AddBackward0>)\n",
            "tensor(25324.2930, grad_fn=<AddBackward0>)\n",
            "tensor(24321.1914, grad_fn=<AddBackward0>)\n",
            "tensor(25129.5195, grad_fn=<AddBackward0>)\n",
            "tensor(25110.0039, grad_fn=<AddBackward0>)\n",
            "tensor(25163.1445, grad_fn=<AddBackward0>)\n",
            "tensor(25436.6973, grad_fn=<AddBackward0>)\n",
            "tensor(24827.3770, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [330/469], Reconst Loss: 164.6035, KL Div: 1.9574\n",
            "tensor(24926.7344, grad_fn=<AddBackward0>)\n",
            "tensor(24938.1602, grad_fn=<AddBackward0>)\n",
            "tensor(25701.8789, grad_fn=<AddBackward0>)\n",
            "tensor(24658.6719, grad_fn=<AddBackward0>)\n",
            "tensor(24666.3008, grad_fn=<AddBackward0>)\n",
            "tensor(25677.3145, grad_fn=<AddBackward0>)\n",
            "tensor(24619.9746, grad_fn=<AddBackward0>)\n",
            "tensor(25947.0195, grad_fn=<AddBackward0>)\n",
            "tensor(24820.4121, grad_fn=<AddBackward0>)\n",
            "tensor(24749.1562, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [340/469], Reconst Loss: 164.7708, KL Div: 1.9055\n",
            "tensor(25171.5176, grad_fn=<AddBackward0>)\n",
            "tensor(25536.2656, grad_fn=<AddBackward0>)\n",
            "tensor(25648.7266, grad_fn=<AddBackward0>)\n",
            "tensor(24853.2031, grad_fn=<AddBackward0>)\n",
            "tensor(26103.1797, grad_fn=<AddBackward0>)\n",
            "tensor(25371.4141, grad_fn=<AddBackward0>)\n",
            "tensor(25649.1484, grad_fn=<AddBackward0>)\n",
            "tensor(24694.6699, grad_fn=<AddBackward0>)\n",
            "tensor(25526.3164, grad_fn=<AddBackward0>)\n",
            "tensor(25486.1953, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [350/469], Reconst Loss: 171.2088, KL Div: 1.8601\n",
            "tensor(25100.5332, grad_fn=<AddBackward0>)\n",
            "tensor(24418.3555, grad_fn=<AddBackward0>)\n",
            "tensor(25807.1191, grad_fn=<AddBackward0>)\n",
            "tensor(25336.4824, grad_fn=<AddBackward0>)\n",
            "tensor(24046.5566, grad_fn=<AddBackward0>)\n",
            "tensor(25213.5098, grad_fn=<AddBackward0>)\n",
            "tensor(24427.2617, grad_fn=<AddBackward0>)\n",
            "tensor(23997.4648, grad_fn=<AddBackward0>)\n",
            "tensor(24864.3887, grad_fn=<AddBackward0>)\n",
            "tensor(26171.5527, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [360/469], Reconst Loss: 176.3919, KL Div: 1.8716\n",
            "tensor(25075.4551, grad_fn=<AddBackward0>)\n",
            "tensor(25988.6973, grad_fn=<AddBackward0>)\n",
            "tensor(24540.5371, grad_fn=<AddBackward0>)\n",
            "tensor(25766.3047, grad_fn=<AddBackward0>)\n",
            "tensor(24691.0781, grad_fn=<AddBackward0>)\n",
            "tensor(25262.7812, grad_fn=<AddBackward0>)\n",
            "tensor(24979.6934, grad_fn=<AddBackward0>)\n",
            "tensor(24308.2500, grad_fn=<AddBackward0>)\n",
            "tensor(24590.7109, grad_fn=<AddBackward0>)\n",
            "tensor(25423.4668, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [370/469], Reconst Loss: 169.9457, KL Div: 1.9117\n",
            "tensor(24634.7051, grad_fn=<AddBackward0>)\n",
            "tensor(24946.7969, grad_fn=<AddBackward0>)\n",
            "tensor(24707.6289, grad_fn=<AddBackward0>)\n",
            "tensor(23967.0566, grad_fn=<AddBackward0>)\n",
            "tensor(25148.2617, grad_fn=<AddBackward0>)\n",
            "tensor(24726.3477, grad_fn=<AddBackward0>)\n",
            "tensor(25112.5586, grad_fn=<AddBackward0>)\n",
            "tensor(25775.3613, grad_fn=<AddBackward0>)\n",
            "tensor(25481.0527, grad_fn=<AddBackward0>)\n",
            "tensor(25842.1680, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [380/469], Reconst Loss: 170.4718, KL Div: 2.0947\n",
            "tensor(25116.1719, grad_fn=<AddBackward0>)\n",
            "tensor(25476., grad_fn=<AddBackward0>)\n",
            "tensor(25026.8105, grad_fn=<AddBackward0>)\n",
            "tensor(25631.3457, grad_fn=<AddBackward0>)\n",
            "tensor(25206.9434, grad_fn=<AddBackward0>)\n",
            "tensor(24670.1016, grad_fn=<AddBackward0>)\n",
            "tensor(24170.9688, grad_fn=<AddBackward0>)\n",
            "tensor(26030.8945, grad_fn=<AddBackward0>)\n",
            "tensor(25469.5996, grad_fn=<AddBackward0>)\n",
            "tensor(25279.8984, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [390/469], Reconst Loss: 170.2583, KL Div: 1.8161\n",
            "tensor(25116.8086, grad_fn=<AddBackward0>)\n",
            "tensor(24933.9434, grad_fn=<AddBackward0>)\n",
            "tensor(24421.9180, grad_fn=<AddBackward0>)\n",
            "tensor(25005.4922, grad_fn=<AddBackward0>)\n",
            "tensor(26021.3398, grad_fn=<AddBackward0>)\n",
            "tensor(25496.0527, grad_fn=<AddBackward0>)\n",
            "tensor(26240.8066, grad_fn=<AddBackward0>)\n",
            "tensor(26071.8086, grad_fn=<AddBackward0>)\n",
            "tensor(26083.7188, grad_fn=<AddBackward0>)\n",
            "tensor(24914.7852, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [400/469], Reconst Loss: 167.0019, KL Div: 1.8430\n",
            "tensor(25430.6113, grad_fn=<AddBackward0>)\n",
            "tensor(26468.7734, grad_fn=<AddBackward0>)\n",
            "tensor(25970.8633, grad_fn=<AddBackward0>)\n",
            "tensor(24332.6250, grad_fn=<AddBackward0>)\n",
            "tensor(25607.3633, grad_fn=<AddBackward0>)\n",
            "tensor(25212.8535, grad_fn=<AddBackward0>)\n",
            "tensor(25686.5352, grad_fn=<AddBackward0>)\n",
            "tensor(25170.9551, grad_fn=<AddBackward0>)\n",
            "tensor(25117.2461, grad_fn=<AddBackward0>)\n",
            "tensor(24567.9316, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [410/469], Reconst Loss: 163.0487, KL Div: 1.9259\n",
            "tensor(25349.6621, grad_fn=<AddBackward0>)\n",
            "tensor(25205.7188, grad_fn=<AddBackward0>)\n",
            "tensor(25049.3242, grad_fn=<AddBackward0>)\n",
            "tensor(25972.3887, grad_fn=<AddBackward0>)\n",
            "tensor(24281.9941, grad_fn=<AddBackward0>)\n",
            "tensor(25429.0820, grad_fn=<AddBackward0>)\n",
            "tensor(23756.1191, grad_fn=<AddBackward0>)\n",
            "tensor(24981.3164, grad_fn=<AddBackward0>)\n",
            "tensor(26728.2402, grad_fn=<AddBackward0>)\n",
            "tensor(24961.1250, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [420/469], Reconst Loss: 164.3901, KL Div: 2.0412\n",
            "tensor(24205.9277, grad_fn=<AddBackward0>)\n",
            "tensor(25388.9297, grad_fn=<AddBackward0>)\n",
            "tensor(24728.0293, grad_fn=<AddBackward0>)\n",
            "tensor(24527.7344, grad_fn=<AddBackward0>)\n",
            "tensor(25504.7578, grad_fn=<AddBackward0>)\n",
            "tensor(24444.7891, grad_fn=<AddBackward0>)\n",
            "tensor(25552.0293, grad_fn=<AddBackward0>)\n",
            "tensor(24424.9102, grad_fn=<AddBackward0>)\n",
            "tensor(24425.3027, grad_fn=<AddBackward0>)\n",
            "tensor(25172.0996, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [430/469], Reconst Loss: 167.0498, KL Div: 1.9738\n",
            "tensor(25427.1055, grad_fn=<AddBackward0>)\n",
            "tensor(25710.6582, grad_fn=<AddBackward0>)\n",
            "tensor(24762.1914, grad_fn=<AddBackward0>)\n",
            "tensor(25499.7246, grad_fn=<AddBackward0>)\n",
            "tensor(25165.1992, grad_fn=<AddBackward0>)\n",
            "tensor(24628.9082, grad_fn=<AddBackward0>)\n",
            "tensor(24476.8867, grad_fn=<AddBackward0>)\n",
            "tensor(24903.4023, grad_fn=<AddBackward0>)\n",
            "tensor(26049.7070, grad_fn=<AddBackward0>)\n",
            "tensor(25602.9023, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [440/469], Reconst Loss: 169.3222, KL Div: 2.0467\n",
            "tensor(25825.1875, grad_fn=<AddBackward0>)\n",
            "tensor(25461.7617, grad_fn=<AddBackward0>)\n",
            "tensor(26119.9219, grad_fn=<AddBackward0>)\n",
            "tensor(25137.4121, grad_fn=<AddBackward0>)\n",
            "tensor(24732.0996, grad_fn=<AddBackward0>)\n",
            "tensor(25666.1035, grad_fn=<AddBackward0>)\n",
            "tensor(25432.2969, grad_fn=<AddBackward0>)\n",
            "tensor(26004.7363, grad_fn=<AddBackward0>)\n",
            "tensor(24811.0508, grad_fn=<AddBackward0>)\n",
            "tensor(24874.1289, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [450/469], Reconst Loss: 167.4551, KL Div: 1.7916\n",
            "tensor(23708.0371, grad_fn=<AddBackward0>)\n",
            "tensor(25461.8496, grad_fn=<AddBackward0>)\n",
            "tensor(24710.7012, grad_fn=<AddBackward0>)\n",
            "tensor(25409.2559, grad_fn=<AddBackward0>)\n",
            "tensor(24620.1230, grad_fn=<AddBackward0>)\n",
            "tensor(25269.9199, grad_fn=<AddBackward0>)\n",
            "tensor(25203.5195, grad_fn=<AddBackward0>)\n",
            "tensor(25000.7832, grad_fn=<AddBackward0>)\n",
            "tensor(25681.0703, grad_fn=<AddBackward0>)\n",
            "tensor(25688.1191, grad_fn=<AddBackward0>)\n",
            "Epoch[25/25], Step [460/469], Reconst Loss: 172.1320, KL Div: 1.9038\n",
            "tensor(24479.3516, grad_fn=<AddBackward0>)\n",
            "tensor(24717.9004, grad_fn=<AddBackward0>)\n",
            "tensor(24855.0918, grad_fn=<AddBackward0>)\n",
            "tensor(25273.2422, grad_fn=<AddBackward0>)\n",
            "tensor(24946.0957, grad_fn=<AddBackward0>)\n",
            "tensor(24951.5742, grad_fn=<AddBackward0>)\n",
            "tensor(25535.1719, grad_fn=<AddBackward0>)\n",
            "tensor(26091.2676, grad_fn=<AddBackward0>)\n",
            "tensor(18102.8535, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Beta VAE Outputs"
      ],
      "metadata": {
        "id": "OnyczVmh4L_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Reconstruction using Random Beta VAE Training where Beta = 15"
      ],
      "metadata": {
        "id": "I_2jnohZ4Pge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_reconstruction(model3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "x2iVhoJRKEer",
        "outputId": "6e2c6c1e-605a-43d8-ce3e-e3368aea0688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Generation using Random Beta VAE Training where Beta = 15"
      ],
      "metadata": {
        "id": "GYwAywnb4V4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_generation(model3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "MXyPGhU0KEo4",
        "outputId": "09ba16db-3e07-4312-c932-a10f38561cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Comparison of Three Models"
      ],
      "metadata": {
        "id": "dABZJOFq4uZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Dataset Creation"
      ],
      "metadata": {
        "id": "2MC3Wk3x47Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "losses = []\n",
        "for loss in epoch_loss_main:\n",
        "  losses.append(loss.detach().numpy())\n",
        "df['Loss_Beta_optimizer'] = losses\n",
        "losses = []\n",
        "for loss in epoch_loss_vae:\n",
        "  losses.append(loss.detach().numpy())\n",
        "df['Loss_vae'] = losses\n",
        "\n",
        "losses = []\n",
        "for loss in epoch_loss_beta_vae:\n",
        "  losses.append(loss.detach().numpy())\n",
        "\n",
        "df['Loss_beta_vae'] = losses\n",
        "df.to_csv('Losses.csv',index =False)"
      ],
      "metadata": {
        "id": "j2wB28MAEI2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Losses"
      ],
      "metadata": {
        "id": "6A7C9UXW4-gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt2\n",
        "plt2.plot(epoch_counts, df['Loss_Beta_optimizer'], 'o--', color='red', alpha=0.3)\n",
        "plt2.plot(epoch_counts, df['Loss_vae'], 'o--', color='green', alpha=0.3)\n",
        "plt2.plot(epoch_counts, df['Loss_beta_vae'], 'o--', color='blue', alpha=0.3)\n",
        "plt2.xlabel('epochs')\n",
        "plt2.ylabel('loss')\n",
        "location = 0 # For the best location\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Beta Optimizer VAE\", \"Normal VAE\", \"15-Beta VAE\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "vg2ZRU7IKraD",
        "outputId": "5b3ca69d-9f2e-4bd3-cbce-adc924dae97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}